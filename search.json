[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sur le chemin de l’enf-R",
    "section": "",
    "text": "Préface",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#lobjectif-de-ce-livre",
    "href": "index.html#lobjectif-de-ce-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "L’objectif de ce livre",
    "text": "L’objectif de ce livre\nL’objectif de ce livre est double :\n\nVous présenter R, un environnement interactif puissant et flexible pour le calcul et la recherche statistiques.\nVous présenter (ou vous familiariser à nouveau) avec l’analyse statistique effectuée dans R.\n\nR n’est pas difficile à apprendre en soi, mais comme pour toute nouvelle langue (parlée ou informatique), la courbe d’apprentissage initiale peut être raide et quelque peu intimidante. L’objectif n’est pas de tout couvrir (ni avec R, ni avec les statistiques), mais simplement de vous aider à franchir le cap (potentiellement plus rapidement) et à vous fournir les compétences de base (et la confiance !) nécessaires pour commencer votre propre voyage avec R et avec des analyses spécifiques.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#livre-multilingue",
    "href": "index.html#livre-multilingue",
    "title": "Sur le chemin de l’enf-R",
    "section": "Livre multilingue",
    "text": "Livre multilingue\nLe livre est fourni comme un livre multilingue qui brise la barrière de la langue et permet potentiellement de faciliter l’apprentissage de R et de son environnement principalement anglophone. Nous sommes toujours à la recherche de bénévoles pour nous aider à développer le livre et à ajouter d’autres langues à la liste qui ne cesse de s’allonger . N’hésite pas à Nous contacter si tu veux nous aider\nSur la page web, tu peux changer de langue via le  dans la barre de navigation. Après avoir changer de langue, tu peux télécharger le document en pdf ou epub pour cet langue .\nListe des langues :\n\nanglais (publié mais à peaufiner)\nfrançais (en développement, en attendant que l’anglais soit peaufiné)\nespagnol (un jour peut-être…)\n… des volontaires pour plus ??",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#comment-utiliser-ce-livre",
    "href": "index.html#comment-utiliser-ce-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "Comment utiliser ce livre",
    "text": "Comment utiliser ce livre\nPour une meilleure expérience, nous te recommandons de lire la version web de ce livre que tu peux trouver à https://biostats-uottawa.github.io/enfR.\nLa version web inclut une barre de navigation incluant des options pour faciliter la lecture , de recherche , pour changer la couleur  et pour suggérer des modifications ou reporter des problèmes . Tu peux aussi télécharger le document  au format pdf ou epub.\nNous utilisons quelques conventions typographiques tout au long de ce livre.\nLe code R et la sortie qui en résulte sont présentés dans des blocs de code dans notre livre.\n\n2 + 2\n\n[1] 4\n\n\nLes fonctions dans le texte sont présentées avec des parenthèses à la fin en utilisant la police de code, c’est-à-dire mean() ou sd() etc.\nLes objets sont représentés à l’aide de la police de code sans les parenthèses, c’est-à-dire obj1, obj2 etc.\nLes paquets R dans le texte sont indiqués en utilisant la police de code et suivis de l’icone 📦, exemple tidyverse 📦.\nUne série d’actions nécessaires pour accéder aux commandes de menu dans RStudio ou VSCode sont identifiées comme suit File -&gt; New File -&gt; R Script ce qui se traduit par “clique sur le menu Fichier, puis clique sur Nouveau fichier et sélectionne R Script”.\nLorsque nous faisons référence à IDE (Integrated Development Environment : Logiciel d’Environnement de Développement Intégré) dans la suite du texte, il s’agit de RStudio ou de VScode.\nLorsque nous parlons de .[Rq]md, nous entendons par là les documents R markdown (.Rmd) ou Quarto (.qmd) et nous parlerons généralement des documents R markdown en faisant référence à l’un ou l’autre des fichiers .Rmd ou .qmd.\nLe manuel tente de mettre en évidence certaines parties du texte à l’aide des encadrés et icônes suivants.\n\n\n\n\n\n\nExercices\n\n\n\nDes choses à faire pour toi\n\n\n\n\n\n\n\n\nSolutions\n\n\n\nCode R et explications\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nAvertissements\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPoints importants\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotes",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sec-qui",
    "href": "index.html#sec-qui",
    "title": "Sur le chemin de l’enf-R",
    "section": "Qui sommes-nous ?",
    "text": "Qui sommes-nous ?\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJulien Martin est professeur à l’Université d’Ottawa en Écologie évolutive. Il a découvert le merveilleux monde R avec la version 1.8.1 et l’enseigne depuis R v2.4.0.\n\n\n: uOttawa page, lab page\n\n\n: jgamartin\n\n\n: juliengamartin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nAugustin Birot est étudiant au doctorat à l’Université d’Ottawa en Écologie évolutive.\n\n\n: page du labo \n\n\n\n\n\nContribution au livre\nJM: développement et écriture de la première version des chapitres en anglais, mise en page, traduction, maintenance\nAB: traduction en français et clarification de tous les chapitres.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#remerciements",
    "href": "index.html#remerciements",
    "title": "Sur le chemin de l’enf-R",
    "section": "Remerciements",
    "text": "Remerciements\nLa première partie du livre sur l’utilisation de R a commencé comme un fork sur github à partir de l’excellent livre en anglais An introduction to R de Douglas, Roos, Mancini, Couto et Lusseau (Douglas 2023). Il a été forké le 23 avril 2023 à partir de Alexd106 github repository puis modifié et mis à jour en fonction de mes propres besoins et de ma perspective d’enseignement de R. Le contenu n’a pas été revu ni approuvé par les développeurs précédents.\nPlusieurs parties du livre sont basées sur des manuels de laboratoire pour les cours de biostatistique à l’Université d’Ottawa écrits par Martin, Findlay, Morin et Rundle.\nSites ayant fourni de nombreuses informations pour le livre :\n\ndplyr introduction\nIntroduction to gam\nIntoduction to gams by Noam Ross",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#crédits-dimage",
    "href": "index.html#crédits-dimage",
    "title": "Sur le chemin de l’enf-R",
    "section": "Crédits d’image",
    "text": "Crédits d’image\nLes photos, images et captures d’écran sont de Julien Martin sauf lorsque indiqué dans la légende.\nL’image de couverture a été générée par Nightcafe Ai Art generator. Le Favicon et l’autocollant hexagonal ont été créés à partir de l’image de couverture.\n\n\n\n\n\n\nNote\n\n\n\nplusieurs captures d’écran sont actuellement réalisées par Alex Douglas et sont en train d’être refaites pour pour se conformer à la déclaration précédente",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Sur le chemin de l’enf-R",
    "section": "Licence",
    "text": "Licence\nJe partage cette version modifiée du livre original sous la licence Licence Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n\n\nLicence Creative Commons\n\nSi tu enseignes R, n’hésite pas à utiliser tout ou partie du contenu de ce livre pour aider tes propres élèves. La seule chose que je te demande, c’est de citer la source originale et les auteurs. Si tu trouves ce livre utile ou si tu as des commentaires ou des suggestions, j’aimerais beaucoup que tu me les fasses parvenir (contact info).",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#citer-le-livre",
    "href": "index.html#citer-le-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "Citer le livre",
    "text": "Citer le livre\nJulien Martin. (2024). Sur le chemin de l’enf-R. Un livre multilingue d’introduction à R. Version: 0.6.0 (2024-10-08). DOI: 10.5281/zenodo.13801265",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#chapters-to-read",
    "href": "index.html#chapters-to-read",
    "title": "Sur le chemin de l’enf-R",
    "section": "Lecture associée au cours",
    "text": "Lecture associée au cours\n\n\n\nTable 1: Course associated reading for biostatistical course at uOttawa\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBioXx58 \nBio8940 \n\n\n\nUtiliser R\n\n\n1.-4.\n✅✅\n😃\n\n\n5. Programmation\n\n✅✅\n\n\n6. Rapports reproductibles\n✔️\n✅✅\n\n\n7. Contrôle de version\n\n✅✅\n\n\nPrincipes de statistiques\n\n\ntous les chapitres\n✅✅\n😃\n\n\nModèles linéaires\n\n\ntous les chapitres\n✅✅\n😃\n\n\nModèles linéaires généralisés\n\n\ntous les chapitres\n✔️\n✅✅\n\n\nModèles mixtes\n\n\ntous les chapitres\n\n✅✅\n\n\nModèles additifs généralisés\n\n\ntous les chapitres\n\n✔️\n\n\nAnalyses multivariées\n\n\ntous les chapitres\n\n✔️\n\n\nApproche Bayésienne\n\n\ntous les chapitres\n\n✅✅\n\n\n\nSuggéré ✔️ ; Obligatoire ✅✅ ; connaissances attendues (pourraient avoir besoin d'une remise à niveau) 😃",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#collant-hex",
    "href": "index.html#collant-hex",
    "title": "Sur le chemin de l’enf-R",
    "section": "Collant Hex",
    "text": "Collant Hex\n\n\n\n\n\n\n\n\nDouglas, A. 2023. An introduction to R.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "01-debut.html",
    "href": "01-debut.html",
    "title": "1  Pour commencer",
    "section": "",
    "text": "Quelques conseils sur R\nBonne chance et n’oubliez pas de vous amuser.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#quelques-conseils-sur-r",
    "href": "01-debut.html#quelques-conseils-sur-r",
    "title": "1  Pour commencer",
    "section": "",
    "text": "Utilisez R souvent et régulièrement. Cela permettra de créer et maintenir une dynamique importante.\nApprendre R n’est pas un test de mémoire. L’un des avantages d’un langage de script est que vous aurez toujours votre code (bien annoté) auquel vous pourrez vous référer lorsque vous oublierez comment faire quelque chose.\nIl n’est pas nécessaire de tout savoir sur R pour être productif.\nSi vous êtes bloqué, faites des recherches en ligne, ce n’est pas de la triche et écrire une bonne requête de recherche est une compétence en soi.\nSi vous vous retrouvez à fixer un code pendant des heures en essayant de comprendre pourquoi il ne fonctionne pas, éloignez-vous quelques minutes.\nEn R, il existe de nombreuses façons d’aborder un problème particulier. Si votre code fait ce que vous voulez qu’il fasse dans un temps raisonnable et de manière robuste, ne vous inquiétez pas.\nR n’est qu’un outil pour vous aider à répondre à vos questions intéressantes. Ne perdez pas de vue ce qui est important : vos questions de recherche et vos données. Aucune compétence en matière d’utilisation de R ne sera utile si votre collecte de données est fondamentalement défectueuse ou si votre question est vague.\nSoyez conscient qu’il y aura des moments où les choses deviendront un peu difficiles ou frustrantes. Essayez d’accepter ces périodes comme faisant partie du processus naturel d’apprentissage d’une nouvelle compétence (nous sommes tous passés par là) et rappelez-vous que le temps et l’énergie que vous investissez maintenant seront largement remboursés dans un avenir pas trop lointain.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#installation",
    "href": "01-debut.html#installation",
    "title": "1  Pour commencer",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\n\n1.1.1 Installation de R\nPour démarrer, la première chose à faire est d’installer R. R est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux à partir du site web du Comprehensive R Archive Network (CRAN) . Pour les utilisateurs de Windows et de Mac, nous vous suggérons de télécharger et d’installer les versions binaires précompilées. Il existe des instructions assez complètes pour installer R pour chaque système d’exploitation (Windows, Mac ou linux).\nQuel que soit le système d’exploitation que vous utilisez, une fois que vous avez installé R, vous devez vérifier qu’il fonctionne correctement. La manière la plus simple de le faire est de lancer R en double-cliquant sur l’icône R (Windows ou Mac) ou en tapant R dans la console (Linux). La console R devrait s’afficher et vous devriez pouvoir taper des commandes R dans la console après l’invite de commande &gt;. Essayez de taper le code R suivant et appuyez sur la touche Entrée :\n\nplot(1)\n\n\n\n\n\n\nFigure 1.1: Le meilleur graph du monde, sert juste à tester R\n\n\n\n\nUn graphique avec un seul point au centre devrait apparaître. Si c’est le cas, vous pouvez commencer. Si ce n’est pas le cas, nous vous suggérons de noter toutes les erreurs produites et d’utiliser ensuite votre moteur de recherche préféré pour résoudre le problème.\n\n1.1.2 Installation d’un IDE\nIl est fortement recommandé d’utiliser un logiciel d’Environnement de Développement Intégré (IDE) pour travailler avec R. Un IDE simple et extrêmement populaire est RStudio . Une alternative à RStudio est Visual Studio Code, ou VSCode . Un IDE peut être considéré comme un complément à R qui fournit une interface plus conviviale, intégrant la console R, un éditeur de scripts et d’autres fonctionnalités utiles (comme R markdown et l’intégration de Git Hub).\n\n\n\n\n\n\nMise en garde\n\n\n\nVous devez installer R avant d’installer un IDE (voir Section 1.1.1 pour plus de détails).\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsque l’on se réfère à un IDE dans la suite du texte, il s’agit de RStudio ou de VScode.\n\n\n\n1.1.2.1 RStudio\nRStudio est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux et peut être téléchargé à partir du site RStudio. Vous devez sélectionner la version ‘RStudio Desktop’.\n\n1.1.2.2 VSCode\nVSCode est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux et peut être téléchargé à partir du site VS Code .\nEn outre, vous devez installer l’extension R pour VSCode. Pour faire de VSCode une véritable centrale pour travailler avec R, nous vous recommandons fortement d’installer également :\n\n\nradian : Une console R moderne qui corrige de nombreuses limitations du terminal R officiel et prend en charge de nombreuses fonctionnalités telles que la coloration syntaxique et l’autocomplétion.\n\nVSCode-R-Debugger : Une extension de VS Code pour supporter les capacités de débogage de R.\n\nhttpgd : Un paquet R 📦 pour fournir un dispositif graphique qui sert de manière asynchrone des graphiques SVG via HTTP et WebSockets.\n\n1.1.2.3 Alternatives à RStudio et VSCode\nPlutôt que d’utiliser un IDE “tout-en-un”, de nombreuses personnes choisissent d’utiliser R et un éditeur de script séparé pour écrire et exécuter du code R. Si vous ne savez pas ce qu’est un éditeur de script, vous pouvez voir ça comme un logiciel de traitement de texte, mais spécialement conçu pour écrire du code. Par chance, de nombreux éditeurs de scripts sont disponibles gratuitement. N’hésitez donc pas à les télécharger et à les tester jusqu’à ce que vous en trouviez un qui vous convienne. Certains éditeurs de scripts ne sont disponibles que pour certains systèmes d’exploitation et tous ne sont pas spécifiques à R. Vous trouverez ci-dessous des suggestions d’éditeurs de scripts. C’est à vous de choisir celui qui vous convient le mieux : l’une des grandes qualités de R est que VOUS choisissez comment vous voulez utiliser R.\n\n1.1.2.3.1 Éditeurs de texte avancés\nUn moyen léger mais efficace de travailler avec R est d’utiliser des éditeurs de texte avancés tels que :\n\n\nAtom (tous les systèmes d’exploitation)\n\nBBedit (Mac OS)\n\ngedit (Linux ; livré avec la plupart des distributions Linux)\n\nMacVim (Mac OS)\n\nNano (Linux)\n\nNotepad++ (en anglais) (Windows)\n\nSublime Text (tous les systèmes d’exploitation)\n\nvim et son extension NVim-R (Linux)\n\n1.1.2.3.2 Environnements de développement intégrés\nCes environnements sont plus puissants que de simples éditeurs de texte et sont similaires à RStudio :\n\n\nEmacs et son extension Statistiques de Emacs Speaks (tous les systèmes d’exploitation)\n\nRKWard (Linux)\n\nTinn-R (Windows)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-orient",
    "href": "01-debut.html#sec-orient",
    "title": "1  Pour commencer",
    "section": "\n1.2 Orientation dans l’IDE",
    "text": "1.2 Orientation dans l’IDE\n\n1.2.1 RStudio\nLorsque vous ouvrez RStudio pour la première fois, vous devriez voir la présentation suivante (elle peut être légèrement différente sur un ordinateur Windows).\n\n\n\n\n\n\n\nFigure 1.2: Fenêtre principale R studio\n\n\n\n\nLa grande fenêtre (ou volet) de gauche est la Console. La fenêtre en haut à droite est le volet Environnement / Historique / Connexions et la fenêtre en bas à droite est le volet Fichiers / Graphiques / Paquets / Aide / Visiualiseur . Nous aborderons chacun de ces volets dans les paragraphes qui suivent. Vous pouvez personnaliser l’emplacement de chaque volet en cliquant sur le menu “Outils”, puis en sélectionnant Options globales –&gt; Disposition des volets. Vous pouvez redimensionner les volets en cliquant sur le milieu des bords de la fenêtre et en le faisant glisser dans la direction souhaitée. Il existe une multitude d’autres façons de personnaliser RStudio.\n\n1.2.1.1 Console\nLa console est le cheval de bataille de R. C’est là que R évalue tout le code que vous écrivez. Vous pouvez taper du code R directement dans la console à l’invite de la ligne de commande, &gt;. Par exemple, si vous tapez 2 + 2 dans la console, vous devriez obtenir la réponse 4 (avec un peu de chance). Ne vous préoccupez pas du [1] au début de la ligne pour l’instant.\n\n\n\n\n\n\n\nFigure 1.3: Vue de la console R studio\n\n\n\n\nCependant, dès que vous commencez à écrire plus de code R, cela devient plutôt encombrant. Au lieu de taper le code R directement dans la console, il est préférable de créer un script R. Un script R est un simple fichier texte portant l’extension .R qui contient vos lignes de code R. Ces lignes de code sont ensuite introduites dans la console R, ligne par ligne. Pour créer un nouveau script R, cliquez sur le menu “Fichier”, puis sélectionnez Nouveau fichier –&gt; Script R.\n\n\n\n\n\n\n\nFigure 1.4: Créer un nouveau fichier script sur R studio\n\n\n\n\nVous remarquerez qu’une nouvelle fenêtre (appelée volet Source) apparaît en haut à gauche de RStudio et que la console se trouve désormais en bas à gauche. La nouvelle fenêtre est un éditeur de script et c’est là que vous écrirez votre code.\n\n\n\n\n\n\n\nFigure 1.5: Fenêtre principale avec un nouveau script sur R studio\n\n\n\n\nPour transférer votre code de votre éditeur de script à la console, placez simplement votre curseur sur la ligne de code, puis cliquez sur le bouton “Exécuter” en haut à droite de la fenêtre de l’éditeur de script.\n\n\n\n\n\n\n\nFigure 1.6: Bouton “Exécuter” sur R studio\n\n\n\n\nVous devriez voir le résultat dans la fenêtre de la console. Si cliquer sur le bouton “Exécuter” devient fastidieux, vous pouvez utiliser le raccourci clavier “ctrl + entrée” (sous Windows et Linux) ou “cmd + entrée” (sous Mac). Vous pouvez enregistrer vos scripts R sous la forme d’un fichier .R en sélectionnant le menu “Fichier” et en cliquant sur “Enregistrer”. Notez que le nom du fichier dans l’onglet devient rouge pour vous rappeler que des modifications n’ont pas été enregistrées. Pour ouvrir votre script R dans RStudio, sélectionnez le menu “Fichier”, puis “Ouvrir le fichier…”. Enfin, il convient de noter que, bien que les scripts R soient enregistrés avec un nom de fichier .R il s’agit en fait de fichiers texte simples qui peuvent être ouverts avec n’importe quel éditeur de texte.\n\n1.2.1.2 Environnement/Histoire/Connexions\nLa fenêtre Environnement / Historique / Connexions contient de nombreuses informations utiles. Vous pouvez accéder à chaque composant en cliquant sur l’onglet approprié dans le volet.\n\nL’onglet “Environnement” affiche tous les objets que vous avez créés dans l’environnement actuel (global). Ces objets peuvent être des données que vous avez importées ou des fonctions que vous avez écrites. Les objets peuvent être affichés sous forme de liste ou de grille en sélectionnant l’option dans le menu déroulant situé en haut à droite de la fenêtre. Si vous utilisez le format Grille, vous pouvez supprimer des objets de l’environnement en cochant la case vide située à côté du nom de l’objet, puis en cliquant sur l’icône de balai. Il existe également un bouton “Importer un ensemble de données” qui permet d’importer des données sauvegardées dans différents formats de fichiers. Cependant, nous vous conseillons de ne pas utiliser cette approche pour importer vos données car elle n’est pas reproductible et donc pas robuste (voir Chapitre 3 pour plus de détails).\nL’onglet “Historique” contient une liste de toutes les commandes que vous avez saisies dans la console R. Vous pouvez rechercher dans votre historique la ligne de code que vous avez oubliée, renvoyer le code sélectionné dans la Console ou dans la fenêtre Source. En général, nous n’utilisons jamais cette fonction car nous nous référons toujours à notre script R.\nL’onglet “Connexions” vous permet de vous connecter à diverses sources de données telles que des bases de données externes.\n\n1.2.1.3 Fichiers/Graphiques/Paquets/Aide/Visualiseur\n\nL’onglet “Fichiers” répertorie tous les fichiers et répertoires externes dans le répertoire de travail actuel de votre ordinateur. Il fonctionne comme l’explorateur de fichiers (Windows) ou le Finder (Mac). Vous pouvez ouvrir, copier, renommer, déplacer et supprimer les fichiers répertoriés dans la fenêtre.\nL’onglet “Graphiques” est l’endroit où tous les graphiques que vous créez dans R sont affichés (sauf indication contraire de votre part). Vous pouvez “zoomer” sur les graphiques pour les agrandir à l’aide du bouton loupe et faire défiler les graphiques créés précédemment à l’aide des boutons flèches. Il est également possible d’exporter les graphiques vers un fichier externe à l’aide du menu déroulant “Exportation”. Les graphiques peuvent être exportés dans différents formats de fichiers tels que jpeg, png, pdf, tiff ou copiés dans le presse-papiers (bien qu’il soit probablement préférable d’utiliser les fonctions R appropriées pour ce faire - voir Chapitre 4 pour plus de détails).\nL’onglet “Paquets” répertorie tous les paquets que vous avez installés sur votre ordinateur. Vous pouvez également installer de nouveaux paquets et mettre à jour les paquets existants en cliquant respectivement sur les boutons “Installer” et “Mettre à jour”.\nL’onglet “Aide” affiche la documentation d’aide R pour chaque fonction. Nous verrons comment consulter les fichiers d’aide et comment rechercher de l’aide dans le Chapitre 2).\nL’onglet “Visualiseur” affiche le contenu web local tel que les graphiques web générés par certains packages.\n\n1.2.2 VSCode\n\n\n\n\n\n\n\nFigure 1.7: Aperçu de la fenêtre VSCode\n\n\n\n\n\n1.2.2.1 Panneau gauche\nContient :\n\nGestionnaire de fichiers et aperçu des fichiers\nSupport R incluant l’environnement R / la recherche R / l’aide R / l’installation de paquets\nInteraction avec Github\n\n\n\n\n\n\n\n\n\n\n(a) volet des fichiers\n\n\n\n\n\n\n\n\n\n(b) volet git\n\n\n\n\n\n\n\n\n\n(c) Volet R\n\n\n\n\n\n\nFigure 1.8: Panneau de gauche VS Code\n\n\n\n1.2.2.2 Onglets de l’éditeur\nComprend :\n\nUn onglet de graphiques (avec historique et navigation)\nUn éditeur de scripts\nUn panneaux de prévisualisation\n\n\n\n\n\n\n\n\nFigure 1.9: Onglet d’édition et panneaux de prévisualisation VSCode\n\n\n\n\n\n1.2.2.3 Fenêtre du terminal\nContient :\n\nLe terminal permettant d’avoir une session R ou tout autre type de terminal nécessaire (bash/tmux/). Il peut être divisé et exécuter plusieurs sessions en même temps.\nUn panneau de problèmes mettant en évidence les problèmes de grammaire et de codage\n\n\n\n\n\n\n\n\nFigure 1.10: Fenêtre du terminal VSCode",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-work-d",
    "href": "01-debut.html#sec-work-d",
    "title": "1  Pour commencer",
    "section": "\n1.3 Répertoires de travail",
    "text": "1.3 Répertoires de travail\nLe répertoire de travail est l’emplacement par défaut où R cherchera les fichiers que vous souhaitez charger et où il placera tous les fichiers que vous enregistrez. L’un des avantages de l’utilisation des projets RStudio est que lorsque vous ouvrez un projet, il définit automatiquement votre répertoire de travail à l’emplacement approprié. Vous pouvez vérifier le chemin d’accès de votre répertoire de travail en utilisant l’une des méthodes suivantes getwd() ou here() fonctions.\n\ngetwd()\n\n[1] \"/home/julien/Documents/courses/biostats/livre/enfR\"\n\n\nDans l’exemple ci-dessus, le répertoire de travail est un dossier appelé “R_way” qui est un sous-dossier de “biostats” dans le dossier “courses” qui lui-même se trouve dans un dossier “Documents” situé dans le dossier “julien” qui lui-même se trouve dans le dossier “home”. Sur un ordinateur fonctionnant sous Windows, notre répertoire de travail comprendrait également une lettre de lecteur (c.-à-d. C:\\home\\julien\\Documents\\courses\\biostats\\R_way).\nSi vous n’utilisez pas d’IDE, vous devez définir votre répertoire de travail à l’aide de la commande setwd() au début de chaque script R (ce que nous avons fait pendant de nombreuses années).\n\nsetwd(\"/home/julien/Documents/courses/biostats/R_way/\")\n\nCependant, le problème avec setwd() est qu’il utilise un chemin d’accès absolue spécifique à l’ordinateur sur lequel vous travaillez. Si vous souhaitez envoyer votre script à quelqu’un d’autre (ou si vous travaillez sur un autre ordinateur), ce chemin d’accès absolu ne fonctionnera pas sur l’ordinateur de votre ami/collègue, car la configuration de ses répertoires sera différente (il est peu probable que vous ayez une structure de répertoires /home/julien/Documents/courses/biostats/ sur votre ordinateur). Il en résulte un projet qui n’est pas autonome et qui n’est pas facilement transportable. Les IDE résolvent ce problème en vous permettant d’utiliser des fichiers relatif qui sont relatifs au chemin d’accès au fichier racine du projet. Le fichier racine du projet est simplement le répertoire qui contient le fichier .Rproj dans Rstudio (first_project.Rproj dans notre cas) ou le dossier de base de votre espace de travail dans VScode. Si vous souhaitez partager vos analyses avec quelqu’un d’autre, il vous suffit de copier le répertoire complet du projet et de l’envoyer à votre collaborateur. Il lui suffira alors d’ouvrir le fichier du projet et tous les scripts R qui contiennent des références à des chemins d’accès relatifs fonctionneront. Par exemple, disons que vous avez créé un sous-répertoire appelé data dans votre répertoire de projet racine, qui contient un fichier délimité au format csv appelé mydata.csv (nous aborderons les structures de répertoire plus loin dans Section 1.4). Pour importer ce fichier dans un projet RStudio à l’aide de la commande read.csv() (ne vous en préoccupez pas pour l’instant, nous aborderons cette question plus en détail dans Chapitre 3), tout ce que vous devez inclure dans votre script R est\ndat &lt;- read.csv(\"data/mydata.csv\")\nParce que le chemin du fichier data/mydata.csv est relatif au répertoire du projet, peu importe où votre collaborateur enregistre le répertoire du projet sur son ordinateur, cela fonctionnera toujours.\nSi vous n’utilisez pas un projet RStudio ou un espace de travail VScode, vous devrez soit définir le répertoire de travail en fournissant le chemin complet de votre répertoire, soit spécifier le chemin complet du fichier de données. Aucune de ces deux options n’est reproductible sur d’autres ordinateurs.\nsetwd(\"/home/julien/Documents/courses/biostats/R_way\")\n\ndat &lt;- read.csv(\"data/mydata.csv\")\nou\ndat &lt;- read.csv(\"/home/julien/Documents/courses/biostats/R_way/data/mydata.csv\")\nPour ceux d’entre vous qui souhaitent pousser plus loin la notion de chemins d’accès relatifs aux fichiers, jetez un coup d’œil à la fonction here() du paquet here 📦. La fonction here() vous permet de créer des chemins d’accès pour n’importe quel fichier par rapport au répertoire racine du projet, qui ne dépendent pas du système d’exploitation (fonctionne sur une machine Mac, Windows ou Linux). Par exemple, pour importer notre mydata.csv à partir du répertoire data il suffit d’utiliser :\nlibrary(here) # il se peut que vous deviez d'abord installer le paquet 'here'\ndat &lt;- read.csv(here(\"data\", \"mydata.csv\"))",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-dir-struc",
    "href": "01-debut.html#sec-dir-struc",
    "title": "1  Pour commencer",
    "section": "\n1.4 Structure du répertoire",
    "text": "1.4 Structure du répertoire\nOutre l’utilisation de RStudio Projects, il est également conseillé de structurer votre répertoire de travail de manière cohérente et logique afin de vous aider, vous et vos collaborateurs. Nous utilisons fréquemment la structure de répertoire suivante dans nos projets basés sur R.\n\n\n\n\n\nroot\nracinedot01\nroot-&gt;dot01\ndot1\nroot-&gt;dot1\ndata\ndonnéesdot21\ndata-&gt;dot21\nfunctions\nfonctionsdot22\nfunctions-&gt;dot22\noutputs\nsortiedot23\noutputs-&gt;dot23\nscripts\nscriptsdot24\nscripts-&gt;dot24\nwd\nvotre répertoire de travailLOT1\nmétadonnées brutes traitéeLOT2\nfonctions RLOT4\nscripts d'analyses documents R markdownLOT3\npdf html figuresdot01-&gt;wd\ndot1-&gt;data\ndot2\ndot1-&gt;dot2\ndot2-&gt;functions\ndot3\ndot2-&gt;dot3\ndot3-&gt;outputs\ndot4\ndot3-&gt;dot4\ndot4-&gt;scripts\ndot21-&gt;LOT1\ndot22-&gt;LOT2\ndot23-&gt;LOT3\ndot24-&gt;LOT4\n\n\n\n\nFigure 1.11: Structure de répertoire recommandée pour des analyses sur R\n\n\n\n\nDans notre répertoire de travail, nous avons les répertoires suivants :\n\nRacine - Il s’agit du répertoire de votre projet contenant votre fichier .Rproj. Nous avons tendance à garder tous les scripts R ou [Rq]md nécessaires à l’analyse/au rapport dans ce dossier racine ou dans le dossier scripts s’il y en a trop.\ndonnées - Nous stockons toutes nos données dans ce répertoire. Le sous-répertoire appelé données contient des fichiers de données brutes et uniquement des fichiers de données brutes. Ces fichiers doivent être traités comme s’ils étaient en lecture seule et ne doivent en aucun cas être modifiés. Si vous devez traiter/nettoyer/modifier vos données, faites-le dans R (et non dans MS Excel), car vous pourrez documenter (et justifier) toutes les modifications apportées. Toutes les données traitées doivent être sauvegardées dans un fichier séparé et stockées dans le fichier données_traitées . Les informations sur les méthodes de collecte des données, les détails du téléchargement des données et toute autre métadonnée utile doivent être sauvegardés dans un document texte (voir les fichiers README ci-dessous) dans le sous-répertoire meta_données.\nfonctions - Il s’agit d’un répertoire facultatif dans lequel nous enregistrons toutes les fonctions R personnalisées que nous avons écrites pour l’analyse en cours. Celles-ci peuvent ensuite être importées dans R à l’aide de la fonction source().\nscripts - Un répertoire optionnel où nous enregistrons nos documents R markdown et/ou les principaux scripts R que nous avons écrits pour le projet en cours. Ces documents ne sont pas enregistrés dans le dossier racine.\nsortie - Les résultats de nos scripts R, tels que les graphiques, les fichiers HTML et les résumés de données, sont enregistrés dans ce répertoire. Cela nous aide, ainsi que nos collaborateurs, à distinguer les fichiers qui sont des sorties de ceux qui sont des fichiers sources.\n\nBien entendu, la structure décrite ci-dessus est celle qui nous convient le mieux la plupart du temps et doit être considérée comme un point de départ pour vos propres besoins. Nous avons tendance à avoir une structure de répertoires assez cohérente dans tous nos projets, car cela nous permet de nous orienter rapidement lorsque nous revenons à un projet après un certain temps. Cela dit, les besoins varient d’un projet à l’autre et nous ajoutons ou supprimons des répertoires en fonction des besoins.\nVous pouvez créer votre structure de répertoire à l’aide de l’explorateur Windows (ou Finder sur Mac) ou dans votre IDE en cliquant sur le bouton “Nouveau dossier” dans le panneau “Fichiers”.\nUne autre approche est d’utiliser la fonction dir.create() dans la console R.\n# créer un répertoire appelé 'données'\ndir.create(\"données\")",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-rsprojs",
    "href": "01-debut.html#sec-rsprojs",
    "title": "1  Pour commencer",
    "section": "\n1.5 Organisation des projets",
    "text": "1.5 Organisation des projets\nComme pour la plupart des choses de la vie, lorsqu’il s’agit de traiter des données et de les analyser, les choses sont tellement plus simples si vous êtes organisé. Une organisation claire du projet permet à la fois à vous (et surtout au futur vous) et à vos collaborateurs de donner un sens à ce que vous avez fait. Il n’y a rien de plus frustrant que de revenir à un projet des mois (parfois des années) plus tard et de devoir passer des jours (ou des semaines) à comprendre où tout se trouve, ce que vous avez fait et pourquoi vous l’avez fait. Un projet bien documenté, doté d’une structure cohérente et logique, augmente les chances de pouvoir reprendre le projet là où il s’est arrêté sans trop de difficultés, quel que soit le temps qui s’est écoulé. En outre, il est beaucoup plus facile d’écrire du code pour automatiser des tâches lorsque les fichiers sont bien organisés et portent des noms judicieux. Cela est d’autant plus vrai maintenant qu’il n’a jamais été aussi facile de collecter de grandes quantités de données qui peuvent être sauvegardées dans des milliers, voire des centaines de milliers de fichiers de données distincts. Enfin, un projet bien organisé réduit le risque d’introduire des bogues ou des erreurs dans votre flux de travail et, s’ils se produisent (ce qui est inévitable à un moment ou à un autre), il est plus facile de retrouver ces erreurs et de les traiter efficacement.\nIl existe également quelques mesures simples que vous pouvez prendre dès le début d’un projet pour aider à maintenir les choses en bon état.\nUn bon moyen de garder les choses organisées est d’utiliser les projets RStudio ou les espaces de travail VSCode, désignés sous le nom de projet. Un projet conserve tous vos scripts R, vos documents R markdown, vos fonctions R et vos données en un seul endroit. Ce qu’il y a de bien avec un projet est que chacun a son propre répertoire, son propre historique et ses propres documents sources, de sorte que les différentes analyses sur lesquelles vous travaillez sont complètement séparées les unes des autres. Cela signifie que vous pouvez très facilement passer d’un projet à l’autre sans craindre qu’ils n’interfèrent l’un avec l’autre.\n\n1.5.1 RStudio\nPour créer un projet, ouvrez RStudio et sélectionnez Fichier –&gt; Nouveau projet... dans le menu. Vous pouvez créer un projet entièrement nouveau, un projet à partir d’un répertoire existant ou un projet à version contrôlée (voir le Chapitre 7 pour plus de détails à ce sujet). Dans ce chapitre, nous allons créer un projet dans un nouveau répertoire.\n\n\n\n\n\n\n\nFigure 1.12: Créer un Projet R Studio étape 1\n\n\n\n\nVous pouvez également créer un nouveau projet en cliquant sur le bouton “Projet” en haut à droite de RStudio et en sélectionnant “Nouveau projet…”\n\n\n\n\n\n\n\nFigure 1.13: Créer un Projet R Studio étape 2\n\n\n\n\nDans la fenêtre suivante, sélectionnez “Nouveau projet”.\n\n\n\n\n\n\n\nFigure 1.14: Créer un Projet R Studio étape 3\n\n\n\n\nSaisissez le nom du répertoire que vous souhaitez créer dans le champ “Nom du répertoire :” (nous l’appellerons premier_projet pour ce chapitre). Si vous souhaitez modifier l’emplacement du répertoire sur votre ordinateur, cliquez sur le bouton “Parcourir…” et naviguez jusqu’à l’endroit où vous souhaitez créer le répertoire. Nous cochons toujours la case “Ouvrir dans une nouvelle session”. Enfin, cliquez sur le bouton “Créer un projet” pour créer le nouveau projet.\n\n\n\n\n\n\n\nFigure 1.15: Créer un Projet R Studio étape 4\n\n\n\n\nUne fois votre nouveau projet créé, vous disposerez d’un nouveau dossier sur votre ordinateur contenant un fichier de projet RStudio appelé premier_projet.Rproj. Ce projet .Rproj contient diverses options de projet (mais vous ne devriez pas vraiment interagir avec lui) et peut également être utilisé comme raccourci pour ouvrir le projet directement à partir du système de fichiers (il suffit de double-cliquer dessus). Vous pouvez le vérifier dans l’onglet “Fichiers” de RStudio (ou dans Finder si vous êtes sur Mac ou dans l’Explorateur de fichiers sous Windows).\n\n\n\n\n\n\n\nFigure 1.16: Créer un Projet R Studio étape finale\n\n\n\n\nLa dernière chose que nous vous suggérons de faire est de sélectionner Outils –&gt; Options du Project... dans le menu. Cliquez sur l’onglet “Général” sur le côté gauche et modifiez les valeurs pour “Restaurer .RData dans l’espace de travail au démarrage” et “Sauvegarder l’espace de travail dans .RData à la sortie” de “Par défaut” à “Non”. Cela garantit que chaque fois que vous ouvrez votre projet, vous démarrez avec une session R propre. Vous n’êtes pas obligé de faire cela (beaucoup de gens ne le font pas), mais nous préférons commencer avec un espace de travail complètement propre chaque fois que nous ouvrons nos projets pour éviter tout conflit potentiel avec des choses que nous avons faites dans des sessions précédentes (ce qui conduit parfois à des résultats surprenants et à des maux de tête pour résoudre le problème). L’inconvénient est que vous devrez réexécuter votre code R à chaque fois que vous ouvrirez votre projet.\n\n\n\n\n\n\n\nFigure 1.17: Créer un Projet R Studio changement d’options\n\n\n\n\nMaintenant que vous avez mis en place un projet RStudio, vous pouvez commencer à créer des scripts R (ou des documents R markdown /Quarto, voir Chapitre 6) ou tout ce dont vous avez besoin pour compléter votre projet. Tous les scripts R seront désormais contenus dans le projet RStudio et enregistrés dans le dossier du projet.\n\n1.5.2 VSCode\nLes espaces de travail sont similaires aux projets RStudio. Vous devez cependant créer un nouveau dossier avec un fichier R (ou un fichier texte) et l’enregistrer en tant qu’espace de travail.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-file-names",
    "href": "01-debut.html#sec-file-names",
    "title": "1  Pour commencer",
    "section": "\n1.6 Nom des fichiers",
    "text": "1.6 Nom des fichiers\nLe nom que vous donnez à vos fichiers a plus d’importance que vous ne le pensez. Nommer les fichiers est également plus difficile que vous ne le pensez. Pour qu’un nom de fichier soit “bon”, il faut qu’il soit informatif et relativement court. Ce n’est pas toujours un compromis facile et il faut souvent y réfléchir. L’idéal est d’éviter les éléments suivants !\n\n\n\n\n\n\n\nFigure 1.18: File renaming song (source:https://xkcd.com/1459/)\n\n\n\n\nBien qu’il n’y ait pas vraiment d’approche standard reconnue pour nommer les fichiers (en fait il y en a une mais tout le monde ne l’utilise pas), il y a quelques points à garder à l’esprit.\n\nÉvitez d’utiliser des espaces dans les noms de fichiers en les remplaçant par des traits de soulignement ou des tirets. Pourquoi cela est-il important ? L’une des raisons est que certains logiciels de ligne de commande (en particulier de nombreux outils bioinformatiques) ne reconnaissent pas un nom de fichier comportant un espace et que vous devez vous livrer à toutes sortes de manigances en utilisant des caractères d’échappement pour vous assurer que les espaces sont traités correctement. Même si vous ne pensez pas utiliser un jour un logiciel de ligne de commande, il se peut que vous le fassiez indirectement. Prenez R markdown par exemple, si vous voulez rendre un document R markdown au format pdf en utilisant le paquet rmarkdown 📦 vous utiliserez en fait une ligne de commande \\(\\LaTeX\\) sous le capot. Une autre bonne raison de ne pas utiliser d’espaces dans les noms de fichiers est que cela rend la recherche de noms de fichiers (ou de parties de noms de fichiers) à l’aide d’expressions régulières dans R (ou tout autre langage) beaucoup plus difficile.\nÉvitez d’utiliser des caractères spéciaux (par exemple @£$%^&*(:/)) dans vos noms de fichiers.\nSi vous créez des versions de vos fichiers à l’aide de nombres séquentiels (par ex. fichier1, fichier2, fichier3 …). Si vous prévoyez d’avoir plus de 9 fichiers, vous devez utiliser 01, 02, 03, …, 10 afin de garantir que les fichiers soient listés dans le bon ordre. Si vous prévoyez d’avoir plus de 99 fichiers, utilisez 001, 002, 003, ….\nPour les dates, utilisez le format ISO 8601 AAAA-MM-JJ (ou AAAAMMJJ) afin de vous assurer que vos fichiers sont listés dans un ordre chronologique correct.\nN’utilisez jamais le mot final dans un nom de fichier - il est extrêmement rare qu’il le soit !\n\nQuelle que soit la convention de dénomination des fichiers que vous décidez d’utiliser, essayez de l’adopter rapidement, de vous y tenir et d’être cohérent.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-proj-doc",
    "href": "01-debut.html#sec-proj-doc",
    "title": "1  Pour commencer",
    "section": "\n1.7 Documentation du script",
    "text": "1.7 Documentation du script\nUn petit mot sur l’écriture du code R et la création de scripts R. À moins que vous ne fassiez quelque chose de vraiment rapide et sale, nous vous suggérons de toujours écrire votre code R sous la forme d’un script R. Les scripts R sont ce qui permet à R d’être plus efficace. Les scripts R sont ce qui rend R si utile. Non seulement vous disposez d’un enregistrement complet de votre analyse, de la manipulation des données à la visualisation et à l’analyse statistique, mais vous pouvez également partager ce code (et ces données) avec vos amis, vos collègues et, surtout, lorsque vous soumettez et publiez votre recherche dans une revue. Dans cette optique, veillez à inclure dans votre script R toutes les informations nécessaires pour rendre votre travail reproductible (noms des auteurs, dates, plan d’échantillonnage, etc.). Ces informations peuvent être incluses sous la forme d’une série de commentaires # ou, mieux encore, en mélangeant code exécutable et narration dans un document R markdown (Chapitre 6). C’est aussi une bonne pratique d’inclure la sortie de la fonction sessionInfo() à la fin de n’importe quel script qui affiche la version de R, les détails du système d’exploitation et les paquets chargés. Une très bonne alternative est d’utiliser la fonction session_info() du paquet xfun 📦 pour un résumé plus concis de notre environnement de session.\nVoici un exemple d’inclusion de méta-informations au début d’un script R\n# Titre: Analyse de séries temporelles de consommation de lasagnes\n\n# Objectif : Ce script effectue une analyse de série temporelle sur\n#           les plats de lasagnes que les enfants veulent manger chaque semaine.\n#           Les données sont des dénombrements de plats de lasagnes (rêvés) par semaine\n#           collectés à partir de 24 enfants à l'école \"Repas-de-rêve\"\n#           entre 2042 et 2056.\n\n# fichier de données: lasagna_dreams.csv\n\n# Auteur: Un. Estomac\n# Coordonnées de contact: un.estomac@univ.repas.com\n\n# Date de création du script: Fri Mar 29 17:06:44 2010 -----------------\n# Date de dernière modification du script: Wed Sep 18 12:14:18 2024 ----\n\n# paquets chargés\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nprint(\"écrivez votre merveilleux code R ici\")\n\n# bonne pratique pour inclure des informations sur la session\n\nxfun::session_info()\nIl ne s’agit que d’un exemple et il n’y a pas de règles strictes, alors n’hésitez pas à développer un système qui vous convienne. Un raccourci très utile dans RStudio est d’inclure automatiquement un horodatage dans votre script R. Pour ce faire, écrivez ts à l’endroit où vous souhaitez insérer votre horodatage dans votre script R, puis appuyez sur les touches ‘shift + tab’. RStudio convertira ts en date et heure actuelles et commentera automatiquement cette ligne avec un #. Un autre raccourci très utile de RStudio consiste à commenter plusieurs lignes de votre script avec le symbole #. Pour ce faire, sélectionnez les lignes de texte que vous souhaitez commenter et appuyez sur ‘ctrl + shift + c’ (ou ‘cmd + shift + c’ sur un mac). Pour décommenter les lignes, utilisez à nouveau ‘ctrl + shift + c’.\nEn plus d’inclure des métadonnées dans vos scripts R, il est également courant de créer un fichier texte séparé pour enregistrer les informations importantes. Par convention, ces fichiers texte sont nommés README. Nous incluons souvent un fichier README dans le répertoire où nous conservons nos données brutes. Dans ce fichier, nous indiquons la date à laquelle les données ont été collectées (ou téléchargées), la manière dont elles ont été collectées, des informations sur l’équipement spécialisé, les méthodes de conservation, le type et la version des machines utilisées (c’est-à-dire l’équipement de séquençage), etc. Vous pouvez créer un fichier README pour votre projet dans RStudio en cliquant sur le menu Fichier –&gt; Nouveau fichier –&gt; Fichier texte.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#guide-de-style-r",
    "href": "01-debut.html#guide-de-style-r",
    "title": "1  Pour commencer",
    "section": "\n1.8 Guide de style R",
    "text": "1.8 Guide de style R\nLa façon dont vous écrivez votre code dépend plus ou moins de vous, bien que votre objectif soit de le rendre aussi facile à lire que possible (pour vous et pour les autres). Bien qu’il n’y ait pas de règles (ni de police du code), nous vous encourageons à prendre l’habitude d’écrire un code R lisible en adoptant un style particulier. Nous vous suggérons de suivre le R style guide de Google dans la mesure du possible. Ce guide de style vous aidera à décider où utiliser les espaces, comment indenter le code et comment utiliser les crochets [ ] et les parenthèses bouclées { }, entre autres choses.\nPour vous aider à formater le code :\n\nVSCode il y a un formateur intégré dans l’extension R pour VSCode. Il vous suffit d’utiliser le raccourci clavier pour reformater le code de manière agréable et automatique.\nPour RStudio, vous pouvez installer le paquet styler 📦 qui inclut une extension RStudio vous permettant de reformater automatiquement le code sélectionné (ou des fichiers et projets entiers) d’un simple clic de souris. Vous pouvez trouver plus d’informations sur le paquet styler 📦, y compris comment l’installer ici . Une fois installé, vous pouvez sélectionner le code que vous souhaitez remodeler, cliquer sur le bouton “Extension” en haut de RStudio et sélectionner l’option “Style Selection”. Voici un exemple de code R mal formaté\n\n\n\n\n\n\n\n\nFigure 1.19: Code mal formaté\n\n\n\n\nMettez maintenant le code en surbrillance et utilisez le paquet styler 📦 pour reformater\n\n\n\n\n\n\n\nFigure 1.20: Structurer le code avec styler\n\n\n\n\nPour produire un code joliment formaté\n\n\n\n\n\n\n\nFigure 1.21: Code bien structuré",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sauvegarde-des-projets",
    "href": "01-debut.html#sauvegarde-des-projets",
    "title": "1  Pour commencer",
    "section": "\n1.9 Sauvegarde des projets",
    "text": "1.9 Sauvegarde des projets\nNe soyez pas la personne qui perd des données et des analyses durement acquises (et souvent coûteuses). Ne soyez pas cette personne qui pense que cela ne m’arrivera jamais - ça vous arrivera ! Pensez toujours au pire scénario, à quelque chose qui vous donnera des sueurs froides la nuit, et faites tout ce qui est en votre pouvoir pour que cela n’arrive jamais. Pour être clair, si vous comptez copier vos précieux fichiers sur un disque dur externe ou une clé USB, ce n’est PAS une stratégie de sauvegarde efficace. Ces objets tombent souvent en panne lorsque vous les glissez dans votre sac à dos ou votre “sac de tous les jours” et que vous les trimballez entre votre bureau et votre domicile. Même si vous les laissez branchés sur votre ordinateur, que se passe-t-il lorsque le bâtiment brûle (nous avons bien dit le pire des cas !)?\nIdéalement, vos sauvegardes devraient être hors site et incrémentielles. Heureusement, il existe de nombreuses options pour sauvegarder vos fichiers. La première chose à faire est de chercher dans votre propre institut. La plupart (toutes ?) des universités disposent d’une forme de stockage en réseau qui devrait être facilement accessible et qui est également étayée par un plan complet de reprise après sinistre. D’autres options incluent des services basés sur le cloud tels que Google Drive et Dropbox (pour n’en citer que quelques-uns), mais assurez-vous que vous ne stockez pas de données sensibles sur ces services et que vous êtes à l’aise avec les politiques de confidentialité souvent exorbitantes.\nSi ces services sont très efficaces pour stocker des fichiers, ils ne sont pas vraiment utiles pour les sauvegardes incrémentielles. Pour retrouver les versions précédentes des fichiers, il faut souvent passer un temps fou à parcourir plusieurs fichiers nommés ‘final.doc’, ‘final_v2.doc’ et ‘final_utiliseceluila.doc’ etc. jusqu’à ce que vous trouviez celui que vous cherchiez. Le meilleur moyen que nous connaissons pour sauvegarder des fichiers et gérer différentes versions de fichiers est d’utiliser Git et GitHub. Pour en savoir plus sur la façon dont vous pouvez utiliser RStudio, Git et GitHub ensemble, consultez Chapitre 7.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#citer-r-et-les-paquets-r",
    "href": "01-debut.html#citer-r-et-les-paquets-r",
    "title": "1  Pour commencer",
    "section": "\n1.10 Citer R et les paquets R",
    "text": "1.10 Citer R et les paquets R\nDe nombreuses personnes ont investi énormément de temps et d’énergie pour faire de R l’excellent logiciel que vous utilisez aujourd’hui. Si vous utilisez R dans votre travail (et nous espérons que vous le ferez), n’oubliez pas de citer non seulement R, mais aussi tous les paquets que vous avez utilisés. Pour obtenir la citation la plus récente pour R, vous pouvez utiliser la fonction citation().\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nSi vous souhaitez citer un package particulier que vous avez utilisé pour votre analyse de données, vous pouvez également utiliser la fonction citation() pour obtenir l’information.\n\ncitation(package = \"here\")\n\nTo cite package 'here' in publications use:\n\n  Müller K (2020). _here: A Simpler Way to Find Your Files_. R package\n  version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {here: A Simpler Way to Find Your Files},\n    author = {Kirill Müller},\n    year = {2020},\n    note = {R package version 1.0.1},\n    url = {https://CRAN.R-project.org/package=here},\n  }\n\n\nSelon nous, l’outil le plus utile pour la citation est le paquet grateful 📦 qui vous permet de générer les informations de citation dans un fichier, ainsi que de créer une phrase ou un tableau citant tous les paquets utilisés. Cela devrait devenir la norme dans tout manuscrit honnêtement. Voir Table 1 pour un exemple de résultat produit avec grateful.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "02-base.html",
    "href": "02-base.html",
    "title": "2  Quelques notions de base sur R",
    "section": "",
    "text": "2.1 Considérations importantes\nLes captures d’écrans présentées proviennent de RStudio mais tout est très similaire sur VSCode.\nAvant de poursuivre, quelques points à garder à l’esprit tout au long de ce chapitre :",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#considérations-importantes",
    "href": "02-base.html#considérations-importantes",
    "title": "2  Quelques notions de base sur R",
    "section": "",
    "text": "R est sensible à la casse, c’est-à-dire que A n’est pas la même chose que a et anova, ce n’est pas Anova.\nTout ce qui suit un # est interprété comme un commentaire et ignoré par R. Ces commentaires doivent être utilisés librement dans votre code, à la fois pour votre propre information et pour aider vos collaborateurs. L’écriture de commentaires est un peu un art que vous maîtriserez de mieux en mieux avec l’expérience.\nDans R, les commandes sont généralement séparées par une nouvelle ligne. Vous pouvez également utiliser un point-virgule ; pour séparer vos commandes, mais nous vous le déconseillons fortement (rend le code très difficilement lisible).\nSi une invite de continuation + apparaît dans la console après l’exécution de votre code, cela signifie que vous n’avez pas terminé votre code correctement. Cela se produit souvent lorsque vous oubliez de fermer une parenthèse, ce qui est particulièrement fréquent lors que l’on utilise des parenthèses imbriquées ((((commande quelconque))). Terminez simplement la commande sur la nouvelle ligne ou appuyez sur la touche “escape” de votre clavier (voir le point ci-dessous) et corrigez la faute de frappe.\nEn général, R est assez tolérant vis-à-vis des espaces supplémentaires insérés dans votre code, en fait l’utilisation d’espaces est activement encouragée. Cependant, les espaces ne doivent pas être insérés dans les opérateurs, c’est-à-dire &lt;- ne peut pas s’écrire &lt; - (notez l’espace). Voir le guide de style pour savoir où placer les espaces afin de rendre votre code plus lisible.\nSi votre console se bloque et ne répond plus après l’exécution d’une commande, vous pouvez souvent vous sortir d’affaire en appuyant sur la touche “escape” (esc) de votre clavier ou en cliquant sur l’icône d’arrêt/stop en haut à droite de votre console. Cela mettra fin à la plupart des opérations en cours.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#première-étape-dans-la-console",
    "href": "02-base.html#première-étape-dans-la-console",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.2 Première étape dans la console",
    "text": "2.2 Première étape dans la console\nDans le Chapitre 1, nous avons appris ce qu’était la console R la création de scripts et de Projets. Nous avons également vu comment écrire votre code R dans un script, puis comment insérer ce code dans la console pour qu’il s’exécute (si vous avez oublié comment faire, revenez à la section sur la console (-Section 1.2.1.1) pour vous rafraîchir la mémoire). Le fait d’écrire votre code dans un script signifie que vous aurez un enregistrement permanent de tout ce que vous avez fait (à condition de sauvegarder votre script) et vous permet également de faire de nombreux commentaires pour vous rappeler ce que vous aviez fait (ou voulu faire) quand vous retournerez si votre code à l’avenir. Ainsi, pendant que vous travaillez sur ce chapitre, nous vous suggérons de créer un nouveau script (ou un Projet Rstudio) pour écrire votre code au fur et à mesure.\nComme nous l’avons vu au Chapitre 1, nous pouvons utiliser R de la même manière qu’une calculatrice. Nous pouvons saisir une expression arithmétique dans notre script, puis l’envoyer dans la console et recevoir un résultat. Par exemple, si nous tapons l’expression 1 + 1 et que l’on exécute cette ligne de code dans la console, on obtient la réponse 2 (😃 !)\n\n1 + 1\n\n[1] 2\n\n\nLe [1] devant le résultat indique que l’observation au début de la ligne est la première. Cela n’est pas très utile dans cet exemple, mais peut l’être lors de l’impression de résultats sur plusieurs lignes (nous en verrons un exemple ci-dessous). Les autres opérateurs arithmétiques évidents sont -, *, / pour la soustraction, la multiplication et la division respectivement. Pour la multiplication de la matrice, l’opérateur est %*%.\nR suit la convention mathématique habituelle de l’ordre des opérations. Par exemple, l’expression 2 + 3 * 4 est interprétée comme ayant la valeur 2 + (3 * 4) = 14 et non (2 + 3) * 4 = 20. Il existe un grand nombre de fonctions mathématiques dans R, dont les plus utiles sont les suivantes : log(), log10(), exp(), sqrt().\n\nlog(1) # logarithme en base e\n\n[1] 0\n\nlog10(1) # logarithme en base 10\n\n[1] 0\n\nexp(1) # antilog naturel, fonction exponentielle\n\n[1] 2.718282\n\nsqrt(4) # racine carrée\n\n[1] 2\n\n4^2 # 4 puissance 2\n\n[1] 16\n\npi # pas une fonction mais utile\n\n[1] 3.141593\n\n\nIl est important de comprendre que lorsque vous exécutez un code comme nous l’avons fait ci-dessus, le résultat du code (ou valeur) n’est affiché que dans la console. Bien que cela puisse parfois être utile, il est généralement beaucoup plus pratique de stocker la ou les valeurs dans un objet.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#objets-en-r",
    "href": "02-base.html#objets-en-r",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.3 Objets en R",
    "text": "2.3 Objets en R\nAu cœur de presque tout ce que vous ferez (ou ferez probablement) en R se trouve le concept selon lequel tout en R est un objet. Ces objets peuvent être pratiquement n’importe quoi, d’un simple nombre ou d’une chaîne de caractères (comme un mot) à des structures très complexes comme la sortie d’un graphique, un résumé de votre analyse statistique ou un ensemble de commandes R effectuant une tâche spécifique. Pour comprendre R, il est essentiel de savoir comment créer des objets et leur attribuer des valeurs.\n\n2.3.1 Création d’objets\nPour créer un objet, il suffit de lui donner un nom. Nous pouvons ensuite attribuer une valeur à cet objet à l’aide d’un opérateur d’affectation &lt;- (parfois appelé opérateur d’obtention). L’opérateur d’affectation est un symbole composite composé d’un symbole “moins que” &lt; et d’un trait d’union -\n\nraccourci clavier : “option” + “-” sur Mac ; “alt” + “-” sur Windows.\n\n\nmon_obj &lt;- 32\n\nDans le code ci-dessus, nous avons créé un objet appelé mon_obj et lui avons attribué la valeur numérique 32 à l’aide de l’opérateur d’affectation (dans notre tête, nous lisons toujours cela comme ‘mon_obj est 32’). Vous pouvez également utiliser = à la place de &lt;- pour assigner des valeurs, mais c’est une mauvaise pratique car cela peut entraîner des confusions plus tard quand vous programmerez en R (voir Chapitre 5) donc nous vous déconseillons d’utiliser cette notation.\nPour afficher la valeur de l’objet, il suffit de taper son nom.\n\nmon_obj\n\n[1] 32\n\n\nMaintenant que nous avons créé cet objet, R le connaît et en gardera la trace pendant la session R en cours. Tous les objets que vous créez sont stockés dans l’espace de travail actuel et vous pouvez visualiser tous les objets de votre espace de travail dans RStudio en cliquant sur l’onglet “Environnement” dans le volet supérieur droit.\n\n\n\n\n\n\n\nFigure 2.1: Onglet Environnement RStudio\n\n\n\n\nSi vous cliquez sur la flèche vers le bas de l’icône “List” (Liste) dans le même volet et que vous passez à l’affichage “Grid” (Grille), RStudio vous présentera un résumé des objets, y compris le type (“numeric” (numérique) - c’est un nombre), la longueur (une seule valeur dans cet objet), sa taille “physique” et sa valeur (32 dans ce cas). Dans VSCode, allez sur le panneau d’extension R et vous obtiendrez les mêmes informations.\n\n\n\n\n\n\n\nFigure 2.2: Onglet Environnement RStudio au format grille\n\n\n\n\nIl existe de nombreux types de valeurs que vous pouvez attribuer à un objet. Par exemple\n\nmon_obj2 &lt;- \"R c'est trop bien\"\n\nNous avons créé un objet appelé mon_obj2 et lui avons attribué la valeur R c'est trop bien qui est une chaîne de caractères. Remarquez que nous avons mis la chaîne de caractères entre guillemets. Si vous oubliez d’utiliser les guillemets, vous recevrez un message d’erreur.\nNotre espace de travail contient maintenant les deux objets que nous avons créés jusqu’à présent avec mon_obj2 de type “character” (caractère).\n\n\n\n\n\n\n\nFigure 2.3: Onglet Environnement RStudio avec mon_obj2 de type caractère\n\n\n\n\nPour modifier la valeur d’un objet existant, il suffit de lui réattribuer une nouvelle valeur. Par exemple, pour modifier la valeur de mon_obj2 de \"R c'est trop bien\" au nombre 1024\n\nmon_obj2 &lt;- 1024\n\nRemarquez que le type est devenu numérique et que la valeur est passée à 1024 dans l’environnement.\n\n\n\n\n\n\n\nFigure 2.4: Onglet Environnement RStudio avec mon_obj2 mis-à-jours en numérique\n\n\n\n\nUne fois que nous avons créé plusieurs objets, nous pouvons faire des choses avec. Par exemple, le code suivant crée un nouvel objet mon_obj3 et lui assigne la valeur de mon_obj ajouté à mon_obj2 soit 1056 (32 + 1024 = 1056).\n\nmon_obj3 &lt;- mon_obj + mon_obj2\nmon_obj3\n\n[1] 1056\n\n\nRemarquez que pour afficher la valeur de mon_obj3 nous devons également écrire le nom de l’objet. Le code ci-dessus fonctionne parce que les valeurs de mon_obj et mon_obj2 sont numériques (donc des nombres). Si vous essayez de faire ça avec des objets dont les valeurs sont des caractères (classe character), vous recevrez une erreur\n\nchar_obj &lt;- \"hello\"\nchar_obj2 &lt;- \"world!\"\nchar_obj3 &lt;- char_obj + char_obj2\n# Error in char_obj+char_obj2:non-numeric argument to binary operator\n\nLe message d’erreur vous indique que l’un ou les deux objets char_obj et char_obj2 n’est pas un nombre et ne peut donc pas être additionné.\nLorsque vous commencez à apprendre R, la gestion des erreurs et des avertissements peut être frustrante car ils sont souvent difficiles à comprendre (qu’est-ce qu’un argument ? qu’est-ce qu’un opérateur binaire ?). Une façon de trouver plus d’informations sur une erreur particulière est de rechercher une version généralisée du message d’erreur. Pour l’erreur ci-dessus, essayez de rechercher ‘non-numeric argument to binary operator error + r’ ou même ‘common r error messages’.\nUn autre message d’erreur que vous obtiendrez assez souvent lorsque vous commencerez à utiliser R est Error : object 'XXX' not found (erreur : objet ‘XXX’ non trouvé). A titre d’exemple, regardez le code ci-dessous\nmon_obj &lt;- 48\nmon_obj4 &lt;- mon_obj + no_obj\n# Error: object 'no_obj' not found\nR renvoie un message d’erreur parce que nous n’avons pas encore créé (défini) l’objet no_obj. Un autre indice qu’il y a un problème avec ce code est que, si vous vérifiez votre environnement, vous verrez que l’objet mon_obj4 n’a pas été créé.\n\n2.3.2 Nommer les objets\nNommer vos objets est l’une des choses les plus difficiles que vous ferez dans R. Idéalement, les noms de vos objets devraient être courts et informatifs, ce qui n’est pas toujours facile. Si vous devez créer des objets avec plusieurs mots dans leur nom, utilisez un trait de soulignement _ ou un point . entre les mots ou mettez les différents mots en majuscules. Nous préférons le format souligné _ et n’incluons jamais de majuscules dans les noms (appelé snake_case).\nresume_sortie &lt;- \"mon analyse\" # recommandé #\nresume.sortie &lt;- \"mon analyse\"\nresumeSortie &lt;- \"mon analyse\"\nIl y a également quelques limitations lorsqu’il s’agit de donner des noms aux objets. Un nom d’objet ne peut pas commencer par un chiffre ou un point suivi d’un chiffre (ex. 2ma_variable ou .2ma_variable). Vous devez également éviter d’utiliser des caractères non alphanumériques ou des accents dans vos noms d’objets (i.e. &, ^, /, !, é, è, etc). De plus, assurez-vous de ne pas nommer vos objets avec des mots réservés (i.e. TRUE, NA) et ce n’est jamais une bonne idée de donner à votre objet le même nom qu’une fonction intégrée. Une fonction qui revient plus souvent qu’on ne peut s’en souvenir est :\ndata &lt;- read.table(\"monfichierdedonees\", header = TRUE)\nOui, data() est une fonction de R qui permet de charger ou de lister les ensembles de données disponibles dans les paquets.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#sec-funcs",
    "href": "02-base.html#sec-funcs",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.4 Utilisation de fonctions dans R",
    "text": "2.4 Utilisation de fonctions dans R\nJusqu’à présent, nous avons créé des objets simples en assignant directement une valeur unique à un objet. Il est très probable que vous souhaitiez bientôt créer des objets plus compliqués au fur et à mesure que vous aurez de l’expérience sur R et que la complexité de vos tâches augmente. Heureusement, R dispose d’une multitude de fonctions pour vous aider à le faire. Vous pouvez considérer une fonction comme un objet qui contient une série d’instructions pour effectuer une tâche spécifique. L’installation de base de R est livrée avec de nombreuses fonctions déjà définies ou vous pouvez augmenter la puissance de R en installant l’un des 10 000 paquets actuellement disponibles. Une fois que vous aurez acquis un peu plus d’expérience dans l’utilisation de R, vous voudrez peut-être définir vos propres fonctions pour effectuer des tâches spécifiques à vos objectifs (plus d’informations à ce sujet dans Chapitre 5).\nLa première fonction que nous allons découvrir est la fonction c(). La fonction c() est l’abréviation de concaténer et nous l’utilisons pour joindre une série de valeurs et les stocker dans une structure de données appelée vecteur (plus d’informations sur les vecteurs dans Chapitre 3).\n\nmon_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\n\nDans le code ci-dessus, nous avons créé un objet appelé mon_vec et lui avons assigné une valeur en utilisant la fonction c(). Il y a quelques points très importants à noter ici. Premièrement, lorsque vous utilisez une fonction dans R, le nom de la fonction est toujours suivi d’une paire de parenthèses rondes (), même s’il n’y a rien entre les parenthèses. Deuxièmement, les arguments d’une fonction sont placés à l’intérieur des parenthèses rondes () et sont séparés par des virgules ,. Vous pouvez considérer un argument comme un moyen de personnaliser l’utilisation ou le comportement d’une fonction. Dans l’exemple ci-dessus, les arguments sont les nombres que nous voulons concaténer. Enfin, l’une des choses les plus délicates lorsque vous commencez à utiliser R est de savoir quelle fonction utiliser pour une tâche particulière et comment l’utiliser. Heureusement, chaque fonction est toujours associée à un document d’aide qui explique comment utiliser la fonction (plus d’informations à ce sujet plus tard Section 2.6) et une recherche rapide sur le web peut également vous aider.\nPour examiner la valeur de notre nouvel objet, nous pouvons simplement taper le nom de l’objet comme nous l’avons fait précédemment\n\nmon_vec\n\n[1] 2 3 1 6 4 3 3 7\n\n\nMaintenant que nous avons créé un vecteur, nous pouvons utiliser d’autres fonctions pour faire des choses utiles avec cet objet. Par exemple, nous pouvons calculer la moyenne, la variance, l’écart-type et le nombre d’éléments de notre vecteur en utilisant les fonctions mean(), var(), sd() et length().\n\nmean(mon_vec) # renvoie la moyenne de mon_vec\n\n[1] 3.625\n\nvar(mon_vec) # renvoie la variance de mon_vec\n\n[1] 3.982143\n\nsd(mon_vec) # renvoie l'écart-type de mon_vec\n\n[1] 1.995531\n\nlength(mon_vec) # renvoie le nombre d'éléments dnas mon_vec\n\n[1] 8\n\n\nSi nous voulons utiliser l’une de ces valeurs plus tard dans notre analyse, il nous suffit d’affecter la valeur obtenue à un autre objet.\n\nmoyenne_vec &lt;- mean(mon_vec) # renvoie la moyenne de mon_vec\nmoyenne_vec\n\n[1] 3.625\n\n\nIl peut parfois être utile de créer un vecteur contenant une séquence régulière de valeurs par pas de un. Dans ce cas, nous pouvons utiliser un raccourci en utilisant le symbole :.\n\nma_seq &lt;- 1:10 # créer une séquence régulière\nma_seq\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nma_seq2 &lt;- 10:1 # en ordre décroissant\nma_seq2\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nD’autres fonctions utiles pour générer des vecteurs de séquences sont seq() et rep(). Par exemple, pour générer une séquence de 1 à 5 par pas de 0,5 :\n\nma_seq2 &lt;- seq(from = 1, to = 5, by = 0.5)\nma_seq2\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nIci, nous avons utilisé les arguments from = et to = pour définir les limites de la séquence et l’argument by = pour spécifier l’incrément (les pas) de la séquence. Jouez avec d’autres valeurs pour ces arguments afin de voir leur effet.\nL’argument rep() vous permet de répliquer (répéter) des valeurs un certain nombre de fois. Pour répéter la valeur ‘2’, 10 fois :\n\nma_seq3 &lt;- rep(2, times = 10) # répète '2', 10 fois\nma_seq3\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\n\nVous pouvez également répéter des valeurs non numériques :\n\nma_seq4 &lt;- rep(\"abc\", times = 3) # répète ‘abc' 3 fois\nma_seq4\n\n[1] \"abc\" \"abc\" \"abc\"\n\n\nou chaque élément d’une série :\n\nma_seq5 &lt;- rep(1:5, times = 3) # répète la série de '1' à '5', 3 fois\nma_seq5\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n\nou des éléments d’une série :\n\nma_seq6 &lt;- rep(1:5, each = 3) # répète chaque élément de la série 3 fois\nma_seq6\n\n [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\n\nOn peut aussi répéter une série non séquentielle :\n\nma_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3) # répète chaque élément de la série 3 fois\nma_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7\n\n\nNotez dans le code ci-dessus comment nous avons utilisé la fonction c() à l’intérieur de la fonction rep(). L’imbrication de fonctions nous permet de construire des commandes assez complexes à l’intérieur d’une seule ligne de code et est une pratique très courante dans l’utilisation de R. Cependant, il faut faire attention car trop de fonctions imbriquées peuvent rendre votre code difficile à comprendre pour les autres (et pour vous-même dans le futur !). Nous pourrions réécrire le code ci-dessus pour séparer explicitement les deux étapes de la génération de notre vecteur. L’une ou l’autre approche donnera le même résultat, il vous suffit d’utiliser votre propre jugement pour déterminer laquelle est la plus lisible.\n\nvec_int &lt;- c(3, 1, 10, 7)\nma_seq7 &lt;- rep(vec_int, each = 3) # répète chaque élément de la série, 3 fois\nma_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#sec-vectors",
    "href": "02-base.html#sec-vectors",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.5 Travailler avec des vecteurs",
    "text": "2.5 Travailler avec des vecteurs\nManipuler, résumer et trier des données à l’aide de R est une compétence importante à maîtriser, mais que de nombreuses personnes trouvent un peu déroutante au début. Nous allons voir ici quelques exemples simples utilisant des vecteurs pour illustrer certains concepts importants, mais nous développerons cela plus en détail dans Chapitre 3 où nous verrons des structures de données plus compliquées (et plus utiles).\n\n2.5.1 Extraction d’éléments\nPour extraire (ou indexer ou souscrire) une ou plusieurs valeurs (plus généralement appelées éléments) d’un vecteur, nous utilisons les crochets [ ]. L’approche générale consiste à nommer l’objet à extraire, puis écrire l’indice de l’élément à extraire dans les crochets. Cet indice peut être une position ou le résultat d’un test logique.\nIndice de position\nPour extraire des éléments en fonction de leur position, il suffit d’écrire la position à l’intérieur des crochets [ ]. Par exemple, pour extraire la 3e valeur de mon_vec :\n\nmon_vec # rappelons-nous à quoi mon_vec ressemble\n\n[1] 2 3 1 6 4 3 3 7\n\nmon_vec[3] # extrait la 3e valeur\n\n[1] 1\n\n# si vous voulez stocker cette valeur dans un autre objet\nval_3 &lt;- mon_vec[3]\nval_3\n\n[1] 1\n\n\nNotez que l’indice de position commence à 1 et non à 0 comme dans d’autres langages de programmation (i.e. Python).\nNous pouvons également extraire plusieurs valeurs en utilisant la fonction c() à l’intérieur des crochets. Ici, nous extrayons le 1er, le 5e, le 6e et le 8e élément de l’objet mon_vec :\n\nmon_vec[c(1, 5, 6, 8)]\n\n[1] 2 4 3 7\n\n\nNous pouvons également extraire une plage de valeurs à l’aide de la fonction :. Pour extraire du 3e au 8e élément :\n\nmon_vec[3:8]\n\n[1] 1 6 4 3 3 7\n\n\n\n2.5.1.1 Indice logique\nUne autre façon très utile d’extraire des données d’un vecteur est d’utiliser une expression logique comme indice. Par exemple, pour extraire tous les éléments dont la valeur est supérieure à 4 dans le vecteur mon_vec :\n\nmon_vec[mon_vec &gt; 4]\n\n[1] 6 7\n\n\nIci, l’expression logique est mon_vec &gt; 4 et R n’extraira que les éléments qui satisfont à cette condition logique. Comment cela fonctionne-t-il réellement ? Si nous regardons la sortie de l’expression logique sans les crochets, vous pouvez voir que R renvoie un vecteur contenant soit TRUE soit FALSE qui indique si la condition logique est remplie pour chaque élément. Dans ce cas, seuls les éléments en 4e et 8e position renvoient un TRUE car leur valeur est supérieure à 4.\n\nmon_vec &gt; 4\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\nAinsi, ce que R fait en réalité sous le capot est équivalent à :\n\nmon_vec[c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE)]\n\n[1] 6 7\n\n\net seuls les éléments qui sont TRUE seront extraits.\nEn plus de &lt; et &gt; vous pouvez également utiliser des opérateurs composites pour augmenter la complexité de vos expressions. Par exemple, l’expression “supérieur ou égal à” est : &gt;=. Pour vérifier si une valeur est égale à une autre, nous devons utiliser un double symbole égal == et pour vérifier si une valeur est différente de, nous utilisons le symbole != (le symbole ! signifie “pas”).\n\nmon_vec[mon_vec &gt;= 4] # valeurs supérieures ou égales à 4\n\n[1] 6 4 7\n\nmon_vec[mon_vec &lt; 4] # valeurs inférieures à 4\n\n[1] 2 3 1 3 3\n\nmon_vec[mon_vec &lt;= 4] # valeurs inférieures ou égales à 4\n\n[1] 2 3 1 4 3 3\n\nmon_vec[mon_vec == 4] # valeurs égales à 4\n\n[1] 4\n\nmon_vec[mon_vec != 4] # valeurs pas égales à 4\n\n[1] 2 3 1 6 3 3 7\n\n\nNous pouvons également combiner plusieurs expressions logiques à l’aide d’expressions booléennes. Dans R, l’élément & signifie ET et le symbole | signifie OU. Par exemple, pour extraire des valeurs dans mon_vec qui sont inférieures à 6 ET supérieures à 2 :\n\nval26 &lt;- mon_vec[mon_vec &lt; 6 & mon_vec &gt; 2]\nval26\n\n[1] 3 4 3 3\n\n\nou extraire des valeurs dans mon_vec qui sont supérieures à 6 OU inférieures à 3 :\n\nval63 &lt;- mon_vec[mon_vec &gt; 6 | mon_vec &lt; 3]\nval63\n\n[1] 2 1 7\n\n\n\n2.5.2 Remplacement d’éléments\nNous pouvons modifier les valeurs de certains éléments d’un vecteur à l’aide des crochets [ ] en combinaison avec l’opérateur d’affectation &lt;-. Par exemple, pour remplacer la 4e valeur de mon_vec de 6 à 500 :\n\nmon_vec[4] &lt;- 500\nmon_vec\n\n[1]   2   3   1 500   4   3   3   7\n\n\nNous pouvons également remplacer plusieurs valeurs ou même remplacer des valeurs sur la base d’une expression logique :\n\n# remplacer les 6e et 7e éléments par 100\nmon_vec[c(6, 7)] &lt;- 100\nmon_vec\n\n[1]   2   3   1 500   4 100 100   7\n\n# remplacer les éléments inférieurs ou égaux à 4 par 1000\nmon_vec[mon_vec &lt;= 4] &lt;- 1000\nmon_vec\n\n[1] 1000 1000 1000  500 1000  100  100    7\n\n\n\n2.5.3 Ordonner les éléments\nOutre l’extraction d’éléments particuliers d’un vecteur, il est également possible d’ordonner les valeurs contenues dans un vecteur. Pour trier les valeurs de la plus petite à la plus grande, nous pouvons utiliser la fonction sort() :\n\nvec_trie &lt;- sort(mon_vec)\nvec_trie\n\n[1]    7  100  100  500 1000 1000 1000 1000\n\n\nPour inverser le tri, du plus élevé au plus bas, nous pouvons soit inclure l’option decreasing = TRUE lors de l’utilisation de la fonction sort() :\n\nvec_trie2 &lt;- sort(mon_vec, decreasing = TRUE)\nvec_trie2\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nsoit trier d’abord le vecteur à l’aide de la fonction sort() puis l’inverser à l’aide de la fonction rev(). Il s’agit là d’un autre exemple d’imbrication d’une fonction dans une autre fonction :\n\nvec_trie3 &lt;- rev(sort(mon_vec))\nvec_trie3\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nBien qu’il soit amusant de trier un seul vecteur, il serait peut-être plus utile de trier un vecteur en fonction des valeurs d’un autre vecteur. Pour ce faire, nous devons utiliser la fonction order() en combinaison avec [ ]. Pour le démontrer, créons un vecteur appelé taille contenant la taille de 5 personnes différentes et un autre vecteur appelé p_noms contenant les noms de ces personnes (Joanna mesure 180 cm, Charlotte mesure 155 cm, etc.)\n\ntaille &lt;- c(180, 155, 160, 167, 181)\ntaille\n\n[1] 180 155 160 167 181\n\np_noms &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\np_noms\n\n[1] \"Joanna\"    \"Charlotte\" \"Helen\"     \"Karen\"     \"Amy\"      \n\n\nNotre objectif est de classer les personnes dans p_noms dans l’ordre croissant de leur taille. La première chose que nous allons faire est d’utiliser la fonction order() avec le vecteur taille pour créer un vecteur appelé taille_ord\n\ntaille_ord &lt;- order(taille)\ntaille_ord\n\n[1] 2 3 4 1 5\n\n\nOK, que se passe-t-il ici ? La première valeur, 2(n’oubliez pas d’ignorer [1]) doit être lue comme “la plus petite valeur de taille est le deuxième élément du vecteur taille”. Si nous le vérifions en regardant le vecteur taille ci-dessus, nous pouvons voir que le 2e élément a une valeur de 155, ce qui est la plus petite valeur. La deuxième valeur la plus petite du vecteur taille est la 3e ce qui, après vérification, donne 160 et ainsi de suite. La plus grande valeur de taille est la 5e qui vaut 181. Maintenant que nous avons le vecteur des indices de position des tailles par ordre croissant (taille_ord), nous pouvons extraire ces valeurs de notre vecteur p_noms dans cet ordre\n\nnoms_ord &lt;- p_noms[taille_ord]\nnoms_ord\n\n[1] \"Charlotte\" \"Helen\"     \"Karen\"     \"Joanna\"    \"Amy\"      \n\n\nVous vous demandez probablement à quoi cela peut bien servir. Imaginons que vous disposiez d’un jeu de données contenant deux colonnes et que vous souhaitiez trier chacune d’entre elles. Si vous utilisez simplement sort() pour trier chaque colonne séparément, les valeurs de chaque colonne seront dissociées les unes des autres. En utilisant order() sur une colonne, un vecteur d’indices de position est créé à partir des valeurs de la colonne dans l’ordre croissant. Ce vecteur peut être utilisé sur la deuxième colonne, en tant qu’indice d’éléments qui renverront un vecteur de valeurs basé sur la première colonne. En toute honnêteté, lorsque vous avez plusieurs vecteurs liés, vous devez utiliser un objet de type data.frame (voir Chapitre 3) au lieu de plusieurs vecteurs indépendants.\n\n2.5.4 Vectorisation\nL’un des avantages des fonctions R est que la plupart d’entre elles sont vectorisées. Cela signifie que la fonction opère sur tous les éléments d’un vecteur sans qu’il soit nécessaire d’appliquer la fonction à chaque élément séparément. Par exemple, pour multiplier chaque élément d’un vecteur par 5, il suffit d’utiliser la fonction :\n\n# créer un vecteur\nmon_vec2 &lt;- c(3, 5, 7, 1, 9, 20)\n\n# multiplier chaque élément par 5\nmon_vec2 * 5\n\n[1]  15  25  35   5  45 100\n\n\nOu nous pouvons additionner les éléments de deux vecteurs ou plus :\n\n# créer un deuxième vecteur\nmon_vec3 &lt;- c(17, 15, 13, 19, 11, 0)\n\n# additionner les 2 vecteurs\nmon_vec2 + mon_vec3\n\n[1] 20 20 20 20 20 20\n\n# multiplier les 2 vecteurs\nmon_vec2 * mon_vec3\n\n[1] 51 75 91 19 99  0\n\n\nCependant, vous devez faire attention lorsque vous utilisez la vectorisation avec des vecteurs de longueurs différentes, car R recyclera tranquillement les éléments du vecteur le plus court plutôt que de signaler une erreur.\n\n# créer un troisième vecteur\nmon_vec4 &lt;- c(1, 2)\n\n# additionner les 2 vecteurs - recyclage tranquille!\nmon_vec2 + mon_vec4\n\n[1]  4  7  8  3 10 22\n\n\n\n2.5.5 Données manquantes\nDans R, les données manquantes sont généralement représentées par un NA qui signifie “Not Available” (Non disponible). Les données peuvent être manquantes pour toute une série de raisons : votre machine est peut-être tombée en panne, vous êtes peut-être tombé en panne, le temps était peut-être trop mauvais pour collecter des données un jour donné, etc. Les données manquantes peuvent être une véritable plaie, tant du point de vue de R que du point de vue statistique. Du point de vue de R, les données manquantes peuvent être problématiques car différentes fonctions traitent les données manquantes de différentes manières. Par exemple, supposons que nous ayons recueilli des relevés de température de l’air pendant 10 jours, mais que notre thermomètre se soit cassé le deuxième et le neuvième jour, de sorte que nous n’avons pas de données pour ces jours-là :\n\ntemp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5)\ntemp\n\n [1] 7.2  NA 7.1 6.9 6.5 5.8 5.8 5.5  NA 5.5\n\n\nNous voulons maintenant calculer la température moyenne sur ces jours à l’aide de la fonction mean() :\n\ntemp_moyenne &lt;- mean(temp)\ntemp_moyenne\n\n[1] NA\n\n\nSi un vecteur a une valeur manquante, la seule valeur possible à renvoyer lors du calcul d’une moyenne est NA. R ne sait pas que vous souhaitez peut-être ignorer la valeur NA (R ne peut pas lire dans vos pensées - pour l’instant !). Si nous regardons le fichier d’aide (en utilisant ?mean - voir la section suivante Section 2.6 pour plus de détails) associé à la fonction mean() nous pouvons voir qu’il y a un argument na.rm = qui prend la valeur FALSE par défaut.\n\nna.rm - une valeur logique indiquant si les valeurs NA doivent être supprimées avant le calcul (“na remove”).\n\nSi nous remplaçons cet argument par na.rm = TRUE lorsque nous utilisons la fonction mean() cela nous permettra d’ignorer les NA lors du calcul de la moyenne :\n\ntemp_moyenne &lt;- mean(temp, na.rm = TRUE)\ntemp_moyenne\n\n[1] 6.2875\n\n\nIl est important de noter que les NA n’ont pas été retirés du vecteur temp (ce serait une mauvaise pratique), mais que l’objet mean() les a simplement ignorées. Le but de ce qui précède est de souligner comment nous pouvons modifier le comportement par défaut d’une fonction à l’aide d’un argument approprié. Le problème est que toutes les fonctions n’ont pas d’argument na.rm = elles peuvent gérer les NA différemment. Cependant, la bonne nouvelle est que chaque fichier d’aide associé à une fonction vous indiquera toujours comment les données manquantes sont traitées par défaut.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#sec-help",
    "href": "02-base.html#sec-help",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.6 Obtenir de l’aide",
    "text": "2.6 Obtenir de l’aide\nCe livre est conçu comme une introduction relativement brève à R et, en tant que tel, vous utiliserez bientôt des fonctions et des paquets qui dépassent le cadre de ce texte d’introduction. Heureusement, l’une des forces de R est son système d’aide complet et facilement accessible, ainsi que la richesse des ressources en ligne où vous pouvez obtenir de plus amples informations.\n\n2.6.1 Aide R\nPour accéder à l’aide intégrée de R et obtenir des informations sur n’importe quelle fonction, il suffit d’utiliser la fonction help(). Par exemple, pour ouvrir la page d’aide de notre amie, la fonction mean() :\nhelp(\"mean\")\nou vous pouvez utiliser le raccourci ? devant la fonction :\n?mean\nla page d’aide est affichée dans l’onglet “Aide”(généralement en bas à droite sur RStudio)\n\n\n\n\n\n\n\nFigure 2.5: Page d’aide pour la fonction mean() dans le panneau Aide sur Rstudio\n\n\n\n\nIl est vrai que les fichiers d’aide peuvent sembler tout sauf utiles lorsque vous commencez à utiliser R. Cela est probablement dû au fait qu’ils sont écrits de manière très concise et que le langage utilisé est souvent assez technique et plein de jargon. Cela dit, on s’y habitue et, avec le temps, on finit même par apprécier une certaine beauté dans cette brièveté (honnêtement !). L’un des aspects les plus intéressants des fichiers d’aide est qu’ils ont tous une structure très similaire, quelle que soit la fonction. Il est donc facile de naviguer dans le fichier pour trouver exactement ce dont vous avez besoin.\nLa première ligne du document d’aide contient des informations telles que le nom de la fonction et le paquet d’où elle provient (entre les accolades {}, ici {base} signifie que la fonction mean() fait partie des fonctions de base de R). D’autres rubriques fournissent des informations plus spécifiques, telles que\n\n\n\n\n\n\nRubriques\nDescription de la rubrique\n\n\n\nDescription :\ndonne une brève description de la fonction et de ce qu’elle fait.\n\n\nUsage :\ndonne le nom des arguments associés à la fonction et les éventuelles valeurs par défaut.\n\n\nArguments :\nfournit plus de détails sur chaque argument et sur ce qu’il fait.\n\n\nDetails :\ndonne des détails supplémentaires sur la fonction si nécessaire.\n\n\nValue :\nle cas échéant, indique le type et la structure de l’objet renvoyé par la fonction ou l’opérateur.\n\n\nSee also :\nfournit des informations sur d’autres pages d’aide au contenu similaire ou connexe.\n\n\nExamples :\ndonne quelques exemples d’utilisation de la fonction.\n\n\n\nLes Examples (Exemples d’application) sont très utiles, il suffit de les copier et de les coller dans la console pour voir ce qui se passe. Vous pouvez également accéder aux exemples à tout moment en utilisant la fonction example() (c’est-à-dire example(\"mean\"))\nLa fonction help() est utile si vous connaissez le nom de la fonction. Si vous n’êtes pas sûr du nom, mais que vous vous souvenez d’un mot clé, vous pouvez faire une recherche dans le système d’aide de R à l’aide de la fonction help.search().\nhelp.search(\"mean\")\nou vous pouvez utiliser le raccourci équivalent ?? :\n??mean\nLes résultats de la recherche seront affichés dans RStudio sous l’onglet “Aide” comme précédemment. help.search() recherche dans la documentation d’aide, les démonstrations de code et les vignettes de paquet et affiche les résultats sous forme de liens cliquables pour une exploration plus approfondie.\n\n\n\n\n\n\n\nFigure 2.6: Sortie de la fonction help.search() dans Rstudio\n\n\n\n\nUne autre fonction utile est apropos(). Cette fonction peut être utilisée pour dresser la liste de toutes les fonctions contenant une chaîne de caractères spécifiée. Par exemple, pour trouver toutes les fonctions avec mean dans leur nom :\n\napropos(\"mean\")\n\n [1] \".colMeans\"     \".rowMeans\"     \"colMeans\"      \"kmeans\"       \n [5] \"mean\"          \"mean_temp\"     \"mean.Date\"     \"mean.default\" \n [9] \"mean.difftime\" \"mean.POSIXct\"  \"mean.POSIXlt\"  \"rowMeans\"     \n[13] \"vec_mean\"      \"weighted.mean\"\n\n\nVous pouvez alors afficher le fichier d’aide de la fonction concernée.\nhelp(\"kmeans\")\nUne autre fonction est RSiteSearch() qui vous permet de rechercher des mots-clés et des phrases dans les pages d’aide des fonctions et les vignettes de tous les paquets CRAN. Cette fonction vous permet d’accéder au moteur de recherche du site web de R https://www.r-project.org/search.html directement à partir de la console et d’afficher les résultats dans votre navigateur web.\nRSiteSearch(\"regression\")\n\n2.6.2 Autres sources d’aide\nIl n’y a jamais eu de meilleur moment pour commencer à apprendre R. Il existe pléthore de ressources en ligne disponibles gratuitement, allant de cours complets à des tutoriels et des listes de diffusion spécifiques à un sujet. Il existe également de nombreuses options payantes si c’est votre truc, mais à moins que vous n’ayez de l’argent à brûler, il n’est vraiment pas nécessaire de dépenser votre argent durement gagné. Vous trouverez ci-dessous quelques ressources que nous avons trouvées utiles.\n\n2.6.2.1 Ressources générales sur les R\n\n\nProjet R: Documentation fournie par l’utilisateur\n\nLe journal R: Journal du projet R pour le calcul statistique\n\nTourbillon: Un paquet R qui vous enseigne R de l’intérieur\nLes antisèches imprimables de RStudio\n\nRseek Une recherche Google personnalisée pour les sites liés à R\n\n2.6.2.2 Obtenir de l’aide\n\nCherchez sur internet: utiliser votre moteur de recherche préféré pour chercher les messages d’erreur que vous obtenez. Ce n’est pas de la triche et tout le monde le fait ! Vous serez surpris du nombre de personnes qui ont probablement rencontré le même problème et l’ont résolu.\n\nStack Overflow: Il y a plusieurs milliers de questions relatives à R sur Stack Overflow. Ici sont les plus populaires, classées par vote. Veillez à rechercher des questions similaires avant de poser la vôtre et à inclure un exemple reproductible afin d’obtenir les conseils les plus utiles. Un exemple reproductible est un exemple minimal qui permet aux personnes qui essaient de vous aider de voir l’erreur elles-mêmes.\n\n2.6.2.3 Ressources R markdown\n\nRéférence de base pour markdown et markdown R\nUne bonne référence en markdown\nUn bon tutoriel de 10 minutes sur le markdown\nFeuille de contrôle de RStudio sur le format R markdown\nFeuille de référence pour R markdown\n\nLa documentation R markdown incluant un guide de démarrage , a galerie de démonstrations et plusieurs articles pour une utilisation plus avancée.\n\nLe site web de knitr contient de nombreux documents de référence utiles sur le fonctionnement de knitr.\n\n2.6.2.4 Ressources Git et GitHub\n\n\nHappy Git: Excellente ressource pour l’utilisation de Git et GitHub\n\nContrôle de version avec RStudio: Document RStudio pour l’utilisation du contrôle de version\n\nUtiliser Git depuis RStudio: Un bon guide en 10 minutes\n\nLa classe R: Guide approfondi de l’utilisation de Git et GitHub avec RStudio\n\n2.6.2.5 Programmation R\n\n\nProgrammation R pour la science des données: Guide approfondi de la programmation R\n\nR pour la science des données: Livre fantastique, orienté tidyverse",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#sauvegarder-des-données-dans-r",
    "href": "02-base.html#sauvegarder-des-données-dans-r",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.7 Sauvegarder des données dans R",
    "text": "2.7 Sauvegarder des données dans R\nVotre approche de l’enregistrement du travail dans R et RStudio dépend de ce que vous voulez enregistrer. La plupart du temps, la seule chose que vous devrez sauvegarder est le code R de vos scripts. N’oubliez pas que votre script est un enregistrement reproductible de tout ce que vous avez fait. Il vous suffit donc d’ouvrir votre script dans une nouvelle session RStudio et de l’exécuter dans la console R pour revenir à l’endroit où vous vous étiez arrêté.\nÀ moins que vous n’ayez suivi notre suggestion de modifier les paramètres par défaut des projets RStudio (voir Section 1.5), il vous sera demandé si vous souhaitez sauvegarder l’image de votre espace de travail à chaque fois que vous quitterez RStudio. Nous pensons que dans 99,9 % des cas, vous ne souhaitez pas le faire. En commençant avec une session RStudio propre chaque fois que nous revenons à notre analyse, nous pouvons être sûrs d’éviter tout conflit potentiel avec les choses que nous avons faites dans les sessions précédentes.\nCependant, il est parfois utile de sauvegarder les objets que vous avez créés dans R. Par exemple, imaginons que vous créiez un objet dont la génération nécessite des heures (voire des jours) de temps de calcul. Il serait extrêmement gênant de devoir attendre tout ce temps à chaque fois que vous revenez sur votre analyse. Cependant, dans ce cas, nous pouvons enregistrer cet objet en tant que fichier externe, .RData que nous pourrons charger dans RStudio la prochaine fois que nous voudrons l’utiliser. Pour enregistrer un objet dans un fichier .RData vous pouvez utiliser la fonction save() (remarquez que nous n’avons pas besoin d’utiliser l’opérateur d’affectation ici) :\nsave(nomDelObjet, file = \"nom_du_fichier.RData\")\nou si vous souhaitez sauvegarder tous les objets de votre espace de travail dans un seul fichier .RData utilisez la fonction save.image() :\nsave.image(file = \"nom_du_fichier.RData\")\nPour charger votre .RData dans RStudio, utilisez la fonction load() :\nload(file = \"nom_du_fichier.RData\")",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "02-base.html#sec-packages",
    "href": "02-base.html#sec-packages",
    "title": "2  Quelques notions de base sur R",
    "section": "\n2.8 Paquets R",
    "text": "2.8 Paquets R\nL’installation de base de R est livrée avec de nombreux paquets utiles. Ces paquets contiennent de nombreuses fonctions que vous utiliserez quotidiennement. Cependant, lorsque vous commencerez à utiliser R pour des projets plus variés (et que votre propre utilisation de R évoluera), vous constaterez qu’il y a un moment où vous aurez besoin d’étendre les capacités de R. Heureusement, des milliers d’utilisateurs de R ont développé du code utile et l’ont partagé sous forme de paquets installables. Vous pouvez considérer un paquet comme une collection de fonctions, de données et de fichiers d’aide rassemblés dans une structure standard bien définie que vous pouvez télécharger et installer dans R. Ces paquets peuvent être téléchargés à partir de diverses sources, mais les plus populaires sont les suivantes CRAN, Bioconductor et GitHub . Actuellement, le CRAN héberge plus de 15 000 paquets et est le dépôt officiel des paquets R fournis par les utilisateurs. Bioconductor fournit des logiciels libres orientés vers la bioinformatique et héberge plus de 1800 paquets R. GitHub est un site web qui héberge des dépôts git pour toutes sortes de logiciels et de projets (pas seulement R). Souvent, les versions de développement de pointe des paquets R sont hébergées sur GitHub, donc si vous avez besoin de toutes les nouvelles fonctionnalités, cela peut être une option. Cependant, l’inconvénient potentiel de l’utilisation de la version de développement d’un paquet R est qu’elle peut ne pas être aussi stable que la version hébergée sur CRAN (elle est en cours de développement !) et que la mise à jour des paquets ne sera pas automatique.\n\n2.8.1 Utilisation des paquets\nUne fois que vous avez installé un paquet sur votre ordinateur, vous ne pouvez pas l’utiliser immédiatement. Pour utiliser un paquet, vous devez d’abord le charger à l’aide de la fonction library(). Par exemple, pour charger le paquet remotes 📦 que vous avez installé précédemment :\nlibrary(remotes)\nLa fonction library() chargera également tous les paquets supplémentaires nécessaires et pourra afficher des informations supplémentaires sur les paquets dans la console. Il est important de savoir que chaque fois que vous démarrez une nouvelle session R (ou que vous restaurez une session précédemment sauvegardée), vous devez charger les paquets que vous utiliserez. Nous avons tendance à mettre tous nos library() nécessaires à notre analyse en tête de nos scripts R afin de les rendre facilement accessibles et de pouvoir les compléter au fur et à mesure du développement de notre code. Si vous essayez d’utiliser une fonction sans avoir préalablement chargé le paquet R correspondant, vous recevrez un message d’erreur indiquant que R n’a pas pu trouver la fonction. Par exemple, si vous essayez d’utiliser la fonction install_github() sans charger le paquet remotes 📦 en premier lieu, vous obtiendrez l’erreur suivante :\ninstall_github(\"tidyverse/dplyr\")\n\n# Error in install_github(\"tidyverse/dplyr\") :\n#  could not find function \"install_github\"\nIl peut parfois être utile d’utiliser une fonction sans utiliser au préalable la fonctionlibrary(). Si, par exemple, vous n’utilisez qu’une ou deux fonctions dans votre script et que vous ne souhaitez pas charger toutes les autres fonctions d’un paquet, vous pouvez accéder directement à la fonction en spécifiant le nom du paquet, suivi de deux points (2 fois) ::, puis du nom de la fonction :\nremotes::install_github(\"tidyverse/dplyr\")\nC’est ainsi que nous avons pu utiliser la fonction install() et install_github() ci-dessous sans charger les paquets au préalableBiocManager 📦 et remotes 📦 . La plupart du temps, nous recommandons d’utiliser la fonction library().\n\n2.8.2 Installation des paquets R\n\n2.8.2.1 Paquets CRAN\nPour installer un paquet à partir du CRAN, vous pouvez utiliser la fonction install.packages(). Par exemple, si vous voulez installer le paquet remotes 📦 entrez le code suivant dans la Console (note : vous aurez besoin d’une connexion internet fonctionnelle pour effectuer cette opération) :\ninstall.packages(\"remotes\", dependencies = TRUE)\nIl vous sera peut-être demandé de choisir un miroir CRAN, sélectionnez simplement ‘0-cloud’ ou un miroir proche de votre localisation. L’argument dependencies = TRUE permet de s’assurer que les paquets supplémentaires nécessaires seront également installés.\nIl est conseillé de mettre régulièrement à jour les paquets déjà installés afin de bénéficier des nouvelles fonctionnalités et des corrections de bogues. Pour mettre à jour les paquets CRAN, vous pouvez utiliser la commande update.packages() (vous aurez besoin d’une connexion internet pour cela) :\nupdate.packages(ask = FALSE)\nL’argument ask = FALSE évite d’avoir à confirmer chaque téléchargement de paquet, ce qui peut être fastidieux si de nombreux paquets sont installés.\n\n2.8.2.2 Paquets Bioconductor\nPour installer des paquets de Bioconductor, le processus est un peu différent. Vous devez d’abord installer le paquet BiocManager 📦. Vous ne devez le faire qu’une seule fois, sauf si vous réinstallez ou mettez à jour R.\ninstall.packages(\"BiocManager\", dependencies = TRUE)\nUne fois que BiocManager a été installé, vous pouvez soit installer tous les paquets “de base” de Bioconductor avec la commande :\nBiocManager::install()\nou installer des paquets spécifiques tels que le GenomicRanges 📦 et edgeR 📦 :\nBiocManager::install(c(\"GenomicRanges\", \"edgeR\"))\nPour mettre à jour les paquets de Bioconductor, il suffit d’utiliser la commande BiocManager::install() à nouveau :\nBiocManager::install(ask = FALSE)\nLà encore, vous pouvez utiliser l’argument ask = FALSE pour éviter d’avoir à confirmer chaque téléchargement de paquet.\n\n2.8.2.3 Paquets GitHub\nIl existe plusieurs options pour installer les paquets hébergés sur GitHub. La méthode la plus efficace est sans doute d’utiliser la fonction install_github() du paquet remotes 📦 (vous avez installé ce paquet précédemment, Section 2.8.2.1). Avant d’utiliser la fonction, vous devez connaître le nom d’utilisateur GitHub du propriétaire du répertoire ainsi que le nom du répertoire. Par exemple, la version de développement de dplyr 📦 de Hadley Wickham est hébergée sur le compte GitHub de tidyverse et porte le nom de répertoire “dplyr” (recherchez simplement “github dplyr”). Pour installer cette version depuis GitHub, utilisez :\nremotes::install_github(\"tidyverse/dplyr\")\nLe moyen le plus sûr (à notre connaissance) de mettre à jour un paquet installé depuis GitHub est de le réinstaller en utilisant la commande ci-dessus.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quelques notions de base sur R</span>"
    ]
  },
  {
    "objectID": "03-donnees.html",
    "href": "03-donnees.html",
    "title": "3  Données",
    "section": "",
    "text": "3.1 Types de données\nIl est important de comprendre les différents types de données et la manière dont R traite ces données. La tentation est grande d’ignorer ces détails techniques, mais attention, cela peut se retourner contre vous très vite si vous ne faites pas attention. Nous avons déjà vu un exemple (Section 2.3.1) de cela lorsque nous avons essayé (et échoué) d’ajouter deux objets de type caractères ensemble en utilisant l’opérateur +.\nR dispose de six types de données de base : numérique (numeric), entier (integer), logique (logical), complexe (complex) et caractère (character). Les plus attentifs d’entre vous remarqueront que nous n’avons listé ici que cinq types de données, le dernier étant le type de données brut (raw), que nous n’aborderons pas car il n’est pas utile dans 99,99 % des cas. Nous n’aborderons pas non plus les nombres complexes, mais nous vous laisserons imaginer cette partie !\nR est (généralement) capable de distinguer automatiquement les différentes classes de données en fonction de leur nature et du contexte dans lequel elles sont utilisées, bien que vous deviez garder à l’esprit que R ne peut pas vraiment lire dans vos pensées et que vous devrez peut-être lui indiquer explicitement comment vous souhaitez traiter un type de données. Vous pouvez connaître le type (ou la classe) de n’importe quel objet en utilisant la fonction class() pour connaître le type (ou la classe) d’un objet.\nnum &lt;- 2.2\nclass(num)\n\n[1] \"numeric\"\n\nchar &lt;- \"hello\"\nclass(char)\n\n[1] \"character\"\n\nlogi &lt;- TRUE\nclass(logi)\n\n[1] \"logical\"\nVous pouvez également demander si un objet appartient à une classe spécifique à l’aide d’un test logique. Le test is.[classOfData]() renvoie soit un TRUE ou un FALSE.\nis.numeric(num)\n\n[1] TRUE\n\nis.character(num)\n\n[1] FALSE\n\nis.character(char)\n\n[1] TRUE\n\nis.logical(logi)\n\n[1] TRUE\nIl peut parfois être utile de pouvoir changer la classe d’une variable à l’aide de la fonction as.[className]() bien qu’il faille être prudent car vous pourriez obtenir des résultats inattendus (voir ce qui se passe ci-dessous lorsque nous essayons de convertir une chaîne de caractères en une chaîne numérique).\n# convertir un objet numérique en caractère\nclass(num)\n\n[1] \"numeric\"\n\nnum_char &lt;- as.character(num)\nnum_char\n\n[1] \"2.2\"\n\nclass(num_char)\n\n[1] \"character\"\n\n# convertir un objet caractère en numérique !\nclass(char)\n\n[1] \"character\"\n\nchar_num &lt;- as.numeric(char)\n\nWarning: NAs introduced by coercion\nVoici un tableau récapitulatif de certaines des fonctions de test logique et de coercion à votre disposition.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#types-de-données",
    "href": "03-donnees.html#types-de-données",
    "title": "3  Données",
    "section": "",
    "text": "Numérique sont des nombres contenant une décimale. En fait, il peut également s’agir de nombres entiers, mais nous n’y reviendrons pas.\nEntiers (integer) sont des nombres entiers (sans virgule).\nLogique prennent la valeur de TRUE ou FALSE. Il existe également un autre type de logique appelé NA pour représenter les valeurs manquantes.\nCaractère sont des chaînes de caractères comme un (ou plusieurs) mot(s). Un type particulier de chaîne de caractères est le facteur qui a des attributs supplémentaires (comme des niveaux ou un ordre). Nous reviendrons sur les facteurs plus tard.\n\n\n\n\n\n\n\n\n\n\nType de test\nTest logique\nCoercition\n\n\n\nCaractère\nis.character\nas.character\n\n\nNumérique\nis.numeric\nas.numeric\n\n\nLogique\nis.logical\nas.logical\n\n\nFacteur\nis.factor\nas.factor\n\n\nComplexe\nis.complex\nas.complex",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#structures-de-données",
    "href": "03-donnees.html#structures-de-données",
    "title": "3  Données",
    "section": "\n3.2 Structures de données",
    "text": "3.2 Structures de données\nMaintenant que vous connaissez certaines des classes de données les plus importantes de R, examinons quelques-unes des principales structures dont nous disposons pour stocker ces données.\n\n3.2.1 Scalaires et vecteurs\nLe type de structure de données le plus simple est sans doute le vecteur. Vous avez déjà été initié aux vecteurs dans la Section 2.4, certains des vecteurs que vous avez créés ne contenaient qu’une seule valeur (longueur 1) on. appelle ça un scalaires. Les vecteurs peuvent contenir des nombres, des caractères, des facteurs ou des logiques, mais la chose essentielle à retenir est que tous les éléments à l’intérieur d’un vecteur doivent être de la même classe. En d’autres termes, les vecteurs peuvent contenir des nombres, des caractères ou des logiques, mais pas des mélanges de ces types de données. Il existe une exception importante à cette règle : vous pouvez inclure des NA (rappelez-vous qu’il s’agit d’un type spécial de logique) pour indiquer les données manquantes dans les vecteurs contenant d’autres types de données.\n\n\n\n\n\n\n\nFigure 3.1: Structure de données scalaires et vecteurs\n\n\n\n\n\n3.2.2 Matrices et tableaux (array)\nLa matrice est une autre structure de données utile, utilisée dans de nombreuses disciplines telles que l’écologie des populations et les statistiques théoriques et appliquées. Une matrice est simplement un vecteur doté d’attributs supplémentaires appelés dimensions. Les tableaux (array) ne sont que des matrices multidimensionnelles. Là encore, les matrices et les tableaux doivent contenir des éléments appartenant tous à la même classe de données.\n\n\n\n\n\n\n\nFigure 3.2: Structure de données matrices et tableaux (array)\n\n\n\n\nUn moyen pratique de créer une matrice ou un tableau est d’utiliser la fonction matrix() et array() respectivement. Ci-dessous, nous allons créer une matrice à partir d’une séquence de 1 à 16 en quatre lignes (nrow = 4) et la remplir par rangée (byrow = TRUE) plutôt que par colonne, comme c’est le cas par défaut. Lors de l’utilisation de l’option array() il faut définir les dimensions à l’aide de la fonction dim = dans notre cas, 2 lignes, 4 colonnes dans 2 matrices différentes :\n\nma_mat &lt;- matrix(1:16, nrow = 4, byrow = TRUE)\nma_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nmon_tabl &lt;- array(1:16, dim = c(2, 4, 2))\nmon_tabl\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]    9   11   13   15\n[2,]   10   12   14   16\n\n\nIl est parfois utile de définir les noms des lignes et des colonnes de votre matrice, mais ce n’est pas obligatoire. Pour ce faire, utilisez la fonction rownames() et colnames() :\n\nrownames(ma_mat) &lt;- c(\"A\", \"B\", \"C\", \"D\")\ncolnames(ma_mat) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nma_mat\n\n   a  b  c  d\nA  1  2  3  4\nB  5  6  7  8\nC  9 10 11 12\nD 13 14 15 16\n\n\nUne fois que vous avez créé vos matrices, vous pouvez faire des choses utiles avec elles et, comme vous pouvez vous y attendre, R dispose de nombreuses fonctions intégrées pour effectuer des opérations sur les matrices. Certaines des plus courantes sont présentées ci-dessous. Par exemple, pour transposer une matrice, nous utilisons la fonction de transposition t() :\n\nma_mat_t &lt;- t(ma_mat)\nma_mat_t\n\n  A B  C  D\na 1 5  9 13\nb 2 6 10 14\nc 3 7 11 15\nd 4 8 12 16\n\n\nPour extraire les éléments diagonaux d’une matrice et les stocker sous forme de vecteur, nous pouvons utiliser la fonction diag() :\n\nma_mat_diag &lt;- diag(ma_mat)\nma_mat_diag\n\n[1]  1  6 11 16\n\n\nLes opérations habituelles d’addition, de multiplication, etc. de matrices peuvent être effectuées. Notez l’utilisation de la fonction %*% pour effectuer une multiplication matricielle.\n\nmat.1 &lt;- matrix(c(2, 0, 1, 1), nrow = 2)\nmat.1 # Notez que la matrice à été remplie par colonne par défaut\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    0    1\n\nmat.2 &lt;- matrix(c(1, 1, 0, 2), nrow = 2)\nmat.2\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    2\n\nmat.1 + mat.2 # Addition de matrices\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    3\n\nmat.1 * mat.2 # Produit élément par élément\n\n     [,1] [,2]\n[1,]    2    0\n[2,]    0    2\n\nmat.1 %*% mat.2 # Multiplication de matrice\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    2\n\n\n\n3.2.3 Listes\nLa prochaine structure de données que nous examinerons rapidement est la liste. Alors que les vecteurs et les matrices sont contraints de contenir des données du même type, les listes peuvent stocker des mélanges de types de données. En fait, nous pouvons même stocker d’autres structures de données telles que des vecteurs et des tableaux à l’intérieur d’une liste ou même avoir une liste de liste. Il s’agit donc d’une structure de données très flexible, idéale pour stocker des données irrégulières ou non rectangulaires (voir Chapitre 5 pour un exemple).\nPour créer une liste, nous pouvons utiliser la fonction list(). Notez que les trois éléments de la liste sont de classes différentes (caractère, logique et numérique) et de longueurs différentes.\n\nlist_1 &lt;- list(\n  c(\"black\", \"yellow\", \"orange\"),\n  c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  matrix(1:6, nrow = 3)\n)\nlist_1\n\n[[1]]\n[1] \"black\"  \"yellow\" \"orange\"\n\n[[2]]\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n[[3]]\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nLes éléments de la liste peuvent être nommés lors de la construction de la liste :\n\nlist_2 &lt;- list(\n  couleurs = c(\"black\", \"yellow\", \"orange\"),\n  evaluation = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  temps = matrix(1:6, nrow = 3)\n)\nlist_2\n\n$couleurs\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$temps\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nou après la création de la liste à l’aide de la fonction names() :\n\nnames(list_1) &lt;- c(\"couleurs\", \"evaluation\", \"temps\")\nlist_1\n\n$couleurs\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$temps\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n3.2.4 Jeu de données\nDe loin la structure de données la plus couramment utilisée : le jeu de données (data frame). Un jeu de données est un objet bidimensionnel puissant composé de lignes et de colonnes qui ressemble superficiellement à une matrice. Toutefois, alors que les matrices ne peuvent contenir que des données du même type, les jeux de données peuvent contenir un mélange de différents types de données. En règle générale, dans un jeu de données, chaque ligne correspond à une observation individuelle et chaque colonne correspond à une variable mesurée ou enregistrée différente. Cette configuration est peut-être familière à ceux d’entre vous qui utilisent LibreOffice Calc ou Microsoft Excel pour gérer et stocker leurs données. Il peut être utile de penser que les jeux de données sont essentiellement constitués d’un ensemble de vecteurs (colonnes), chaque vecteur contenant son propre type de données, mais le type de données peut être différent d’un vecteur à l’autre.\nPar exemple, le jeu de données ci-dessous contient les résultats d’une expérience visant à déterminer l’effet des soins parentaux (avec ou sans) chez les licornes (Unicornus magnificens) sur la croissance des petits sous trois régimes de disponibilité alimentaire différents. Le jeu de données contient 8 variables (colonnes) et chaque ligne représente une licorne individuelle. Les variables p_care et food sont des facteurs (variables catégoriques). La variable p_care a 2 niveaux (care et no_care) et la variable food a 3 niveaux (low, medium et high). Les variables height, weight, mane_size et fluffyness sont numériques et la variable horn_rings est un nombre entier représentant le nombre d’anneaux sur la corne. Bien que la variable block possède des valeurs numériques, celles-ci n’ont pas vraiment d’ordre et pourraient également être traitées comme un facteur (i.e. elles auraient également pu être appelées A et B).\n\n\n\nTable 3.1: Données importées sur les licornes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_care\nfood\nblock\nheight\nweight\nmane_size\nfluffyness\nhorn_rings\n\n\n\ncare\nmedium\n1\n7.5\n7.62\n11.7\n31.9\n1\n\n\ncare\nmedium\n1\n10.7\n12.14\n14.1\n46.0\n10\n\n\ncare\nmedium\n1\n11.2\n12.76\n7.1\n66.7\n10\n\n\ncare\nmedium\n1\n10.4\n8.78\n11.9\n20.3\n1\n\n\ncare\nmedium\n1\n10.4\n13.58\n14.5\n26.9\n4\n\n\ncare\nmedium\n1\n9.8\n10.08\n12.2\n72.7\n9\n\n\nno_care\nlow\n2\n3.7\n8.10\n10.5\n60.5\n6\n\n\nno_care\nlow\n2\n3.2\n7.45\n14.1\n38.1\n4\n\n\nno_care\nlow\n2\n3.9\n9.19\n12.4\n52.6\n9\n\n\nno_care\nlow\n2\n3.3\n8.92\n11.6\n55.2\n6\n\n\nno_care\nlow\n2\n5.5\n8.44\n13.5\n77.6\n9\n\n\nno_care\nlow\n2\n4.4\n10.60\n16.2\n63.3\n6\n\n\n\n\n\n\n\n\nIl y a deux choses importantes à garder à l’esprit à propos des jeux de données. Ces types d’objets sont connus sous le nom de données rectangulaires (ou données ordonnées), car chaque colonne doit comporter le même nombre d’observations. Aussi, toute donnée manquante doit être enregistrée sous la forme d’un NA comme nous l’avons fait pour nos vecteurs.\nNous pouvons construire un jeu de données à partir d’objets de données existants, tels que des vecteurs, à l’aide de la fonction data.frame(). À titre d’exemple, créons trois vecteurs p_taille, p_poids et p_noms et incluons tous ces vecteurs dans un jeu de données appelé dataf.\n\np_taille &lt;- c(180, 155, 160, 167, 181)\np_poids &lt;- c(65, 50, 52, 58, 70)\np_noms &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(taille = p_taille, \n                    poids = p_poids, \n                    noms = p_noms)\ndataf\n\n  taille poids      noms\n1    180    65    Joanna\n2    155    50 Charlotte\n3    160    52     Helen\n4    167    58     Karen\n5    181    70       Amy\n\n\nVous remarquerez que chacune des colonnes est nommée avec le nom de la variable que nous avons fourni lorsque nous avons utilisé la fonction data.frame() lorsque nous avons utilisé la fonction. Il semble également que la première colonne du cadre de données soit une série de nombres allant de 1 à 5. En fait, il ne s’agit pas vraiment d’une colonne, mais du nom de chaque ligne. Nous pouvons le vérifier en demandant à R de renvoyer les dimensions du jeu de données dataf à l’aide de la fonction dim(). Nous constatons qu’il y a 5 lignes et 3 colonnes.\n\ndim(dataf) # 5 lignes et 3 colonnes\n\n[1] 5 3\n\n\nUne autre fonction très utile que nous utilisons en permanence est str() qui renvoie un résumé compact de la structure de l’objet data frame (ou de tout autre objet).\n\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ height: num  180 155 160 167 181\n $ weight: num  65 50 52 58 70\n $ names : chr  \"Joanna\" \"Charlotte\" \"Helen\" \"Karen\" ...\n\n\nLa fonction str() nous donne les dimensions du jeu de données et nous rappelle que dataf est un data.frame. Il énumère également toutes les variables (colonnes) contenues dans le jeu de données, nous indique le type de données qu’elles contiennent et leurs 5 premières valeurs. Nous copions souvent ce résumé pour le mettre dans le scripts R avec des commentaires au début de chaque ligne afin de pouvoir nous y référer facilement lors de l’écriture du code. Nous vous avons montré comment commenter les blocs dans RStudio dans la Section 1.7.\nNotez également que R a automatiquement décidé que notre p_noms doit être un caractère (chr) lors de la création du jeu de données. La question de savoir si c’est une bonne idée ou non dépendra de la manière dont vous souhaitez utiliser cette variable dans des analyses ultérieures. Si nous décidons que ce n’est pas une bonne idée, nous pouvons modifier le comportement par défaut du data.frame() en incluant l’argument stringsAsFactors = TRUE. Nos chaînes de caractères seront maintenant automatiquement converties en facteurs.\n\np_taille &lt;- c(180, 155, 160, 167, 181)\np_poids &lt;- c(65, 50, 52, 58, 70)\np_noms &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(\n  taille = p_taille, poids = p_poids, noms = p_noms,\n  stringsAsFactors = TRUE\n)\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ taille: num  180 155 160 167 181\n $ poids : num  65 50 52 58 70\n $ noms  : Factor w/ 5 levels \"Amy\",\"Charlotte\",..: 4 2 3 5 1",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#importer-des-données",
    "href": "03-donnees.html#importer-des-données",
    "title": "3  Données",
    "section": "\n3.3 Importer des données",
    "text": "3.3 Importer des données\nBien que la création de jeux de données à partir de structures de données existantes soit extrêmement utile, l’approche de loin la plus courante consiste à créer un jeu de données en important des données à partir d’un fichier externe. Pour ce faire, vos données doivent être correctement formatées et enregistrées dans un format de fichier que R est capable de reconnaître. Heureusement pour nous, R est capable de reconnaître une grande variété de formats de fichiers, même si, en réalité, vous n’en utiliserez probablement que deux ou trois régulièrement.\n\n3.3.1 Enregistrement de fichiers à importer\nLa méthode la plus simple pour créer un fichier de données à importer dans R consiste à saisir vos données dans une feuille de calcul à l’aide de Microsoft Excel ou LibreOffice Calc et à l’enregistrer sous la forme d’un fichier délimité par des virgules. Nous préférons LibreOffice Calc car c’est un logiciel libre, indépendant de plate-forme et gratuit, mais MS Excel convient également (mais voir ici pour quelques problèmes). Voici les données de l’expérience sur les licornes dont nous avons parlé précédemment, affichées dans LibreOffice. Si vous souhaitez suivre en même temps que le livre, vous pouvez télécharger le fichier de données (‘unicorn.xlsx’) à partir de l’Annexe A.\n\n\n\n\n\n\n\nFigure 3.3: Données Licornes (Unicorn) dans LibreOffice Calc\n\n\n\n\nPour ceux d’entre vous qui ne connaissent pas le format de fichier délimité par des virgules, cela signifie simplement que les données des différentes colonnes sont séparées par le caractère “,” et sont généralement enregistrées dans un fichier portant l’extension “.csv” (Comma-Separated Values).\nPour enregistrer une feuille de calcul en tant que fichier délimité par des virgules (CSV) dans LibreOffice Calc, sélectionnez Fichier -&gt; Enregistrer sous... dans le menu principal. Vous devrez spécifier l’emplacement où vous souhaitez enregistrer votre fichier dans l’option “Enregistrer dans le dossier” et le nom du fichier dans l’option “Nom”. Dans le menu déroulant situé au-dessus du bouton “Enregistrer”, remplacez l’option par défaut “Tous les formats” par “Texte CSV (.csv)”.\n\n\n\n\n\n\n\nFigure 3.4: Choisir le format csv lors de l’enregistrement avec LibreOffice Calc\n\n\n\n\nCliquez sur le bouton Enregistrer, puis sélectionnez l’option “Utiliser le format CSV texte”. Cliquez sur OK pour enregistrer le fichier.\nIl y a quelques points à prendre en compte lors de l’enregistrement des fichiers à importer dans R, qui vous faciliteront la vie à long terme. Les titres de vos colonnes (si vous en avez) doivent être courts et informatifs. Évitez également les espaces dans vos titres de colonnes en les remplaçant par un trait de soulignement _ ou un point . (c’est-à-dire remplacer taille de la criniere par taille_de_la_criniere ou taille.de.la.criniere) et d’éviter d’utiliser des caractères spéciaux (par ex. aire (mm^2)), des accents (par ex. écrire criniere au lieu de crinière) ou des majuscules pour vous simplifier la vie. N’oubliez pas que si vous avez des données manquantes dans votre cadre de données (cellules vides), vous devez utiliser un champ NA pour les représenter. Cela permettra de conserver un cadre de données ordonné.\n\n3.3.2 Fonctions d’importation\nUne fois que vous avez enregistré votre fichier de données dans un format approprié, nous pouvons maintenant lire ce fichier dans R. La fonction la plus utilisée pour importer des données dans R est la fonction read.table() (nous examinerons d’autres solutions plus loin dans ce chapitre). La fonction read.table() est une fonction très flexible qui dispose d’un grand nombre d’arguments (voir ?read.table), mais elle est assez simple à utiliser. Importons le fichier délimité par des virgules appelé unicorns.csv qui contient les données que nous avons vues précédemment dans ce chapitre (Section 3.2.4) que l’on va assigner à un objet appelé licornes. Le fichier est situé dans un dossier data (“données”) qui est lui-même situé dans notre répertoire racine (Section 1.4). La première ligne des données contient les noms des variables (colonnes). Pour utiliser les read.table() pour importer ce fichier\n\nlicornes &lt;- read.table(\n  file = \"data/unicorns.csv\", header = TRUE, sep = \",\", dec = \".\",\n  stringsAsFactors = TRUE\n)\n\nIl y a quelques points à noter à propos de la commande ci-dessus. Tout d’abord, le chemin d’accès au fichier et le nom du fichier (y compris l’extension) doivent être placés entre guillemets simples ou doubles (c-à-d. le data/unicorns.csv), car la fonction read.table() attend une chaîne de caractères. Si votre répertoire de travail est déjà le répertoire qui contient le fichier, il n’est pas nécessaire d’inclure le chemin d’accès complet au fichier, mais seulement le nom du fichier. Dans l’exemple ci-dessus, le chemin d’accès au fichier est séparé par une simple barre oblique. /. Cela fonctionne quel que soit le système d’exploitation que vous utilisez et nous vous recommandons de vous en tenir à cela. Cependant, les utilisateurs de Windows peuvent être plus familiers avec la notation de la barre oblique inverse simple et si vous voulez continuer à l’utiliser, vous devrez l’inclure en tant que barre oblique inverse double.\n\n\n\n\n\n\nAvertissement\n\n\n\nNotez cependant que la notation de la double barre oblique inverse fonctionnera pas sur les ordinateurs utilisant les systèmes d’exploitation Mac OSX ou Linux. Nous le déconseillons donc fortement car il n’est pas reproductible\n\n\nLes header = TRUE spécifie que la première ligne de vos données contient les noms des variables (c.-à-d. food, block etc.) Si ce n’est pas le cas, vous pouvez spécifier header = FALSE (en fait, c’est la valeur par défaut, vous pouvez donc omettre complètement cet argument). L’argument sep = \",\" indique à R quel est le délimiteur de fichier.\nD’autres arguments utiles sont dec = et na.strings =. Les dec = permet de modifier le caractère par défaut (.) utilisé par défaut pour le point décimal. Ceci est utile si vous êtes dans un pays où les décimales sont généralement représentées par une virgule (c.-à-d. dec = \",\"). L’argument na.strings = vous permet d’importer des données où les valeurs manquantes sont représentées par un symbole autre que NA. Cela peut être assez courant si vous importez des données à partir d’un autre logiciel statistique tel que Minitab, qui représente les valeurs manquantes avec le symbole * (na.strings = \"*\").\nUne série de fonctions prédéfinies sont disponibles dans read.table() pour définire des options spécifiques au format. Mais nous pouvons aussi simplement utiliser read.csv()pour lire un fichier csv, avec la séparation “,” et “.” pour les décimales. Dans les pays où “,” est utilisé pour les décimales, les fichiers csv utilisent “;” comme séparateur. Dans ce cas, l’utilisation de read.csv2() serait nécessaire. Lorsque l’on travaille avec des fichiers délimités par des tabulations, les fonctions read.delim() et read.delim2() peuvent être utilisées avec “.” et “,” comme décimales respectivement.\nAprès avoir importé nos données dans R, pour voir le contenu du jeu de données, il suffit de taper le nom de l’objet comme nous l’avons fait précédemment. MAIS avant de faire cela, réfléchissez à la raison pour laquelle vous faites cela. Si votre jeu de données est autre chose que minuscule, tout ce que vous allez faire, c’est remplir votre Console de données. Ce n’est pas comme si vous pouviez facilement vérifier s’il y a des erreurs ou si vos données ont été importées correctement. Une bien meilleure solution consiste à utiliser notre vieil ami, la fonction str() pour obtenir un résumé compact et informatif de votre base de données.\n\nstr(licornes)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : Factor w/ 2 levels \"care\",\"no_care\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food      : Factor w/ 3 levels \"high\",\"low\",\"medium\": 3 3 3 3 3 3 3 3 3 3 ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nIci, nous voyons que licornes est un objet “data.frame” qui contient 96 lignes et 8 variables (colonnes). Chacune des variables est répertoriée avec sa classe de données et les 10 premières valeurs. Comme nous l’avons mentionné précédemment dans ce chapitre, il peut être très pratique de copier/coller ces données dans votre script R sous la forme d’un bloc de commentaires afin de pouvoir s’y référer ultérieurement.\nNotez également que vos variables de type chaîne de caractères (care et food) ont été importées en tant que facteurs parce que nous avons utilisé l’argument stringsAsFactors = TRUE. Si ce n’est pas ce que vous voulez, vous pouvez utiliser l’argument stringsAsFactors = FALSE ou à partir de la version 4.0.0 de R, vous pouvez simplement ne pas utiliser cet argument car stringsAsFactors = FALSE est la valeur par défaut.\nNous pouvons donc aussi extraire le jeu de données depuis un fichier texte (.txt) grâce à la fonction read.delim() :\n\nlicornes &lt;- read.delim(file = \"data/unicorns.txt\")\nstr(licornes)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nSi nous voulons simplement voir les noms de nos variables (colonnes) dans le jeu de données, nous pouvons utiliser l’argument names() qui renverra un vecteur de caractères contenant les noms des variables.\n\nnames(licornes)\n\n[1] \"p_care\"     \"food\"       \"block\"      \"height\"     \"weight\"    \n[6] \"mane_size\"  \"fluffyness\" \"horn_rings\"\n\n\nVous pouvez même importer des feuilles de calcul de MS Excel ou d’autres logiciels de statistiques directement dans R, mais nous vous conseillons d’éviter cette méthode dans la mesure du possible, car elle ne fait qu’ajouter une couche d’incertitude entre vous et vos données. À notre avis, il est presque toujours préférable d’exporter vos feuilles de calcul sous forme de fichiers délimités par des tabulations (.txt) ou des virgules (.csv), puis de les importer dans R à l’aide de l’un des fonctions dérivées de read.table() (c-à-d. read.delim(), read.csv(), etc.). Si vous tenez absolument à importer directement des données à partir d’un autre logiciel, vous devrez installer le paquet foreign 📦 qui contient des fonctions permettant d’importer des fichiers Minitab, SPSS, Stata et SAS. Pour les feuilles de calcul MS Excel et LO Calc, quelques paquets peuvent être utilisés.\n\n3.3.3 Frustrations courantes liées à l’importation\nIl est assez courant d’obtenir un tas de messages d’erreur vraiment frustrants lorsque l’on commence à importer des données dans R. Le plus courant est sans doute\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") :\n  cannot open file 'unicorns.txt': No such file or directory\nCe message d’erreur vous indique que R ne peut pas trouver le fichier que vous essayez d’importer. Il apparaît généralement pour l’une ou l’autre des raisons suivantes (ou pour toutes !). La première est que vous avez fait une erreur dans l’orthographe du nom de fichier ou du chemin d’accès au fichier. Une autre erreur fréquente est d’avoir oublié d’inclure l’extension du fichier dans le nom du fichier (par ex. .txt). Enfin, le fichier ne se trouve pas à l’endroit indiqué ou vous avez utilisé un chemin d’accès incorrect. L’utilisation de projets RStudio (Section 1.5) et d’une structure de répertoire logique (Section 1.4) permet d’éviter (limiter) ce type d’erreurs.\nUne autre erreur très fréquente est d’oublier d’inclure l’élément header = TRUE lorsque la première ligne des données contient des noms de variables. Par exemple, si nous omettons cet argument lorsque nous importons notre fichier unicorns.txt tout semble correct au début (pas de message d’erreur au moins).\n\nlicornes_erreur &lt;- read.table(file = \"data/unicorns.txt\", sep = \"\\t\")\n\nmais lorsque nous jetons un coup d’œil à notre jeu de données en utilisant str()\n\nstr(licornes_erreur)\n\n'data.frame':   97 obs. of  8 variables:\n $ V1: chr  \"p_care\" \"care\" \"care\" \"care\" ...\n $ V2: chr  \"food\" \"medium\" \"medium\" \"medium\" ...\n $ V3: chr  \"block\" \"1\" \"1\" \"1\" ...\n $ V4: chr  \"height\" \"7.5\" \"10.7\" \"11.2\" ...\n $ V5: chr  \"weight\" \"7.62\" \"12.14\" \"12.76\" ...\n $ V6: chr  \"mane_size\" \"11.7\" \"14.1\" \"7.1\" ...\n $ V7: chr  \"fluffyness\" \"31.9\" \"46\" \"66.7\" ...\n $ V8: chr  \"horn_rings\" \"1\" \"10\" \"10\" ...\n\n\nNous constatons un problème évident, toutes nos variables ont été importées en tant que facteurs et sont nommées V1, V2, V3 … V8. Le problème vient du fait que nous n’avons pas dit au read.table() que la première ligne contient les noms des variables et donc elle les traite comme des données. Dès que nous avons une chaîne de caractères dans l’une de nos variable, R les traite comme des données de type caractère (rappelez-vous que tous les éléments d’un vecteur doivent contenir le même type de données (Section 3.2.1)).\nCe n’est qu’un argument de plus pour utiliser read.csv() ou read.delim() avec des valeurs par défaut appropriées pour les arguments.\n\n3.3.4 Autres options d’importation\nIl existe de nombreuses autres fonctions permettant d’importer des données à partir d’une variété de sources et de formats. La plupart de ces fonctions sont contenues dans des paquets que vous devez installer avant de les utiliser. Vous trouverez ci-dessous une liste des paquets et des fonctions les plus utiles.\nLes paquets fread() du paquet data.table 📦 est idéale pour importer rapidement et efficacement de grands fichiers de données (beaucoup plus rapidement que la fonction read.table()). L’un des aspects les plus intéressants de la fonction fread() est qu’elle détecte automatiquement la plupart des arguments que vous devriez normalement spécifier (comme sep = etc.) Une choses à prendre en compte cependant est que la fonction fread() renverra un objet de type data.table et non data.frame comme ce serait le cas avec la fonction read.table(). Ce n’est généralement pas un problème, car vous pouvez passer un objet data.table à n’importe quelle fonction n’acceptant normalement que des objets data.frame. Pour en savoir plus sur les différences entre data.table et data.frame voir ici .\n\nlibrary(data.table)\ntoutes_donnees &lt;- fread(file = \"data/unicorns.txt\")\n\nDiverses fonctions du paquet readr 📦 sont également très efficaces pour lire des fichiers de données volumineux. Le paquet readr 📦 fait partie de la collection de paquet ‘tidyverse’ et fournit de nombreuses fonctions équivalentes à celles de la version de base de R pour l’importation de données. Les fonctions de readr sont utilisées de la même manière que les fonctions read.table() ou read.csv() et la plupart des arguments sont les mêmes (voir ?readr::read_table pour plus de détails). Il existe cependant quelques différences. Par exemple, lors de l’utilisation de l’option read_table() l’argument header = TRUE est remplacé par col_names = TRUE et la fonction renvoie un objet de classe tibble qui est l’équivalent dans le tidyverse d’un objet de classe data.frame (voir ici pour les différences).\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention ! certain.e.s de vos modèles ou fonctions, etc. peuvent ne pas fonctionner ou fonctionner différement à cause de ça (vérifier/reconvertir votre jeu de données au format data.frame peut vous économiser du temps de débeugage).\n\n\n\nlibrary(readr)\n# importation de fichiers délimités par des espaces blancs\ntoutes_donnees &lt;- read_table(file = \"data/unicorns.txt\", col_names = TRUE)\n\n# importation de fichiers délimités par des virgules\ntoutes_donnees &lt;- read_csv(file = \"data/unicorns.csv\")\n\n# importation de fichiers délimités par des tabulations\ntoutes_donnees &lt;- read_delim(file = \"data/unicorns.txt\", delim = \"\\t\")\n\n# ou utilisez\ntoutes_donnees &lt;- read_tsv(file = \"data/unicorns.txt\")\n\nSi votre fichier de données est énorme, les paquets ff 📦 et bigmemory 📦 peuvent être utiles car ils contiennent tous deux des fonctions d’importation capables de stocker des données volumineuses de manière efficace en termes de mémoire. Pour en savoir plus sur ces fonctions ici et ici .",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#manipuler-des-jeux-de-données",
    "href": "03-donnees.html#manipuler-des-jeux-de-données",
    "title": "3  Données",
    "section": "\n3.4 Manipuler des jeux de données",
    "text": "3.4 Manipuler des jeux de données\nMaintenant que vous avez réussi à importer vos données dans R depuis un fichier externe, la prochaine étape est de faire quelque chose d’utile avec nos données. La manipulation de données est une compétence fondamentale que vous devrez développer et avec laquelle vous devrez vous sentir à l’aise, car vous en ferez probablement beaucoup au cours de n’importe quel projet. La bonne nouvelle, c’est que R est particulièrement efficace pour manipuler, résumer et visualiser les données. La manipulation de données (souvent connue sous le nom de “data wrangling” ou “munging”) en R peut sembler un peu intimidante au début pour un nouvel utilisateur, mais si vous suivez quelques règles logiques simples, vous prendrez rapidement le coup de main, surtout avec un peu de pratique.\nRappelons la structure du jeu de données licornes que nous avons importée dans la section précédente :\n\nlicornes &lt;- read.table(file = \"data/unicorns.txt\", header = TRUE, sep = \"\\t\")\nstr(licornes)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nPour accéder aux données de n’importe quelle variable (colonne) de notre jeu de données, nous pouvons utiliser la notation $. Par exemple, pour accéder à la variable height dans le jeu de données licornes on écrit licornes$height. Cela indique à R que la variable height est contenue dans le jeu de données licornes.\n\nlicornes$height\n\n [1]  7.5 10.7 11.2 10.4 10.4  9.8  6.9  9.4 10.4 12.3 10.4 11.0  7.1  6.0  9.0\n[16]  4.5 12.6 10.0 10.0  8.5 14.1 10.1  8.5  6.5 11.5  7.7  6.4  8.8  9.2  6.2\n[31]  6.3 17.2  8.0  8.0  6.4  7.6  9.7 12.3  9.1  8.9  7.4  3.1  7.9  8.8  8.5\n[46]  5.6 11.5  5.8  5.6  5.3  7.5  4.1  3.5  8.5  4.9  2.5  5.4  3.9  5.8  4.5\n[61]  8.0  1.8  2.2  3.9  8.5  8.5  6.4  1.2  2.6 10.9  7.2  2.1  4.7  5.0  6.5\n[76]  2.6  6.0  9.3  4.6  5.2  3.9  2.3  5.2  2.2  4.5  1.8  3.0  3.7  2.4  5.7\n[91]  3.7  3.2  3.9  3.3  5.5  4.4\n\n\nCette opération renvoie un vecteur des données de la variable height. Si nous le souhaitons, nous pouvons assigner ce vecteur à un autre objet et faire d’autres choses avec, comme calculer une moyenne ou obtenir un résumé de la variable à l’aide de la fonction summary() :\n\nf_taille &lt;- licornes$height\nmean(f_taille)\n\n[1] 6.839583\n\nsummary(f_taille)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nOu si nous ne voulons pas créer un objet supplémentaire, nous pouvons utiliser des fonctions “à la volée” pour afficher uniquement la valeur dans la console.\n\nmean(licornes$height)\n\n[1] 6.839583\n\nsummary(licornes$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nTout comme nous l’avons fait avec les vecteurs (Section 2.5), nous pouvons également accéder aux données contenues dans le jeu de données en utilisant les crochets [ ]. Cependant, au lieu d’utiliser un seul index, nous devons maintenant utiliser deux index, l’un pour spécifier les lignes et l’autre pour les colonnes. Pour ce faire, nous pouvons utiliser la notation mes_donnees[ligne, colonne] où ligne et colonne sont des indices et mes_donnees est le nom du jeu de données. Une fois encore, comme pour nos vecteurs, nos index peuvent être positionnels ou résulter d’un test logique.\n\n3.4.1 Index positionnels\nPour utiliser les index positionnels, il suffit d’écrire la position des lignes et des colonnes que l’on veut extraire à l’intérieur des crochets [ ]. Par exemple, si, pour une raison quelconque, nous voulons extraire la première valeur (1ère ligne) de la variable height (4e colonne) :\n\nlicornes[1, 4]\n\n[1] 7.5\n\n# on peut obtenir le même résultat avec cette notation :\nlicornes$height[1]\n\n[1] 7.5\n\n\nNous pouvons également extraire les valeurs de plusieurs lignes ou colonnes en spécifiant ces index sous forme de vecteurs à l’intérieur des crochets [ ]. Pour extraire les 10 premières lignes et les 4 premières colonnes, il suffit de fournir un vecteur contenant une séquence de 1 à 10 pour l’index des lignes (1:10) et un vecteur de 1 à 4 pour l’index des colonnes (1:4) :\n\nlicornes[1:10, 1:4]\n\n   p_care   food block height\n1    care medium     1    7.5\n2    care medium     1   10.7\n3    care medium     1   11.2\n4    care medium     1   10.4\n5    care medium     1   10.4\n6    care medium     1    9.8\n7    care medium     1    6.9\n8    care medium     1    9.4\n9    care medium     2   10.4\n10   care medium     2   12.3\n\n\nSi les lignes et les colonnes ne sont pas séquentielles, nous pouvons fournir des vecteurs de positions à l’aide de la fonction c(). Pour extraire les 1ère, 5e, 12e et 30e lignes des 1ère, 3e, 6e et 8e colonnes :\n\nlicornes[c(1, 5, 12, 30), c(1, 3, 6, 8)]\n\n   p_care block mane_size horn_rings\n1    care     1      11.7          1\n5    care     1      14.5          4\n12   care     2      12.6          6\n30   care     2      11.6          5\n\n\nTout ce que nous faisons dans les deux exemples ci-dessus est de créer des vecteurs de positions pour les lignes et les colonnes que nous voulons extraire. Pour ce faire, nous avons utilisé les compétences que nous avons développées dans la Section 2.4 lorsque nous avons généré des vecteurs à l’aide de la fonction c() ou en utilisant la notation :.\nMais qu’en est-il si nous voulons extraire toutes les lignes ou toutes les colonnes ? Il serait extrêmement fastidieux de devoir générer des vecteurs pour toutes les lignes ou pour toutes les colonnes. Heureusement, R dispose d’un raccourci. Si vous ne spécifiez pas d’index de ligne ou de colonne dans les crochets [ ] R interprète cela comme signifiant que vous voulez toutes les lignes ou toutes les colonnes. Par exemple, pour extraire les 4 premières lignes et toutes les colonnes du jeu de données licornes :\n\nlicornes[1:4, ]\n\n  p_care   food block height weight mane_size fluffyness horn_rings\n1   care medium     1    7.5   7.62      11.7       31.9          1\n2   care medium     1   10.7  12.14      14.1       46.0         10\n3   care medium     1   11.2  12.76       7.1       66.7         10\n4   care medium     1   10.4   8.78      11.9       20.3          1\n\n\nou toutes les lignes et les 3 premières colonnes 1 .\nunicorns[, 1:3]\n\n\n    p_care   food block\n1     care medium     1\n2     care medium     1\n3     care medium     1\n4     care medium     1\n5     care medium     1\n92 no_care    low     2\n93 no_care    low     2\n94 no_care    low     2\n95 no_care    low     2\n96 no_care    low     2\n\n\nNous pouvons même utiliser des index de position négatifs pour exclure certaines lignes et colonnes. Par exemple, extrayons toutes les lignes à l’exception des 85 premières et toutes les colonnes à l’exception de la 4e, 7e et 8e. Remarquez que nous devons utiliser -() lorsque nous générons nos vecteurs de position de ligne. Si nous avions simplement utilisé -1:85 cela générerait en fait une séquence régulière de -1 à 85, ce qui n’est pas ce que nous voulons (nous pouvons bien sûr utiliser -1:-85).\n\nlicornes[-(1:85), -c(4, 7, 8)]\n\n    p_care food block weight mane_size\n86 no_care  low     1   6.01      17.6\n87 no_care  low     1   9.93      12.0\n88 no_care  low     1   7.03       7.9\n89 no_care  low     2   9.10      14.5\n90 no_care  low     2   9.05       9.6\n91 no_care  low     2   8.10      10.5\n92 no_care  low     2   7.45      14.1\n93 no_care  low     2   9.19      12.4\n94 no_care  low     2   8.92      11.6\n95 no_care  low     2   8.44      13.5\n96 no_care  low     2  10.60      16.2\n\n\nOutre l’utilisation d’un index de position pour extraire des colonnes (variables) particulières, nous pouvons également nommer les variables directement en utilisant les crochet [ ]. Par exemple, extrayons les 5 premières lignes des variables care, food et mane_size. Au lieu d’utiliser licornes[1:5, c(1, 2, 6)] nous pouvons utiliser :\n\nlicornes[1:5, c(\"p_care\", \"food\", \"mane_size\")]\n\n  p_care   food mane_size\n1   care medium      11.7\n2   care medium      14.1\n3   care medium       7.1\n4   care medium      11.9\n5   care medium      14.5\n\n\nEn général, on préférera utiliser cette méthode plutôt que l’index positionnel pour sélectionner les colonnes, car elle nous donnera toujours ce que nous voulons même si nous avons changé l’ordre des colonnes dans notre jeu de données pour une raison quelconque.\n\n3.4.2 Index logiques\nTout comme nous l’avons fait avec les vecteurs, nous pouvons également extraire des données de notre jeu de données sur la base d’un test logique. Nous pouvons utiliser tous les opérateurs logiques que nous avons utilisés dans les exemples des vecteurs. Si ceux-ci vous ont échappé, jetez un coup d’œil à la Section 2.5.1.1 pour vous rafraîchir la mémoire. Extrayons toutes les lignes pour lesquelles la variable height a une valeur supérieure à 12 et extrayons toutes les colonnes par défaut (rappelez-vous, si vous n’incluez pas d’index de colonne après la virgule, cela signifie toutes les colonnes).\n\ngrandes_cornes &lt;- licornes[licornes$height &gt; 12, ]\ngrandes_cornes\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n10   care medium     2   12.3  13.48      16.1       36.9          8\n17   care   high     1   12.6  18.66      18.6       54.0          9\n21   care   high     1   14.1  19.12      13.1      113.2         13\n32   care   high     2   17.2  19.20      10.9       89.9         14\n38   care    low     1   12.3  11.27      13.7       28.7          5\n\n\nRemarquez dans le code ci-dessus qu’il faut utiliser l’élément licornes$height pour le test logique. Si nous nommions simplement la variable height sans le nom du jeu de données, nous recevrions une erreur nous indiquant que R ne peut pas trouver la variable height. La raison en est que la variable height n’existe qu’à l’intérieur du jeu de données licornes vous devez donc indiquer à R où elle se trouve exactement.\ngrandes_cornes &lt;- licornes[height &gt; 12, ]\nError in `[.data.frame`(licornes, height &gt; 12, ) : \n  object 'height' not found\nComment cela fonctionne-t-il ? Le test logique est le suivant licornes$height &gt; 12 et R n’extraira que les lignes qui satisfont à cette condition logique. Si nous regardons la sortie de la seule condition logique, vous pouvez voir qu’elle renvoie un vecteur contenant TRUE si height est supérieur à 12 et FALSE si height n’est pas supérieur à 12.\n\nlicornes$height &gt; 12\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[37] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nNotre indice de ligne est donc un vecteur contenant soit TRUE ou FALSE et seulement les lignes qui sont TRUE sont sélectionnées.\nD’autres opérateurs couramment utilisés sont présentés ci-dessous :\n\nlicornes[licornes$height &gt;= 6, ] # valeurs supérieures ou égales à 6\n\nlicornes[licornes$height &lt;= 6, ] # valeurs inférieures ou égales à 6\n\nlicornes[licornes$height == 8, ] # valeurs égales à 8\n\nlicornes[licornes$height != 8, ] # valeurs différentes de 8\n\nNous pouvons également extraire des lignes en fonction de la valeur d’une chaîne de caractères ou d’un niveau de facteur. Extrayons toutes les lignes pour lesquelles la valeur de la variable food est égal à high (là encore, nous extrairons toutes les colonnes). Remarquez que la double égalité == doit être utilisé pour un test logique et que la chaîne de caractères doit être placée entre guillemets simples ou doubles (c.-à-d. \"high\").\n\nnourriture_bcp &lt;- licornes[licornes$food == \"high\", ]\nrbind(head(nourriture_bcp, n = 10), tail(nourriture_bcp, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n17    care high     1   12.6  18.66      18.6       54.0          9\n18    care high     1   10.0  18.07      16.9       90.5          3\n19    care high     1   10.0  13.29      15.8      142.7         12\n20    care high     1    8.5  14.33      13.2       91.4          5\n21    care high     1   14.1  19.12      13.1      113.2         13\n22    care high     1   10.1  15.49      12.6       77.2         12\n23    care high     1    8.5  17.82      20.5       54.4          3\n24    care high     1    6.5  17.13      24.1      147.4          6\n25    care high     2   11.5  23.89      14.3      101.5         12\n26    care high     2    7.7  14.77      17.2      104.5          4\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\nOu nous pouvons extraire toutes les lignes où food est différent de medium (en utilisant !=) et ne sélectionner que les colonnes 1 à 4.\n\nnourriture_pas_moyenne &lt;- licornes[licornes$food != \"medium\", 1:4]\nrbind(head(nourriture_pas_moyenne, n = 10), tail(nourriture_pas_moyenne, n = 10))\n\n    p_care food block height\n17    care high     1   12.6\n18    care high     1   10.0\n19    care high     1   10.0\n20    care high     1    8.5\n21    care high     1   14.1\n22    care high     1   10.1\n23    care high     1    8.5\n24    care high     1    6.5\n25    care high     2   11.5\n26    care high     2    7.7\n87 no_care  low     1    3.0\n88 no_care  low     1    3.7\n89 no_care  low     2    2.4\n90 no_care  low     2    5.7\n91 no_care  low     2    3.7\n92 no_care  low     2    3.2\n93 no_care  low     2    3.9\n94 no_care  low     2    3.3\n95 no_care  low     2    5.5\n96 no_care  low     2    4.4\n\n\nNous pouvons accroître la complexité de nos tests logiques en les combinant avec des des expressions booléennes comme nous l’avons fait pour les objets vectoriels. Par exemple, pour extraire toutes les lignes où la taille (height) est supérieure ou égale à 6 ET food est égal à medium ET care est égal à no_care nous combinons une série d’expressions logiques avec le symbole & :\n\nfaible_soins_non_taille6 &lt;- licornes[licornes$height &gt;= 6 &\n                                       licornes$food == \"medium\" &\n                                       licornes$p_care == \"no_care\", ]\nfaible_soins_non_taille6\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n51 no_care medium     1    7.5  13.60      13.6      122.2         11\n54 no_care medium     1    8.5  10.04      12.3      113.6          4\n61 no_care medium     2    8.0  11.43      12.6       43.2         14\n\n\nPour extraire des lignes sur la base d’une expression booléenne “OR” (“OU”), nous pouvons utiliser le symbole |. Extrayons toutes les lignes où la taille (height) est supérieure à 12,3 OU inférieure à 2,2.\n\ntaille2.2_12.3 &lt;- licornes[licornes$height &gt; 12.3 | licornes$height &lt; 2.2, ]\ntaille2.2_12.3\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n\n\nUne autre méthode pour sélectionner des parties d’un jeu de données sur la base d’une expression logique consiste à utiliser la fonction subset() au lieu des crochets [ ]. L’avantage d’utiliser subset() est qu’il n’est plus nécessaire d’utiliser le symbole $ pour spécifier des variables à l’intérieur du jeu de données, car le premier argument de la fonction est le nom du jeu de données à subdiviser. L’inconvénient est que subset() est moins flexible que la notation [ ].\n\nsoins_moy_2 &lt;- subset(licornes, p_care == \"care\" & food == \"medium\" & block == 2)\nsoins_moy_2\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n9    care medium     2   10.4  10.48      10.5       57.8          5\n10   care medium     2   12.3  13.48      16.1       36.9          8\n11   care medium     2   10.4  13.18      11.1       56.8         12\n12   care medium     2   11.0  11.56      12.6       31.3          6\n13   care medium     2    7.1   8.16      29.6        9.7          2\n14   care medium     2    6.0  11.22      13.0       16.4          3\n15   care medium     2    9.0  10.20      10.8       90.1          6\n16   care medium     2    4.5  12.55      13.4       14.4          6\n\n\nEt si vous ne voulez que certaines colonnes, vous pouvez utiliser l’arguement select = :\n\nuni_p_care &lt;- subset(licornes, p_care == \"care\" & food == \"medium\" & block == 2,\n  select = c(\"p_care\", \"food\", \"mane_size\")\n)\nuni_p_care\n\n   p_care   food mane_size\n9    care medium      10.5\n10   care medium      16.1\n11   care medium      11.1\n12   care medium      12.6\n13   care medium      29.6\n14   care medium      13.0\n15   care medium      10.8\n16   care medium      13.4\n\n\n\n3.4.3 Ordonner un jeu de données\nRappelez-vous lorsque nous avons utilisé la fonction order() pour ordonner un vecteur en fonction de l’ordre d’un autre vecteur (dans la Section 2.5.3). Cette fonction est très utile si vous souhaitez réorganiser les lignes de votre jeu de données. Par exemple, si nous voulons que toutes les lignes du jeu de données licornes soient classées par ordre croissant en fonction de la variable height et éditer toutes les colonnes par défaut :\n\ntaille_ord &lt;- licornes[order(licornes$height), ]\nhead(taille_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n\n\nNous pouvons également ordonner par ordre décroissant d’une variable (i.e. mane_size) en utilisant l’argument decreasing = TRUE :\n\ntaille_criniere_ord &lt;- licornes[order(licornes$mane_size, decreasing = TRUE), ]\nhead(taille_criniere_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n13    care medium     2    7.1   8.16      29.6        9.7          2\n24    care   high     1    6.5  17.13      24.1      147.4          6\n65 no_care   high     1    8.5  22.53      20.8      166.9         16\n23    care   high     1    8.5  17.82      20.5       54.4          3\n66 no_care   high     1    8.5  17.33      19.8      184.4         12\n73 no_care   high     2    4.7  13.42      19.8      124.7          5\n80 no_care   high     2    5.2  17.70      19.1      181.1          8\n17    care   high     1   12.6  18.66      18.6       54.0          9\n49 no_care medium     1    5.6  11.03      18.6       49.9          8\n\n\nNous pouvons même ordonner des jeux de données sur la base de plusieurs variables. Par exemple, pour ordonner le jeu de données licornes dans l’ordre croissant des deux variables block et height :\n\nbloc_taille_ord &lt;- licornes[order(licornes$block, licornes$height), ]\nhead(bloc_taille_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n87 no_care    low     1    3.0   9.93      12.0       56.6          6\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n88 no_care    low     1    3.7   7.03       7.9       36.7          5\n\n\nEt si nous voulions ordonner licornes par ordre croissant selon la variable block mais par ordre décroissant de height? Nous pouvons utiliser une astuce simple en ajoutant un - avant l’argument licornes$height lorsque nous utilisons la fonction order(). Cela transformera essentiellement toutes les valeurs de height en négatives, ce qui aura pour effet d’inverser l’ordre. Notez que cette astuce ne fonctionne qu’avec des variables numériques.\n\nbloc_invtaille_ord &lt;- licornes[order(licornes$block, -licornes$height), ]\nrbind(head(bloc_invtaille_ord, n = 10), tail(bloc_invtaille_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n21    care   high     1   14.1  19.12      13.1      113.2         13\n17    care   high     1   12.6  18.66      18.6       54.0          9\n38    care    low     1   12.3  11.27      13.7       28.7          5\n3     care medium     1   11.2  12.76       7.1       66.7         10\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n2     care medium     1   10.7  12.14      14.1       46.0         10\n4     care medium     1   10.4   8.78      11.9       20.3          1\n5     care medium     1   10.4  13.58      14.5       26.9          4\n22    care   high     1   10.1  15.49      12.6       77.2         12\n18    care   high     1   10.0  18.07      16.9       90.5          3\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n93 no_care    low     2    3.9   9.19      12.4       52.6          9\n91 no_care    low     2    3.7   8.10      10.5       60.5          6\n94 no_care    low     2    3.3   8.92      11.6       55.2          6\n92 no_care    low     2    3.2   7.45      14.1       38.1          4\n42    care    low     2    3.1   8.74      16.1       39.1          3\n76 no_care   high     2    2.6  18.88      16.4      181.5         14\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n\n\nSi nous voulons faire la même chose avec une variable de type facteur (ou caractère) comme food il faut utiliser la fonction xtfrm() sur cette variable, à l’intérieur de notre fonction order() :\n\ninvnourriture_taille_ord &lt;- licornes[order(-xtfrm(licornes$food), licornes$height), ]\nrbind(head(invnourriture_taille_ord, n = 10), tail(invnourriture_taille_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n58 no_care medium     2    3.9   9.07       9.6       90.4          7\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n52 no_care medium     1    4.1  12.58      13.9      136.6         11\n16    care medium     2    4.5  12.55      13.4       14.4          6\n60 no_care medium     2    4.5  13.68      14.8      125.5          9\n55 no_care medium     1    4.9   6.89       8.2       52.9          3\n29    care   high     2    9.2  13.26      11.3      108.0          9\n78 no_care   high     2    9.3  18.75      18.4      181.1         16\n18    care   high     1   10.0  18.07      16.9       90.5          3\n19    care   high     1   10.0  13.29      15.8      142.7         12\n22    care   high     1   10.1  15.49      12.6       77.2         12\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n25    care   high     2   11.5  23.89      14.3      101.5         12\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n\n\nIl est à noter que la variable food a été classée dans l’ordre alphabétique inverse et que la variable height a été ordonnée par valeurs croissantes, à l’intérieur de chaque niveau de food.\nSi nous voulions ordonner le jeu de données selon les niveau de la variable food, c.-à-d. low –&gt; medium –&gt; high au lieu de l’ordre alphabétique par défaut (high, low, medium), nous devons d’abord modifier l’ordre des niveaux de food dans le jeu de données à l’aide de la fonction factor(). Une fois que nous avons fait cela, nous pouvons utiliser la fonction order() comme d’habitude. Remarque : si vous lisez la version pdf de ce livre, la sortie a été tronquée pour économiser de l’espace.\n\nlicornes$food &lt;- factor(licornes$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nnourriture_ord &lt;- licornes[order(licornes$food), ]\nrbind(head(nourriture_ord, n = 10), tail(nourriture_ord, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n33    care  low     1    8.0   6.88       9.3       16.1          4\n34    care  low     1    8.0  10.23      11.9       88.1          4\n35    care  low     1    6.4   5.97       8.7        7.3          2\n36    care  low     1    7.6  13.05       7.2       47.2          8\n37    care  low     1    9.7   6.49       8.1       18.0          3\n38    care  low     1   12.3  11.27      13.7       28.7          5\n39    care  low     1    9.1   8.96       9.7       23.8          3\n40    care  low     1    8.9  11.48      11.1       39.4          7\n41    care  low     2    7.4  10.89      13.3        9.5          5\n42    care  low     2    3.1   8.74      16.1       39.1          3\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\n\n3.4.4 Ajout de colonnes et de lignes\nIl est parfois utile de pouvoir ajouter des lignes et des colonnes de données supplémentaires à nos jeux de données. Il existe plusieurs façons d’y parvenir (comme toujours en R !) en fonction des circonstances. Pour ajouter simplement des lignes supplémentaires à un jeu de données existant, nous pouvons utiliser la fonction rbind() (row/‘ligne’ - bind/‘liaison’)  et pour ajouter des colonnes, la fonction cbind() (columns/‘colonnes’ - bind/‘liaison’) . Créons quelques jeux de données test pour voir cela en action en utilisant notre vieille amie, la fonction data.frame() :\n\n# rbind pour les lignes ('rows')\ndf1 &lt;- data.frame(\n  id = 1:4, taille = c(120, 150, 132, 122),\n  poids = c(44, 56, 49, 45)\n)\ndf1\n\n  id taille poids\n1  1    120    44\n2  2    150    56\n3  3    132    49\n4  4    122    45\n\ndf2 &lt;- data.frame(\n  id = 5:6, taille = c(119, 110),\n  poids = c(39, 35)\n)\ndf2\n\n  id taille poids\n1  5    119    39\n2  6    110    35\n\ndf3 &lt;- data.frame(\n  id = 1:4, taille = c(120, 150, 132, 122),\n  poids = c(44, 56, 49, 45)\n)\ndf3\n\n  id taille poids\n1  1    120    44\n2  2    150    56\n3  3    132    49\n4  4    122    45\n\ndf4 &lt;- data.frame(localisation = c(\"UK\", \"CZ\", \"CZ\", \"UK\"))\ndf4\n\n  localisation\n1           UK\n2           CZ\n3           CZ\n4           UK\n\n\nNous pouvons utiliser la fonction rbind() pour ajouter les lignes du jeu de données df2 aux lignes de df1 et créer le nouveau jeu de données à df_lcomb :\n\ndf_lcomb &lt;- rbind(df1, df2)\ndf_lcomb\n\n  id taille poids\n1  1    120    44\n2  2    150    56\n3  3    132    49\n4  4    122    45\n5  5    119    39\n6  6    110    35\n\n\nEt cbind pour ajouter la colonne de df4 à la colonne df3 et l’assigner à df_ccomb :\n\ndf_ccomb &lt;- cbind(df3, df4)\ndf_ccomb\n\n  id height weight location\n1  1    120     44       UK\n2  2    150     56       CZ\n3  3    132     49       CZ\n4  4    122     45       UK\n\n\nUne autre situation dans laquelle l’ajout d’une nouvelle colonne à un jeu de données est utile est lorsque vous souhaitez effectuer une sorte de transformation sur une variable existante. Par exemple, supposons que nous voulions appliquer une transformation logarithmique, log10 à une variable existante, la variable taille dans le jeu de données df_rcomb que nous avons créée ci-dessus. Nous pourrions simplement créer une variable distincte contenant ces valeurs, mais il est préférable de créer cette variable directement en tant que nouvelle colonne dans notre jeu de données existant afin de conserver toutes nos données ensemble. Appelons cette nouvelle variable taille_log10.\n\n# transformation log10\ndf_lcomb$taille_log10 &lt;- log10(df_lcomb$taille)\ndf_lcomb\n\n  id taille poids taille_log10\n1  1    120    44     2.079181\n2  2    150    56     2.176091\n3  3    132    49     2.120574\n4  4    122    45     2.086360\n5  5    119    39     2.075547\n6  6    110    35     2.041393\n\n\nCette situation survient également lorsque nous voulons changer le type d’une variable existante dans un jeu de données. Par exemple, la variable id dans la base de données df_lcomb est une donnée de type numérique (utilisez la fonction str() ou class() pour le vérifier). Si nous voulions convertir id en un facteur à utiliser plus tard dans notre analyse, nous pouvons créer une nouvelle variable appelée id_f dans notre jeu de données et utiliser la fonction factor() pour convertir la variable id.\n\n# conversion en un facteur\ndf_lcomb$id_f &lt;- factor(df_lcomb$id)\ndf_lcomb\n\n  id taille poids taille_log10 id_f\n1  1    120    44     2.079181    1\n2  2    150    56     2.176091    2\n3  3    132    49     2.120574    3\n4  4    122    45     2.086360    4\n5  5    119    39     2.075547    5\n6  6    110    35     2.041393    6\n\nstr(df_lcomb)\n\n'data.frame':   6 obs. of  5 variables:\n $ id          : int  1 2 3 4 5 6\n $ taille      : num  120 150 132 122 119 110\n $ poids       : num  44 56 49 45 39 35\n $ taille_log10: num  2.08 2.18 2.12 2.09 2.08 ...\n $ id_f        : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6\n\n\n\n3.4.5 Fusionner des jeux de données\nAu lieu d’ajouter simplement des lignes ou des colonnes à un jeu de données, nous pouvons également fusionner deux jeux de données. Supposons que nous ayons un jeu de données contenant des informations taxonomiques sur certains invertébrés communs des côtes rocheuses du Royaume-Uni (appelé taxa) et un autre jeu de données qui contient des informations sur l’endroit où ils se trouvent habituellement sur le littoral rocheux (appelé zone). Nous pouvons fusionner ces jeux cadres de données pour produire un seul jeu de données contenant à la fois des informations taxonomiques et des informations sur l’emplacement. Commençons par créer ces deux jeux de données (en réalité, il vous suffira probablement d’importer vos différents ensembles de données).\n\ntaxa &lt;- data.frame(\n  GENUS = c(\"Patella\", \"Littorina\", \"Halichondria\", \"Semibalanus\"),\n  espece = c(\"vulgata\", \"littoria\", \"panacea\", \"balanoides\"),\n  famille = c(\"patellidae\", \"Littorinidae\", \"Halichondriidae\", \"Archaeobalanidae\")\n)\ntaxa\n\n         GENUS     espece          famille\n1      Patella    vulgata       patellidae\n2    Littorina   littoria     Littorinidae\n3 Halichondria    panacea  Halichondriidae\n4  Semibalanus balanoides Archaeobalanidae\n\nzone &lt;- data.frame(\n  genus = c(\n    \"Laminaria\", \"Halichondria\", \"Xanthoria\", \"Littorina\",\n    \"Semibalanus\", \"Fucus\"\n  ),\n  espece = c(\n    \"digitata\", \"panacea\", \"parietina\", \"littoria\",\n    \"balanoides\", \"serratus\"\n  ),\n  zone = c(\"v_bas\", \"bas\", \"v_haut\", \"bas_moy\", \"haut\", \"bas_moy\")\n)\nzone\n\n         genus     espece    zone\n1    Laminaria   digitata   v_bas\n2 Halichondria    panacea     bas\n3    Xanthoria  parietina  v_haut\n4    Littorina   littoria bas_moy\n5  Semibalanus balanoides    haut\n6        Fucus   serratus bas_moy\n\n\nPuisque nos deux jeux de données contiennent au moins une variable en commun (espece dans notre cas), nous pouvons simplement utiliser la fonction merge() (‘fusionner’) pour créer un nouveau jeu de données appelé taxa_zone.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   littoria    Littorina     Littorinidae    Littorina low_mid\n3    panacea Halichondria  Halichondriidae Halichondria     low\n\n\nRemarquez que le jeu de données fusionné ne contient que les lignes ayant des espèces dans les 2 jeux de données. Il existe également deux colonnes appelées GENUS et genus car la fonction merge() les traite comme deux variables différentes provenant des deux jeux de données.\nSi nous voulons inclure toutes les données des deux jeux de données, nous devrons utiliser l’argument all = TRUE dans la fonction merge(). Les valeurs manquantes seront incluses en tant que NA :\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, all = TRUE)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   digitata         &lt;NA&gt;             &lt;NA&gt;    Laminaria   v_low\n3   littoria    Littorina     Littorinidae    Littorina low_mid\n4    panacea Halichondria  Halichondriidae Halichondria     low\n5  parietina         &lt;NA&gt;             &lt;NA&gt;    Xanthoria  v_high\n6   serratus         &lt;NA&gt;             &lt;NA&gt;        Fucus low_mid\n7    vulgata      Patella       patellidae         &lt;NA&gt;    &lt;NA&gt;\n\n\nSi les noms des variables sur lesquelles vous souhaitez baser la fusion sont différents dans chaque jeux de données (par exemple GENUS et genus), vous pouvez spécifier les noms dans le premier jeu de données (appelé x) et dans le second jeu de données (appelé y) à l’aide des arguments by.x = et by.y =.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, by.x = \"GENUS\", by.y = \"genus\", all = TRUE)\ntaxa_zone\n\n         GENUS  species.x           family  species.y    zone\n1        Fucus       &lt;NA&gt;             &lt;NA&gt;   serratus low_mid\n2 Halichondria    panacea  Halichondriidae    panacea     low\n3    Laminaria       &lt;NA&gt;             &lt;NA&gt;   digitata   v_low\n4    Littorina   littoria     Littorinidae   littoria low_mid\n5      Patella    vulgata       patellidae       &lt;NA&gt;    &lt;NA&gt;\n6  Semibalanus balanoides Archaeobalanidae balanoides    high\n7    Xanthoria       &lt;NA&gt;             &lt;NA&gt;  parietina  v_high\n\n\nOu utiliser plusieurs noms de variables :\n\ntaxa_zone &lt;- merge(\n  x = taxa, y = zone, by.x = c(\"espece\", \"GENUS\"),\n  by.y = c(\"espece\", \"genus\"), all = TRUE\n)\ntaxa_zone\n\n      espece        GENUS          famille    zone\n1 balanoides  Semibalanus Archaeobalanidae    haut\n2   digitata    Laminaria             &lt;NA&gt;   v_bas\n3   littoria    Littorina     Littorinidae bas_moy\n4    panacea Halichondria  Halichondriidae     bas\n5  parietina    Xanthoria             &lt;NA&gt;  v_haut\n6   serratus        Fucus             &lt;NA&gt; bas_moy\n7    vulgata      Patella       patellidae    &lt;NA&gt;\n\n\n\n3.4.6 Remodeler des jeux de données\nLe remodelage des données dans différents formats est une tâche courante. Avec des données de type rectangulaire (les jeux de données ont le même nombre de lignes dans chaque colonne), vous rencontrerez deux formes principales de jeu de données : le format “long” (parfois appelé “empilé”) et le format “large”. Un exemple de jeu de données au format “long” est donné ci-dessous. Nous pouvons voir que chaque ligne représente une observation unique d’un sujet individuel et que chaque sujet peut avoir plusieurs lignes. Il en résulte une seule colonne de notre mesures.\n\ndonnees_longue &lt;- data.frame(\n  sujet = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 3),\n  sexe = rep(c(\"M\", \"F\", \"F\", \"M\"), each = 3),\n  condition = rep(c(\"controle\", \"cond1\", \"cond2\"), times = 4),\n  mesures = c(\n    12.9, 14.2, 8.7, 5.2, 12.6, 10.1, 8.9,\n    12.1, 14.2, 10.5, 12.9, 11.9\n  )\n)\ndonnees_longue\n\n   sujet sexe condition mesures\n1      A    M  controle    12.9\n2      A    M     cond1    14.2\n3      A    M     cond2     8.7\n4      B    F  controle     5.2\n5      B    F     cond1    12.6\n6      B    F     cond2    10.1\n7      C    F  controle     8.9\n8      C    F     cond1    12.1\n9      C    F     cond2    14.2\n10     D    M  controle    10.5\n11     D    M     cond1    12.9\n12     D    M     cond2    11.9\n\n\nNous pouvons également formater les mêmes données au format “large”, comme indiqué ci-dessous. Dans ce format, nous avons plusieurs observations de chaque sujet dans une seule ligne avec des mesures dans différentes colonnes (controle, cond1 et cond2). Il s’agit d’un format courant lorsqu’il s’agit de mesures répétées à partir d’unités d’échantillonnage.\n\ndonnees_large &lt;- data.frame(\n  sujet = c(\"A\", \"B\", \"C\", \"D\"),\n  sexe = c(\"M\", \"F\", \"F\", \"M\"),\n  controle = c(12.9, 5.2, 8.9, 10.5),\n  cond1 = c(14.2, 12.6, 12.1, 12.9),\n  cond2 = c(8.7, 10.1, 14.2, 11.9)\n)\ndonnees_large\n\n  sujet sexe controle cond1 cond2\n1     A    M     12.9  14.2   8.7\n2     B    F      5.2  12.6  10.1\n3     C    F      8.9  12.1  14.2\n4     D    M     10.5  12.9  11.9\n\n\nBien qu’il n’y ait pas de problème inhérent à l’un ou l’autre de ces formats, il est parfois nécessaire d’effectuer une conversion entre les deux, car certaines fonctions requièrent un format spécifique pour fonctionner. Le format le plus courant est le format “long”.\nIl existe de nombreuses façons de convertir ces deux formats, mais nous utiliserons les fonctions melt() et dcast() du paquet reshape2 📦 (vous devez d’abord l’installer). Les fonctions melt() est utilisée pour convertir les formats larges en formats longs. Le premier argument de la fonction melt() est la base de données que nous voulons faire fondre (dans notre cas wide_data). La fonction id.vars = c(\"subject\", \"sex\") est un vecteur des variables que vous souhaitez empiler, l’argument measured.vars = c(\"control\", \"cond1\", \"cond2\") identifie les colonnes des mesures dans les différentes conditions, l’argument variable.name = \"condition\" spécifie ce que vous voulez appeler la colonne empilée de vos différentes conditions dans votre cadre de données de sortie et l’argument value.name = \"measurement\" est le nom de la colonne de vos mesures empilées dans votre cadre de données de sortie.\n\nlibrary(reshape2)\ndonnees_large # rappelons-nous à quoi ressemble le format \"large\"\n\n  sujet sexe controle cond1 cond2\n1     A    M     12.9  14.2   8.7\n2     B    F      5.2  12.6  10.1\n3     C    F      8.9  12.1  14.2\n4     D    M     10.5  12.9  11.9\n\n# conversion du \"large\" en \"long\"\nmon_df_long &lt;- melt(\n  data = donnees_large, id.vars = c(\"sujet\", \"sexe\"),\n  vars.mesures = c(\"controle\", \"cond1\", \"cond2\"),\n  nom.variable = \"condition\", value.name = \"mesures\"\n)\nmon_df_long\n\n   sujet sexe variable mesures\n1      A    M controle    12.9\n2      B    F controle     5.2\n3      C    F controle     8.9\n4      D    M controle    10.5\n5      A    M    cond1    14.2\n6      B    F    cond1    12.6\n7      C    F    cond1    12.1\n8      D    M    cond1    12.9\n9      A    M    cond2     8.7\n10     B    F    cond2    10.1\n11     C    F    cond2    14.2\n12     D    M    cond2    11.9\n\n\nLa fonction dcast() est utilisée pour convertir un jeu de données au format “long” en un jeu de données au format “large”. Le premier argument est à nouveau le jeu de données que nous voulons convertir (donnees_longue pour cet exemple). Le deuxième argument est la syntaxe de la formule. L’argument sujet + sexe de la formule signifie que nous voulons garder ces colonnes séparées, et l’élément ~ condition est la colonne qui contient les étiquettes que nous voulons diviser en nouvelles colonnes dans notre nouveau jeu de données. La colonne var.valeur = \"mesures\" est la colonne qui contient les données mesurées.\n\ndonnees_longue # rappelons-nous à quoi ressemble le format \"long\"\n\n   sujet sexe condition mesures\n1      A    M  controle    12.9\n2      A    M     cond1    14.2\n3      A    M     cond2     8.7\n4      B    F  controle     5.2\n5      B    F     cond1    12.6\n6      B    F     cond2    10.1\n7      C    F  controle     8.9\n8      C    F     cond1    12.1\n9      C    F     cond2    14.2\n10     D    M  controle    10.5\n11     D    M     cond1    12.9\n12     D    M     cond2    11.9\n\n# conversion du \"long\" en \"large\"\nmon_df_large &lt;- dcast(\n  data = donnees_longue, sujet + sexe ~ condition,\n  var.valeur = \"mesures\"\n)\nmon_df_large\n\n  sujet sexe cond1 cond2 controle\n1     A    M  14.2   8.7     12.9\n2     B    F  12.6  10.1      5.2\n3     C    F  12.1  14.2      8.9\n4     D    M  12.9  11.9     10.5",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#introduction-au-tidyverse-lunivers-ordonné",
    "href": "03-donnees.html#introduction-au-tidyverse-lunivers-ordonné",
    "title": "3  Données",
    "section": "\n3.5 Introduction au tidyverse (L’univers ordonné)",
    "text": "3.5 Introduction au tidyverse (L’univers ordonné)\nil semble que ce ne soit pas très ordonné ici et que nous devons améliorer ça.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#résumer-des-jeux-de-données",
    "href": "03-donnees.html#résumer-des-jeux-de-données",
    "title": "3  Données",
    "section": "\n3.6 Résumer des jeux de données",
    "text": "3.6 Résumer des jeux de données\nMaintenant que nous sommes en mesure de manipuler et extraire des données de nos jeux de données, notre prochaine tâche est de commencer à explorer et connaître nos données. Dans cette section, nous commencerons à produire des tableaux de statistiques récapitulatives utiles sur les variables de notre jeu de données et, dans les deux chapitres suivants, nous aborderons la visualisation de nos données à l’aide de graphiques R de base et l’utilisation du paquet ggplot2 📦.\nUn point de départ très utile consiste à produire des statistiques récapitulatives simples pour toutes les variables de notre jeu de données licornes à l’aide de la fonction summary() :\n\nsummary(licornes)\n\n    p_care              food        block         height           weight      \n Length:96          low   :32   Min.   :1.0   Min.   : 1.200   Min.   : 5.790  \n Class :character   medium:32   1st Qu.:1.0   1st Qu.: 4.475   1st Qu.: 9.027  \n Mode  :character   high  :32   Median :1.5   Median : 6.450   Median :11.395  \n                                Mean   :1.5   Mean   : 6.840   Mean   :12.155  \n                                3rd Qu.:2.0   3rd Qu.: 9.025   3rd Qu.:14.537  \n                                Max.   :2.0   Max.   :17.200   Max.   :23.890  \n   mane_size       fluffyness       horn_rings    \n Min.   : 5.80   Min.   :  5.80   Min.   : 1.000  \n 1st Qu.:11.07   1st Qu.: 39.05   1st Qu.: 4.000  \n Median :13.45   Median : 70.05   Median : 6.000  \n Mean   :14.05   Mean   : 79.78   Mean   : 7.062  \n 3rd Qu.:16.45   3rd Qu.:113.28   3rd Qu.: 9.000  \n Max.   :49.20   Max.   :189.60   Max.   :17.000  \n\n\nPour les variables numériques (c’est-à-dire height, weight etc.), la moyenne, le minimum, le maximum, la médiane, le premier quartile (inférieur) et le troisième quartile (supérieur) sont présentés. Pour les variables factorielles (i.e. care et food), le nombre d’observations dans chacun des niveaux est indiqué. Si une variable contient des données manquantes, le nombre de NA est également indiqué.\nSi nous voulons résumer un sous-ensemble plus petit de variables dans notre jeu de données, nous pouvons utiliser nos compétences en matière d’indexation en combinaison avec la fonction summary(). Par exemple, pour résumer uniquement les variables height, weight, mane_size et fluffyness nous pouvons inclure les index de colonne appropriés avec les crochets [ ]. Remarquez que nous incluons toutes les lignes en ne spécifiant pas d’index de ligne :\n\nsummary(licornes[, 4:7])\n\n     height           weight         mane_size       fluffyness    \n Min.   : 1.200   Min.   : 5.790   Min.   : 5.80   Min.   :  5.80  \n 1st Qu.: 4.475   1st Qu.: 9.027   1st Qu.:11.07   1st Qu.: 39.05  \n Median : 6.450   Median :11.395   Median :13.45   Median : 70.05  \n Mean   : 6.840   Mean   :12.155   Mean   :14.05   Mean   : 79.78  \n 3rd Qu.: 9.025   3rd Qu.:14.537   3rd Qu.:16.45   3rd Qu.:113.28  \n Max.   :17.200   Max.   :23.890   Max.   :49.20   Max.   :189.60  \n\n# ou de manière équivalente\n# summary(licornes[, c(\"height\", \"weight\", \"mane_size\", \"fluffyness\")])\n\nEt pour résumer une seule variable :\n\nsummary(licornes$mane_size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.80   11.07   13.45   14.05   16.45   49.20 \n\n# ou de manière équivalente\n# summary(licornes[, 6])\n\nComme vous l’avez vu plus haut, le summary() indique le nombre d’observations dans chaque niveau de nos variables factorielles. Une autre fonction utile pour générer des tableaux de comptage est la fonction table(). La fonction table() peut être utilisée pour construire des tables de contingence de différentes combinaisons de niveaux de facteurs. Par exemple, pour compter le nombre d’observations pour chaque niveau de food :\n\ntable(licornes$food)\n\n\n   low medium   high \n    32     32     32 \n\n\nNous pouvons aller plus loin en produisant un tableau des effectifs pour chaque combinaison des niveau de food et care :\n\ntable(licornes$food, licornes$p_care)\n\n        \n         care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nUne version plus souple de la fonction table() est la fonction xtabs(). La fonction xtabs() utilise une notation de formule (~) pour construire des tables de contingence avec les variables de classification croisée séparées par un + à droite de la formule. xtabs() contient également un argument utile, data =, pour ne pas avoir à inclure le nom du jeu de données lors de la spécification de chaque variable :\n\nxtabs(~ food + p_care, data = licornes)\n\n        p_care\nfood     care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nNous pouvons même construire des tables de contingence plus compliquées en utilisant davantage de variables. Notez que dans l’exemple ci-dessous, la variable xtabs() a discrètement contraint notre block à être un facteur.\n\nxtabs(~ food + p_care + block, data = licornes)\n\n, , block = 1\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n, , block = 2\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n\nEt pour un tableau mieux formaté, nous pouvons imbriquer la fonction xtabs() à l’intérieur de la fonction ftable() pour “aplatir” le tableau.\n\nftable(xtabs(~ food + p_care + block, data = licornes))\n\n               block 1 2\nfood   p_care           \nlow    care          8 8\n       no_care       8 8\nmedium care          8 8\n       no_care       8 8\nhigh   care          8 8\n       no_care       8 8\n\n\nNous pouvons également résumer nos données pour chaque niveau d’une variable factorielle. Supposons que nous voulions calculer la valeur moyenne de height pour chacun de nos niveaux low, meadium et high de la variable food. Pour ce faire, nous utilisons la fonction mean() que nous appliquons à la variable height pour chaque niveau de food en utilisant la fonction tapply().\n\ntapply(licornes$height, licornes$food, mean)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nLa fonction tapply() n’est pas limitée au calcul des valeurs moyennes, vous pouvez l’utiliser pour appliquer de nombreuses fonctions fournies avec R ou même des fonctions que vous avez écrites vous-même (voir le Chapitre 5 pour plus de détails). Par exemple, nous pouvons appliquer la fonction sd() pour calculer l’écart-type pour chaque niveau de food ou même la fonction summary().\n\ntapply(licornes$height, licornes$food, sd)\n\n     low   medium     high \n2.828425 3.005345 3.483323 \n\ntapply(licornes$height, licornes$food, summary)\n\n$low\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   3.600   5.550   5.853   8.000  12.300 \n\n$medium\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   4.500   7.000   7.013   9.950  12.300 \n\n$high\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   5.800   7.450   7.653   9.475  17.200 \n\n\nRemarque : si la variable que vous souhaitez résumer contient des valeurs manquantes (NA), vous devrez également inclure un argument spécifiant comment vous souhaitez que la fonction traite les valeurs manquantes (NA). Nous en avons vu un exemple dans la Section 2.5.5 où la fonction mean() a renvoyé une valeur NA en cas de données manquantes. Pour inclure les NA, il suffit d’ajouter l’argument na.rm = TRUE lors de l’utilisation de tapply().\n\ntapply(licornes$height, licornes$food, mean, na.rm = TRUE)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nNous pouvons également utiliser tapply() pour appliquer des fonctions à plus d’un seul facteur. La seule chose à retenir est que les facteurs doivent être fournis à la fonction tapply() sous la forme d’une liste à l’aide de la fonction list(). Pour calculer la moyenne de height pour chaque combinaison de food et care nous pouvons utiliser la notation list(licornes$food, licornes$p_care).\n\ntapply(licornes$height, list(licornes$food, licornes$p_care), mean)\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nEt si vous en avez un peu marre d’avoir à écrire licornes$ pour chaque variable, vous pouvez imbriquer la notation tapply() à l’intérieur de la fonction with(). La fonction with() permet à R d’évaluer une expression par rapport à un objet de données nommé (dans ce cas, le jeu de données licornes).\n\nwith(licornes, tapply(height, list(food, p_care), mean))\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nLa fonction with() fonctionne également avec de nombreuses autres fonctions et peut vous faire économiser beaucoup de temps de frappe !\nUne autre fonction très utile pour résumer des données est la fonction aggregate(). La fonction aggregate() fonctionne de manière très similaire à la fonction tapply() mais est un peu plus flexible.\nPar exemple, pour calculer la moyenne des variables height, weight, mane_size et fluffyness pour chaque niveau de food :\n\naggregate(licornes[, 4:7], by = list(food = licornes$food), FUN = mean)\n\n    food   height    weight mane_size fluffyness\n1    low 5.853125  8.652812  11.14375    45.1000\n2 medium 7.012500 11.164062  13.83125    67.5625\n3   high 7.653125 16.646875  17.18125   126.6875\n\n\nDans le code ci-dessus, nous avons indexé les colonnes que nous voulons résumer dans le jeu de données licornes à l’aide de la notation licornes[, 4:7]. Les by = (“par =”) spécifie une liste de facteurs (list(food = licornes$food)) et l’argument FUN = désigne la fonction à appliquer (mean dans cet exemple).\nComme dans le cas de la fonction tapply() nous pouvons inclure plus d’un facteur sur lequel appliquer une fonction. Ici, nous calculons les valeurs moyennes pour chaque combinaison de food et care :\n\naggregate(licornes[, 4:7], by = list(\n  food = licornes$food,\n  p_care = licornes$p_care\n), FUN = mean)\n\n    food  p_care  height    weight mane_size fluffyness\n1    low    care 8.03750  9.016250   9.96250   30.30625\n2 medium    care 9.18750 11.011250  13.48750   40.59375\n3   high    care 9.60000 16.689375  15.54375   98.05625\n4    low no_care 3.66875  8.289375  12.32500   59.89375\n5 medium no_care 4.83750 11.316875  14.17500   94.53125\n6   high no_care 5.70625 16.604375  18.81875  155.31875\n\n\nNous pouvons également utiliser la fonction aggregate() d’une manière différente en utilisant la méthode de la formule (comme nous l’avons fait avec la fonction xtabs()). Dans la partie gauche de la formule (~), nous spécifions la variable à laquelle nous voulons appliquer la fonction mean() et, dans la partie droite, nos facteurs séparés par un symbole +. La méthode de la formule permet également d’utiliser l’argument data = pour des raisons de commodité.\n\naggregate(height ~ food + p_care, FUN = mean, data = licornes)\n\n    food  p_care  height\n1    low    care 8.03750\n2 medium    care 9.18750\n3   high    care 9.60000\n4    low no_care 3.66875\n5 medium no_care 4.83750\n6   high no_care 5.70625\n\n\nL’un des avantages de la méthode de la formule est qu’elle permet également d’utiliser l’argument subset = pour appliquer la fonction à des sous-ensembles des données originales. Par exemple, pour calculer la moyenne de height pour chaque combinaison de food et care mais seulement pour les licornes qui ont moins de 7 anneaux sur leur corne (horn_rings).\n\naggregate(height ~ food + p_care, FUN = mean, subset = horn_rings &lt; 7, data = licornes)\n\n    food  p_care   height\n1    low    care 8.176923\n2 medium    care 8.570000\n3   high    care 7.900000\n4    low no_care 3.533333\n5 medium no_care 5.316667\n6   high no_care 3.850000\n\n\nOu seulement pour les licornes du block 1.\n\naggregate(height ~ food + p_care, FUN = mean, subset = block == \"1\", data = licornes)\n\n    food  p_care  height\n1    low    care  8.7500\n2 medium    care  9.5375\n3   high    care 10.0375\n4    low no_care  3.3250\n5 medium no_care  5.2375\n6   high no_care  5.9250",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#exporter-des-données",
    "href": "03-donnees.html#exporter-des-données",
    "title": "3  Données",
    "section": "\n3.7 Exporter des données",
    "text": "3.7 Exporter des données\nNous espérons que vous avez maintenant une idée de la puissance et de l’utilité de R pour manipuler et résumer des données (et nous n’avons fait qu’effleurer la surface). L’un des grands avantages de l’utilisation de R pour manipuler vos données est que vous disposez d’un enregistrement permanent de tout ce que vous avez fait avec vos données. Fini le temps des modifications non documentées dans Excel ou Calc ! En traitant vos données en “lecture seule” et en documentant toutes vos décisions dans R, vous aurez fait un grand pas en avant pour rendre votre analyse plus reproductible et plus transparente pour les autres. Il est toutefois important de savoir que les modifications apportées à votre base de données dans R ne changeront pas le fichier de données original que vous avez importé dans R (et c’est une bonne chose). Heureusement, il est facile d’exporter des cadres de données vers des fichiers externes dans une grande variété de formats.\n\n3.7.1 Fonctions d’exportation\nLa principale fonction d’exportation de jeux de données est write.table(). Comme pour la fonction read.table() la fonction write.table() est très flexible et dispose de nombreux arguments permettant de personnaliser son comportement. Prenons l’exemple de notre jeu de données d’origine licornes et exportons ces modifications vers un fichier externe.\nDe la même manière que pour read.table(), write.table() possède une série de fonctions avec des valeurs par défaut spécifiques au format, telles que write.csv() et write.delim() qui utilisent respectivement “,” et les tabulations comme délimiteurs, et incluent les noms de colonnes par défaut.\nOrdonnons les lignes du jeu de données par ordre croissant de la variable height à l’intérieur de chaque niveau de food. Nous appliquerons également une transformation racine carrée à la variable du nombre d’anneaux de la corne (horn_rings) et une transformation logarithmique, log10 sur la variable height et enregistrons ça en tant que colonnes supplémentaires dans notre jeu de données (nous espérons que cela vous est familier maintenant !).\n\nlicornes_df2 &lt;- licornes[order(licornes$food, licornes$height), ]\nlicornes_df2$horn_rings_sqrt &lt;- sqrt(licornes_df2$horn_rings)\nlicornes_df2$log10_height &lt;- log10(licornes_df2$height)\nstr(licornes_df2)\n\n'data.frame':   96 obs. of  10 variables:\n $ p_care         : chr  \"no_care\" \"no_care\" \"no_care\" \"no_care\" ...\n $ food           : Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ block          : int  1 1 1 2 1 2 2 2 1 2 ...\n $ height         : num  1.8 2.2 2.3 2.4 3 3.1 3.2 3.3 3.7 3.7 ...\n $ weight         : num  6.01 9.97 7.28 9.1 9.93 8.74 7.45 8.92 7.03 8.1 ...\n $ mane_size      : num  17.6 9.6 13.8 14.5 12 16.1 14.1 11.6 7.9 10.5 ...\n $ fluffyness     : num  46.2 63.1 32.8 78.7 56.6 39.1 38.1 55.2 36.7 60.5 ...\n $ horn_rings     : int  4 2 6 8 6 3 4 6 5 6 ...\n $ horn_rings_sqrt: num  2 1.41 2.45 2.83 2.45 ...\n $ log10_height   : num  0.255 0.342 0.362 0.38 0.477 ...\n\n\nNous pouvons maintenant exporter notre nouveau jeu de données licornes_df2 à l’aide de la fonction write.table(). Le premier argument est le jeu de données que vous voulez exporter (licornes_df2 dans notre exemple). Nous donnons ensuite le nom du fichier que nous voulons créer (avec son extension) et le chemin d’accès au fichier entre guillemets simples ou doubles en utilisant l’argument file =. Dans cet exemple, nous exportons le jeu de données vers un fichier appelé licornes_transformee.csv dans le répertoire donnees. L’argument row.names = FALSE empêche R d’inclure les noms des lignes dans la première colonne du fichier.\n\nwrite.csv(licornes_transformee,\n  file = \"donnees/licornes_transformee.csv\", \n  row.names = FALSE\n)\n\nComme nous avons enregistré le fichier en tant que fichier texte délimité par des virgules (CSV), nous pouvons ouvrir ce fichier dans n’importe quel éditeur de texte.\nNous pouvons bien sûr exporter nos fichiers dans une variété d’autres formats.\n\n3.7.2 Autres fonctions d’exportation\nComme pour l’importation de fichiers de données dans R, il existe également de nombreuses fonctions alternatives pour exporter des données vers des fichiers externes, en plus de la fonction write.table(). Si vous avez suivi la rubrique “Autres fonctions d’importation” Section 3.3.4 de ce chapitre, vous aurez déjà installé les paquets nécessaires.\nLa fonction fwrite() du paquet data.table 📦 est très efficace pour exporter des objets de données volumineux et est beaucoup plus rapide que la fonction write.table(). Il est également assez simple à utiliser car il possède la plupart des arguments de la fonction write.table(). Pour exporter un fichier texte délimité par des tabulations, il suffit de spécifier le nom du jeu de données, le nom et le chemin du fichier de sortie et le séparateur entre les colonnes :\n\nlibrary(data.table)\nfwrite(licornes_df2, file = \"donnees/licornes_04_12.txt\", sep = \"\\t\")\n\nPour exporter un fichier csv délimité, c’est encore plus facile car nous n’avons même pas besoin d’inclure l’option sep = :\n\nlibrary(data.table)\nfwrite(licornes_df2, file = \"donnees/licornes_04_12.csv\")\n\nLe paquet readr 📦 est également fourni avec deux fonctions utiles pour exporter rapidement des données dans des fichiers externes : la fonction write_tsv() pour écrire des fichiers délimités par des tabulations et la fonction write_csv() pour enregistrer des fichiers de valeurs séparées par des virgules (csv).\n\nlibrary(readr)\nwrite_tsv(licornes_df2, path = \"donnees/licornes_04_12.txt\")\n\nwrite_csv(licornes_df2, path = \"donnees/licornes_04_12.csv\")",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "03-donnees.html#footnotes",
    "href": "03-donnees.html#footnotes",
    "title": "3  Données",
    "section": "",
    "text": "Pour des raisons de place et de simplicité, nous ne montrons que les cinq premières et cinq dernières lignes.↩︎",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Données</span>"
    ]
  },
  {
    "objectID": "04-graphique.html",
    "href": "04-graphique.html",
    "title": "4  Figures",
    "section": "",
    "text": "4.1 Tracés de base simples en R\nIl existe de nombreuses fonctions dans R pour produire des graphiques, des plus simples aux plus complexes. Il est impossible de couvrir tous les aspects de la production de graphiques en R dans ce livre. Nous vous présenterons donc la plupart des méthodes courantes de représentation graphique des données et nous décrirons comment personnaliser vos graphiques plus tard dans Section 4.5.\nLa fonction de haut niveau la plus couramment utilisée pour produire des graphiques en R est (sans surprise) la fonction plot() fonction. Par exemple, traçons le graphique weight de licornes de notre unicorns que nous avons importé dans Section 3.3.2.\nunicorns &lt;- read.csv(file = \"data/unicorns.csv\")\n\nplot(unicorns$weight)\nR a tracé les valeurs de weight (sur l’axe des y) en fonction d’un indice puisque nous ne traçons qu’une seule variable. L’indice est simplement l’ordre des weight dans le cadre de données (1 en premier dans le cadre de données et 97 en dernier). L’indice weight a été automatiquement inclus comme étiquette de l’axe des y et les échelles des axes ont été automatiquement définies.\nSi nous n’avions inclus que la variable weight plutôt que unicorns$weight, l’indicateur plot() affichera une erreur car la variable weight n’existe que dans le unicorns l’objet “data frame”.\nplot(weight)\n## Error in plot(weight) : object 'weight' not found\nComme de nombreuses fonctions de traçage de base de R n’ont pas d’objet data = pour spécifier directement le nom de la base de données, nous pouvons utiliser la fonction with() en combinaison avec la fonction plot() comme raccourci.\nwith(unicorns, plot(weight))\nPour tracer un nuage de points d’une variable numérique par rapport à une autre variable numérique, il suffit d’inclure les deux variables en tant qu’arguments lors de l’utilisation de la fonction plot() comme arguments. Par exemple, pour tracer fluffyness sur l’axe des y et weight de l’axe des x.\nplot(x = unicorns$weight, y = unicorns$fluffyness)\nIl existe une approche équivalente pour ces types de parcelles, ce qui est souvent source de confusion au début. Vous pouvez également utiliser la notation de formule lors de l’utilisation de la fonction plot() lorsque vous utilisez la fonction Cependant, contrairement à la méthode précédente, la méthode de la formule exige que vous spécifiiez d’abord la variable de l’axe des y, puis un ~ puis la variable de l’axe des x.\nplot(fluffyness ~ weight, data = unicorns)\n\n\n\n\n\n\nFigure 4.2\nCes deux approches étant équivalentes, nous vous suggérons de choisir celle que vous préférez et de l’appliquer.\nVous pouvez également spécifier le type de graphique que vous souhaitez tracer en utilisant l’argument type =. Vous pouvez tracer uniquement les points (type = \"p\" c’est l’option par défaut), seulement les lignes (type = \"l\"), les points et les lignes connectés (type = \"b\"), les points et les lignes avec les lignes passant par les points (type = \"o\") et les points vides reliés par des lignes (type = \"c\"). Par exemple, utilisons nos connaissances acquises lors de Section 2.4 pour générer deux vecteurs de nombres (my_x et my_y), puis tracer l’un par rapport à l’autre en utilisant différents type = pour voir quels types de tracés sont produits. Ne vous préoccupez pas des par(mfrow = c(2, 2)) ligne de code pour l’instant. Nous l’utilisons simplement pour diviser le dispositif de traçage afin de pouvoir placer les quatre tracés sur le même dispositif pour gagner de la place. Voir Section 4.4 dans le chapitre pour plus de détails à ce sujet. Le graphique en haut à gauche est type = \"l\", le graphe en haut à droite type = \"b\" en bas à gauche type = \"o\" et en bas à droite est type = \"c\".\nmy_x &lt;- 1:10\nmy_y &lt;- seq(from = 1, to = 20, by = 2)\n\npar(mfrow = c(2, 2))\nplot(my_x, my_y, type = \"l\")\nplot(my_x, my_y, type = \"b\")\nplot(my_x, my_y, type = \"o\")\nplot(my_x, my_y, type = \"c\")\nIl est vrai que les parcelles que nous avons produites jusqu’à présent n’ont rien d’extraordinaire. Cependant, les plot() est incroyablement polyvalente et peut générer un large éventail de tracés que vous pouvez personnaliser à votre guise. Nous verrons comment personnaliser les ggplots dans Section 4.5. Pour l’anecdote, la fonction plot() est également ce que l’on appelle une fonction générique, ce qui signifie qu’elle peut modifier son comportement par défaut en fonction du type d’objet utilisé comme argument. Vous en verrez un exemple dans Section 9.6 où nous utilisons la fonction plot() pour générer des graphiques de diagnostic des résidus d’un objet de modèle linéaire (je parie que vous ne pouvez pas attendre !).",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphique.html#ggplot2",
    "href": "04-graphique.html#ggplot2",
    "title": "4  Figures",
    "section": "\n4.2 ggplot2",
    "text": "4.2 ggplot2\nComme nous l’avons déjà mentionné ggplot la grammaire nécessite plusieurs éléments pour produire un graphique (Figure 4.1) et un minimum de 3 éléments est nécessaire :\n\nun cadre de données\nun système de cartographie définissant x et y\nune couche géométrique\n\nLes données et la cartographie sont fournies dans le cadre de l’appel à l’application ggplot() à l’aide de la fonction data et mapping arguments. La couche géométrique est ajoutée à l’aide de fonctions spécifiques.\nEn fait, toutes les couches sont nécessaires, mais les valeurs simples par défaut des autres couches sont automatiquement fournies.\nPour refaire la Figure 4.2, qui ne contient qu’un nuage de points, nous pouvons utiliser la commande geom_point() fonction.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\n\n\n\n\n\nFigure 4.3\n\n\n\n\nMaintenant que nous avons une compréhension de base de ggplotnous pouvons explorer quelques graphiques en utilisant à la fois le code de base de R et le code ggplot",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphique.html#tracés-simples",
    "href": "04-graphique.html#tracés-simples",
    "title": "4  Figures",
    "section": "\n4.3 Tracés simples",
    "text": "4.3 Tracés simples\n\n4.3.1 Diagrammes de dispersion\nType de diagramme simple très utile pour étudier la relation entre deux variables, par exemple. Voici le code pour le faire en utilisant la base R (Figure 4.2)\n\nplot(fluffyness ~ weight, data = unicorns)\n\nou ggplot (Figure 4.3)\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\nUn grand avantage de ggplot pour les nuages de points simples est la facilité avec laquelle nous pouvons ajouter une régression, une ligne plus lisse (loes ou gam) au tracé en utilisant stat_smooth()pour ajouter une couche statistique au graphique.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point() +\n  stat_smooth()\n\n\n\n\n\n\n\n\n4.3.2 Histogrammes\nLes histogrammes de fréquence sont utiles pour se faire une idée de la distribution des valeurs d’une variable numérique. En utilisant la base R, la fonction hist() prend un tableau numérique comme argument principal. Dans ggplot, nous devons utiliser geom_histogram(). Générons un histogramme de la height valeurs.\nAvec la base R\n\nhist(unicorns$height)\n\n\n\n\n\n\n\navec ggplot2\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nLes hist() et geom_histogram() crée automatiquement les points de rupture (ou bins) dans l’histogramme, à moins que vous n’indiquiez le contraire à l’aide de la fonction breaks = pour spécifier le contraire. Par exemple, disons que nous voulons tracer notre histogramme avec des points de rupture tous les 1 cm de hauteur des licornes. Nous générons d’abord une séquence allant de zéro à la valeur maximale de height (18 arrondi vers le haut) par pas de 1 à l’aide de la fonction seq() à l’aide de la fonction Nous pouvons ensuite utiliser cette séquence avec la fonction breaks = argument. Pendant que nous y sommes, remplaçons également le titre moche par quelque chose d’un peu mieux en utilisant l’option main = argument\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height, breaks = brk, main = \"Unicorn height\")\n\n\n\n\n\n\n\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\nVous pouvez également afficher l’histogramme sous forme de proportion plutôt que de fréquence en utilisant l’argument freq = FALSE à l’argument hist() ou en indiquant aes(y = after_stat(density)) dans geom_histogram().\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\nUne alternative au tracé d’un simple histogramme est d’ajouter une densité du noyau au tracé. Dans la version de base de R, vous devez d’abord calculer les estimations de la densité du noyau à l’aide de la fonction density() puis ajouter les estimations à un tracé sous forme de ligne à l’aide de la fonction lines() pour tracer une ligne.\n\ndens &lt;- density(unicorns$height)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nlines(dens)\n\n\n\n\n\n\n\nAvec ggplot, vous pouvez simplement ajouter la fonction geom_density() au tracé\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  geom_density() +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\n\n4.3.3 Diagrammes en boîte\nD’accord, nous allons le dire franchement, nous adorons les diagrammes en boîte et leur relation étroite avec le diagramme en violon. Les boxplots (ou box-and-whisker plots pour leur nom complet) sont très utiles pour résumer graphiquement la distribution d’une variable, identifier d’éventuelles valeurs inhabituelles et comparer les distributions entre différents groupes. La raison pour laquelle nous les aimons est leur facilité d’interprétation, leur transparence et leur rapport données/encre relativement élevé (c’est-à-dire qu’ils sont plus faciles à interpréter qu’à utiliser). ils transmettent efficacement une grande quantité d’informations). Nous vous suggérons d’utiliser les diagrammes en boîte autant que possible lorsque vous explorez vos données et d’éviter la tentation d’utiliser le diagramme en barres plus omniprésent (même avec des barres d’erreur standard ou d’intervalles de confiance à 95 %). Le problème des diagrammes en bâtons (ou diagrammes en dynamite) est qu’ils cachent au lecteur des informations importantes telles que la distribution des données et qu’ils supposent que les barres d’erreur (ou les intervalles de confiance) sont symétriques par rapport à la moyenne. Bien sûr, c’est à vous de décider ce que vous faites, mais si vous êtes tenté d’utiliser des diagrammes à barres, cherchez “dynamite plots are evil” (les diagrammes de dynamite sont diaboliques) ou voyez ici ou ici pour une discussion plus complète.\nPour créer un diagramme en boîte dans R, nous utilisons la fonction boxplot() fonction. Par exemple, créons un diagramme en boîte de la variable weight à partir de notre unicorns cadre de données. Nous pouvons également inclure une étiquette pour l’axe des y en utilisant la fonction ylab = à l’aide de l’argument\n\nboxplot(unicorns$weight, ylab = \"weight (g)\")\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight)) +\n  geom_boxplot() +\n  labs(y = \"weight (g)\")\n\n\n\n\n\n\n\nLa ligne horizontale épaisse au milieu de la boîte est la valeur médiane de weight (environ 11 g). La ligne supérieure de la boîte est le quartile supérieur (75e percentile) et la ligne inférieure est le quartile inférieur (25e percentile). La distance entre les quartiles supérieur et inférieur est appelée l’intervalle interquartile et représente les valeurs de weight pour 50 % des données. Les lignes verticales en pointillés sont appelées moustaches et leur longueur est égale à 1,5 x l’intervalle interquartile. Les points de données qui sont tracés en dehors des moustaches représentent des observations inhabituelles potentielles. Cela ne signifie pas qu’ils sont inhabituels, mais simplement qu’ils méritent un examen plus approfondi. Nous recommandons d’utiliser les boxplots en combinaison avec les dotplots de Cleveland pour identifier les observations potentiellement inhabituelles (voir les Section 4.3.5 pour plus de détails). Ce qui est intéressant avec les boxplots, c’est qu’ils ne fournissent pas seulement une mesure de la tendance centrale (la valeur médiane), mais qu’ils vous donnent également une idée de la distribution des données. Si la ligne médiane se trouve plus ou moins au milieu de la boîte (entre les quartiles supérieur et inférieur) et que les moustaches sont plus ou moins de la même longueur, vous pouvez être raisonnablement sûr que la distribution de vos données est symétrique.\nSi nous voulons examiner comment la distribution d’une variable change entre différents niveaux d’un facteur, nous devons utiliser la notation de la formule avec l’attribut boxplot() avec la fonction Par exemple, traçons notre weight mais cette fois-ci, voyons comment elle évolue avec chaque niveau de food. Lorsque nous utilisons la notation de la formule avec boxplot() nous pouvons utiliser la notation data = afin d’économiser de la frappe. Nous introduirons également une étiquette pour l’axe des x à l’aide de la fonction xlab = à l’aide de l’argument\n\nboxplot(weight ~ food,\n  data = unicorns,\n  ylab = \"Weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nLes niveaux des facteurs sont représentés dans l’ordre défini par notre variable factorielle food (souvent par ordre alphabétique). Pour modifier l’ordre, nous devons changer l’ordre de nos niveaux du facteur food dans notre cadre de données à l’aide de la fonction factor() puis redessiner le graphique. Traçons notre diagramme en boîte avec nos niveaux de facteurs allant de low à high.\n\nunicorns$food &lt;- factor(unicorns$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nNous pouvons également regrouper nos variables par deux facteurs dans le même graphique. Traçons notre weight mais, cette fois, traçons une boîte séparée pour chaque food et le traitement des soins parentaux (p_care).\n\nboxplot(weight ~ food * p_care,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\") +\n  facet_grid(.\n  ~ p_care)\n\n\n\n\n\n\n\nCe graphique est beaucoup plus intéressant dans ggplot, l’utilisation de facet_grid permettant de réaliser des graphiques similaires en fonction d’une troisième (ou même d’une quatrième) variable.\n\n4.3.4 Tracés de violon\nLes diagrammes en forme de violon sont une combinaison d’un diagramme en boîte et d’un diagramme de densité de noyau (vous avez vu un exemple de diagramme de densité de noyau dans la section histogramme ci-dessus), le tout en une seule figure. Nous pouvons créer un diagramme en violon dans R à l’aide de la fonction vioplot() à partir de la fonction vioplot du paquet. Vous devrez d’abord installer ce paquet en utilisant install.packages('vioplot') comme d’habitude. L’avantage de l’option vioplot() est qu’elle s’utilise à peu près de la même manière que la fonction boxplot() fonction. Nous utiliserons également l’argument col = \"lightblue\" pour changer la couleur de remplissage en bleu clair.\n\nlibrary(vioplot)\nvioplot(weight ~ food,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food Concentration\",\n  col = \"lightblue\"\n)\n\n\n\n\n\n\n\nDans le diagramme de violon ci-dessus, nous avons notre diagramme en boîte familier pour chaque food mais cette fois, la valeur médiane est représentée par un cercle blanc. Autour de chaque diagramme en boîte figure le diagramme de densité de noyau qui représente la distribution des données pour chaque niveau d’alimentation.\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\n\n4.3.5 Graphiques en pointillés\nIl est extrêmement important d’identifier les observations inhabituelles (appelées “valeurs aberrantes”) dans les variables numériques, car elles peuvent influencer les estimations des paramètres de votre modèle statistique ou indiquer une erreur dans vos données. Un graphique très utile (bien que sous-estimé) pour aider à identifier les valeurs aberrantes est le graphique en pointillés de Cleveland. Vous pouvez produire un graphique en pointillés en R très simplement en utilisant la commande dotchart() pour produire un graphique en pointillés.\n\ndotchart(unicorns$height)\n\n\n\n\n\n\n\nDans le diagramme en pointillés ci-dessus, les données de la base de données height sont représentées le long de l’axe des x et les données sont représentées dans l’ordre dans lequel elles apparaissent dans la fonction unicorns sur l’axe des y (les valeurs situées en haut de l’axe des y apparaissent plus tard dans la base de données et celles situées plus bas apparaissent au début de la base de données). Sur ce graphique, une seule valeur s’étend vers la droite à environ 17 cm, mais elle ne semble pas particulièrement importante par rapport aux autres. Un exemple de diagramme à points avec une observation inhabituelle est donné ci-dessous.\n\n\n\n\n\n\n\n\nNous pouvons également regrouper les valeurs dans notre height par une variable factorielle telle que food en utilisant le groups = argument. Ceci est utile pour identifier des observations inhabituelles au sein d’un niveau de facteur qui pourraient être masquées lorsque l’on examine toutes les données ensemble.\n\ndotchart(unicorns$height, groups = unicorns$food)\n\n\n\n\n\n\n\n\n\nggdotchart(data = unicorns, x = \"height\", y = \"food\")\n\n\n\n\n\n\n\n\n4.3.6 Diagrammes de paires\nDans ce chapitre, nous avons déjà utilisé la fonction plot() pour créer un nuage de points afin d’explorer la relation entre deux variables numériques. Dans le cas d’ensembles de données contenant de nombreuses variables numériques, il est souvent utile de créer plusieurs diagrammes de dispersion pour visualiser les relations entre toutes ces variables. Nous pouvons utiliser la fonction plot() pour créer chacun de ces diagrammes individuellement, mais il est beaucoup plus facile d’utiliser la fonction pairs() fonction. La fonction pairs() crée un nuage de points à plusieurs panneaux (parfois appelé matrice de nuage de points) qui représente toutes les combinaisons de variables. Créons un nuage de points multi-panneaux de toutes les variables numériques de notre unicorns cadre de données. Notez que vous devrez peut-être cliquer sur le bouton “Zoom” dans RStudio pour afficher clairement le graphique.\n\npairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n# or we could use the equivalent\n# pairs(unicorns[, 4:8])\n\nIl faut un peu de temps pour s’habituer à l’interprétation du diagramme des paires. Les panneaux sur la diagonale indiquent les noms des variables. La première rangée de diagrammes affiche les height sur l’axe des y et les variables weight, mane_size, fluffyness et unicorns sur l’axe des x pour chacun des quatre graphiques respectivement. La rangée suivante de placettes comporte weight sur l’axe des y et height, mane_size, fluffyness et unicorns sur l’axe des x. Nous interprétons les autres lignes de la même manière, la dernière ligne affichant la valeur unicorns sur l’axe des y et les autres variables sur l’axe des x. Nous espérons que vous remarquerez que les graphiques situés sous la diagonale sont les mêmes que ceux situés au-dessus de la diagonale, mais avec l’axe inversé.\nPour réaliser des tracés par paires avec ggplot, vous avez besoin de l’option ggpairs()de GGallypaquet. La sortie est assez similaire mais vous n’avez que la partie inférieure de la matrice des tracés, vous obtenez un tracé de densité sur la diagonale et les corrélations sur la partie supérieure du tracé.\n\nggpairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n\nLes pairs() peut être modifiée pour faire des choses similaires et plus, mais elle est plus complexe. Jetez un coup d’œil à l’excellent fichier d’aide de la fonction pairs() (?pairs), qui fournit tous les détails permettant de faire quelque chose comme le tracé ci-dessous.\n\n\n\n\n\n\n\n\n\n4.3.7 Tracés\nLorsque l’on examine la relation entre deux variables numériques, il est souvent utile de pouvoir déterminer si une troisième variable obscurcit ou modifie la relation. Un graphique très pratique à utiliser dans ces situations est un graphique de conditionnement (également connu sous le nom de graphique de dispersion conditionnel) que nous pouvons créer dans R à l’aide de la fonction coplot() fonction. La fonction coplot() trace deux variables, mais chaque tracé est conditionné (|) par une troisième variable. Cette troisième variable peut être soit numérique, soit un facteur. À titre d’exemple, voyons comment la relation entre le nombre de licornes (unicorns ) et le weight de licornes change en fonction de mane_size. Notez que les coplot() a une fonction data = il n’est donc pas nécessaire d’utiliser l’argument $ la notation.\n\ncoplot(horn_rings ~ weight | mane_size, data = unicorns)\n\n\n\n\n\n\n\n\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = mane_size\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIl faut un peu de pratique pour interpréter les coplots. Le nombre de licornes est représenté sur l’axe des y et le poids des licornes sur l’axe des x. Les six diagrammes montrent la relation entre ces deux variables pour différents intervalles de surface foliaire. Le diagramme à barres en haut indique la plage de valeurs de la surface foliaire pour chacun des diagrammes. Les panneaux sont lus de bas en gauche à haut en droite le long de chaque ligne. Par exemple, le panneau inférieur gauche montre la relation entre le nombre de licornes et le poids des licornes dont la surface foliaire est la plus faible (environ 5 - 11 cm2). Le graphique en haut à droite montre la relation entre le nombre de licornes et le poids des licornes dont la surface foliaire est comprise entre 16 et 50 cm2. Remarquez que la plage de valeurs de la surface foliaire diffère d’un panneau à l’autre et que les plages se chevauchent d’un panneau à l’autre. Les coplot() fait de son mieux pour diviser les données afin de s’assurer qu’il y a un nombre adéquat de points de données dans chaque panneau. Si vous ne souhaitez pas produire des graphiques avec des données qui se chevauchent dans le panneau, vous pouvez définir l’option overlap = à overlap = 0\nVous pouvez également utiliser l’option coplot() avec des variables de conditionnement factorielles. Avec gg_coplot() vous devez d’abord définir le facteur comme numérique avant de tracer le graphique et spécifier overlap=0. Par exemple, nous pouvons examiner la relation entre unicorns et weight conditionnées par le facteur food. Le graphique en bas à gauche représente la relation entre unicorns et weight pour les licornes de la low traitement alimentaire. Le graphique en haut à gauche montre la même relation mais pour les licornes dans le traitement alimentaire. high traitement alimentaire.\n\ncoplot(horn_rings ~ weight | food, data = unicorns)\n\n\n\n\n\n\n\n\nunicorns &lt;- mutate(unicorns, food_num = as.numeric(food))\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = food_num, overlap = 0\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n4.3.8 Résumé de la fonction du graphique\n\n\n\n\n\n\n\nType de graphique\nggplot2\nFonction de base de R\n\n\n\nnuage de points\ngeom_point()\nplot()\n\n\nhistogramme de fréquence\ngeom_histogram()\nhist()\n\n\ndiagramme en boîte\ngeom_boxplot()\nboxplot()\n\n\nCleveland dotplot\nggdotchart()\ndotchart()\n\n\nmatrice de nuage de points\nggpairs()\npairs()\n\n\ngraphique de conditionnement\ngg_coplot()\ncoplot()\n\n\n\nNous espérons que vous avez compris qu’il est possible de créer facilement des graphiques exploratoires très instructifs à l’aide des graphiques de base de R ou de ggplot. Le choix de l’un ou l’autre est entièrement libre (c’est ce qui fait l’intérêt de R, vous pouvez choisir) et nous mélangeons volontiers les deux pour répondre à nos besoins. Dans la section suivante, nous verrons comment personnaliser vos graphiques R de base pour leur donner l’aspect que vous souhaitez.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphique.html#sec-mult-graphs",
    "href": "04-graphique.html#sec-mult-graphs",
    "title": "4  Figures",
    "section": "\n4.4 Graphiques multiples",
    "text": "4.4 Graphiques multiples\n\n4.4.1 Base R\nDans la base R, l’une des méthodes les plus courantes pour tracer plusieurs graphiques consiste à utiliser la fonction graphique principale par() pour diviser le dispositif de traçage en un certain nombre de sections définies à l’aide de la fonction mfrow = pour diviser le dispositif de traçage en plusieurs sections définies. Avec cette méthode, vous devez d’abord spécifier le nombre de lignes et de colonnes de tracés que vous souhaitez, puis exécuter le code pour chaque tracé. Par exemple, pour tracer deux graphiques côte à côte, nous utiliserions par(mfrow = c(1, 2)) pour diviser le dispositif en une ligne et deux colonnes.\n\npar(mfrow = c(1, 2))\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight\",\n  ylab = \"Fluffyness\"\n)\nboxplot(fluffyness ~ food, data = unicorns, cex.axis = 0.6)\n\n\n\n\n\n\n\nUne fois que vous avez terminé vos tracés, n’oubliez pas de réinitialiser votre dispositif de traçage à la normale avec par(mfrow = c(1,1)).\n\n4.4.2 ggplot\nEn plus des fonctions facet_grid() et facet_wrap qui permettent de répéter et d’organiser facilement plusieurs tracés en fonction de variables spécifiques, ggplot permet d’organiser plusieurs ggplots ensemble. L’approche que nous recommandons est d’utiliser le paquet patchwork.\nVous devrez d’abord l’installer (si vous ne l’avez pas encore) et créer le fichier patchwork 📦 disponible.\n\ninstall.packages(\"patchwork\")\nlibrary(patchwork)\n\nUne note importante : pour ceux qui ont utilisé la base R pour produire leurs chiffres et qui sont familiers avec l’utilisation de par(mfrow = c(2,2)) (qui permet de tracer quatre figures sur deux lignes et deux colonnes), sachez que cela ne fonctionne pas avec ggplot2 les objets. Vous devrez utiliser soit la fonction patchwork ou d’autres paquets tels que gridArrange ou cowplot ou dissimuler le ggplot2 objets en grobs.\nPour tracer les deux tracés ensemble, nous devons assigner chaque figure à un objet distinct, puis utiliser ces objets lorsque nous utilisons la fonction patchwork.\nNous pouvons donc générer 2 figures et les assigner à des objets. Comme vous pouvez le constater, les figures n’apparaissent pas dans la fenêtre de tracé. Elles n’apparaîtront que lorsque vous appellerez l’objet.\n\nfirst_figure &lt;- ggplot(\n  aes(x = height, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\nsecond_figure &lt;- ggplot(\n  aes(x = weight, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\n\nDeux options simples et immédiates s’offrent à nous avec le patchwork : disposer les figures les unes sur les autres (spécifiées avec un /) ou arranger les figures côte à côte (spécifié avec soit un + ou a |). Essayons de tracer les deux figures, l’une au-dessus de l’autre.\n\nfirst_figure / second_figure\n\n\n\n\n\n\n\nS’amuser Essayez de créer une version juxtaposée de la figure ci-dessus (indice : essayez les autres opérateurs).\nNous pouvons aller plus loin et assigner des opérateurs imbriqués patchwork à un objet et l’utiliser à son tour pour créer des étiquettes pour les figures individuelles.\n\nnested_compare &lt;- first_figure / second_figure\n\nnested_compare +\n  plot_annotation(tag_levels = \"A\", tag_suffix = \")\")",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphique.html#sec-custom-plot",
    "href": "04-graphique.html#sec-custom-plot",
    "title": "4  Figures",
    "section": "\n4.5 Personnalisation de ggplots",
    "text": "4.5 Personnalisation de ggplots\nPromenade pour être édité :licorne :",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphique.html#sec-export-plots",
    "href": "04-graphique.html#sec-export-plots",
    "title": "4  Figures",
    "section": "\n4.6 Exportation des parcelles",
    "text": "4.6 Exportation des parcelles\nCréer des graphiques dans R, c’est bien, mais que faire si vous souhaitez utiliser ces graphiques dans votre thèse, votre rapport ou votre publication ? Une option consiste à cliquer sur le bouton “Exporter” dans l’onglet “Tracés” de RStudio. Vous pouvez également exporter vos tracés de R vers un fichier externe en écrivant du code dans votre script R. L’avantage de cette approche est que vous avez un peu plus de contrôle sur le format de sortie et qu’elle vous permet également de générer (ou de mettre à jour) des graphiques automatiquement chaque fois que vous exécutez votre script. Vous pouvez exporter vos tracés dans de nombreux formats différents, mais les plus courants sont pdf, png, jpeg et tiff.\nPar défaut, R (et donc RStudio) dirige tous les tracés que vous créez vers la fenêtre de tracé. Pour enregistrer votre tracé dans un fichier externe, vous devez d’abord rediriger votre tracé vers un périphérique graphique différent. Pour ce faire, vous pouvez utiliser l’une des nombreuses fonctions de périphérique graphique pour démarrer un nouveau périphérique graphique. Par exemple, pour enregistrer un tracé au format pdf, nous utiliserons la fonction pdf() pour sauvegarder un tracé au format pdf. Le premier argument de la fonction pdf() est le chemin d’accès et le nom du fichier que nous voulons enregistrer (n’oubliez pas d’inclure l’extension .pdf). Une fois que nous avons utilisé la fonction pdf() nous pouvons alors écrire tout le code que nous avons utilisé pour créer notre graphique, y compris les paramètres graphiques tels que le réglage des marges et la division du dispositif de traçage. Une fois le code exécuté, nous devons fermer le dispositif de traçage pdf à l’aide de la fonction dev.off() à l’aide de la fonction\n\npdf(file = \"output/my_plot.pdf\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nSi nous voulons sauvegarder ce tracé au format png, il nous suffit d’utiliser la fonction png() plus ou moins de la même manière que nous avons utilisé la fonction pdf() de la même manière que nous avons utilisé la fonction\n\npng(\"output/my_plot.png\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nD’autres fonctions utiles sont ; jpeg(), tiff() et bmp(). Des arguments supplémentaires à ces fonctions vous permettent de modifier la taille, la résolution et la couleur d’arrière-plan de vos images enregistrées. Voir aussi ?png pour plus de détails.\nggplot2 📦 fournir une fonction très utile ggsave() qui simplifie grandement la sauvegarde des tracés, mais ne fonctionne que pour les ggplots.\nAprès avoir produit un tracé et l’avoir vu dans votre IDE, vous pouvez simplement exécuter ggsave() avec l’argument adéquat pour sauvegarder le dernier ggplot produit. Vous pouvez aussi, bien sûr, spécifier quel tracé doit être sauvegardé.\n\nggsave(\"file.png\")\n\n\n\n\n\nWilkinson, L. 2005. The Grammar of Graphics. Springer Science & Business Media.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "05-programmation.html",
    "href": "05-programmation.html",
    "title": "5  Programmation",
    "section": "",
    "text": "5.1 Regarder derrière le rideau\nUne bonne façon de commencer à apprendre à programmer en R est de voir ce que d’autres ont fait. Nous pouvons commencer par jeter un bref coup d’œil derrière le rideau. Avec de nombreuses fonctions en R, si vous voulez jeter un coup d’œil rapide à la machinerie en coulisses, nous pouvons simplement écrire le nom de la fonction, mais sans l’attribut ().\nNotez que l’affichage du code source des paquets R de base (ceux qui sont livrés avec R) nécessite quelques étapes supplémentaires que nous ne couvrirons pas ici (voir ce lien si cela vous intéresse), mais pour la plupart des autres paquets que vous installez vous-même, il suffit généralement d’entrer le nom de la fonction sans la mention() affichera le code source de la fonction.\nVous pouvez jeter un coup d’oeil à la fonction d’ajustement d’un modèle linéaire lm()\nlm\n\nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \n{\n    ret.x &lt;- x\n    ret.y &lt;- y\n    cl &lt;- match.call()\n    mf &lt;- match.call(expand.dots = FALSE)\n    m &lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\", \n        \"offset\"), names(mf), 0L)\n    mf &lt;- mf[c(1L, m)]\n    mf$drop.unused.levels &lt;- TRUE\n    mf[[1L]] &lt;- quote(stats::model.frame)\n    mf &lt;- eval(mf, parent.frame())\n    if (method == \"model.frame\") \n        return(mf)\n    else if (method != \"qr\") \n        warning(gettextf(\"method = '%s' is not supported. Using 'qr'\", \n            method), domain = NA)\n    mt &lt;- attr(mf, \"terms\")\n    y &lt;- model.response(mf, \"numeric\")\n    w &lt;- as.vector(model.weights(mf))\n    if (!is.null(w) && !is.numeric(w)) \n        stop(\"'weights' must be a numeric vector\")\n    offset &lt;- model.offset(mf)\n    mlm &lt;- is.matrix(y)\n    ny &lt;- if (mlm) \n        nrow(y)\n    else length(y)\n    if (!is.null(offset)) {\n        if (!mlm) \n            offset &lt;- as.vector(offset)\n        if (NROW(offset) != ny) \n            stop(gettextf(\"number of offsets is %d, should equal %d (number of observations)\", \n                NROW(offset), ny), domain = NA)\n    }\n    if (is.empty.model(mt)) {\n        x &lt;- NULL\n        z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, \n            ncol(y)) else numeric(), residuals = y, fitted.values = 0 * \n            y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != \n            0) else ny)\n        if (!is.null(offset)) {\n            z$fitted.values &lt;- offset\n            z$residuals &lt;- y - offset\n        }\n    }\n    else {\n        x &lt;- model.matrix(mt, mf, contrasts)\n        z &lt;- if (is.null(w)) \n            lm.fit(x, y, offset = offset, singular.ok = singular.ok, \n                ...)\n        else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n            ...)\n    }\n    class(z) &lt;- c(if (mlm) \"mlm\", \"lm\")\n    z$na.action &lt;- attr(mf, \"na.action\")\n    z$offset &lt;- offset\n    z$contrasts &lt;- attr(x, \"contrasts\")\n    z$xlevels &lt;- .getXlevels(mt, mf)\n    z$call &lt;- cl\n    z$terms &lt;- mt\n    if (model) \n        z$model &lt;- mf\n    if (ret.x) \n        z$x &lt;- x\n    if (ret.y) \n        z$y &lt;- y\n    if (!qr) \n        z$qr &lt;- NULL\n    z\n}\n&lt;bytecode: 0x5ce5e486c128&gt;\n&lt;environment: namespace:stats&gt;\nCe que nous voyons ci-dessus est le code sous-jacent de cette fonction particulière. Nous pourrions le copier et le coller dans notre propre script et y apporter toutes les modifications que nous jugerions nécessaires, mais en faisant preuve de prudence et en testant les changements que vous avez apportés.\nNe vous inquiétez pas outre mesure si la majeure partie du code contenu dans les fonctions n’a pas de sens immédiat. C’est particulièrement vrai si vous êtes novice en matière de R, auquel cas cela semble incroyablement intimidant. Honnêtement, cela peut être intimidant même après des années d’expérience avec R. Pour y remédier, nous commencerons par créer nos propres fonctions en R dans la section suivante.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programmation</span>"
    ]
  },
  {
    "objectID": "05-programmation.html#fonctions-en-r",
    "href": "05-programmation.html#fonctions-en-r",
    "title": "5  Programmation",
    "section": "\n5.2 Fonctions en R",
    "text": "5.2 Fonctions en R\nLes fonctions sont le pain et le beurre de R, les éléments essentiels qui vous permettent de travailler avec R. Elles sont créées (la plupart du temps) avec le plus grand soin et la plus grande attention, mais peuvent finir par ressembler à un monstre de Frankenstein - avec des membres bizarrement attachés. Mais aussi alambiqués qu’ils puissent être, ils feront toujours fidèlement la même chose.\nCela signifie que les fonctions peuvent également être très stupides.\nSi nous vous demandons d’aller au supermarché pour nous procurer des ingrédients pour faire des du poulet Balmoral même si vous ne savez pas ce que c’est, vous serez capable de deviner et d’apporter au moins quelque chose quelque chose. Vous pouvez aussi décider de faire autre chose. Ou vous pouvez demander de l’aide à un chef cuisinier. Ou vous pouvez sortir votre téléphone et chercher en ligne ce que vous voulez faire. Poulet Balmoral est. Le fait est que, même si nous ne vous avons pas donné suffisamment d’informations pour accomplir la tâche, vous êtes suffisamment intelligent pour, au moins, essayer de trouver une solution de contournement.\nSi, au contraire, nous demandions à une fonction de faire la même chose, elle écouterait attentivement notre demande, puis renverrait simplement une erreur. Elle répéterait cela à chaque fois que nous lui demanderions de faire le travail lorsque la tâche n’est pas claire. Ce qu’il faut retenir ici, c’est que le code et les fonctions ne peuvent pas trouver de solutions de contournement à des informations mal fournies, ce qui est une excellente chose. Il dépend entièrement de vous pour lui dire très explicitement ce qu’il doit faire, étape par étape.\nN’oubliez pas deux choses : l’intelligence du code vient du codeur, pas de l’ordinateur, et les fonctions ont besoin d’instructions exactes pour fonctionner.\nPour éviter que les fonctions ne soient trop stupides, vous devez fournir les informations dont la fonction a besoin pour fonctionner. Comme pour la fonction poulet du Balmoral si nous avions fourni une liste de recettes à la fonction, tout se serait bien passé. C’est ce que nous appelons “remplir un argument”. La grande majorité des fonctions exigent de l’utilisateur qu’il remplisse au moins un argument.\nCeci peut être illustré dans le pseudocode ci-dessous. Lorsque nous créons une fonction, nous pouvons :\n\nspécifier les arguments que l’utilisateur doit remplir (par exemple arg1 et arg2)\nfournissent des valeurs par défaut aux arguments (par exemple arg2 = TRUE)\ndéfinir ce qu’il faut faire avec les arguments (expression) :\n\n\nmy_function &lt;- function(arg1, arg2, ...) {\n  expression\n}\n\nLa première chose à noter est que nous avons utilisé la fonction function() pour créer une nouvelle fonction appelée my_function. Pour parcourir le code ci-dessus, nous créons une fonction appelée my_function. Entre les crochets ronds, nous spécifions les informations (c’est-à-dire arguments) dont la fonction a besoin pour fonctionner (autant ou aussi peu que nécessaire). Ces arguments sont ensuite transmis à la partie expression de la fonction. L’expression peut être n’importe quelle commande R valide ou n’importe quel ensemble de commandes R et est généralement entre une paire d’accolades { }. Une fois que vous avez exécuté le code ci-dessus, vous pouvez utiliser votre nouvelle fonction en tapant :\n\nmy_function(arg1, arg2)\n\nPrenons un exemple pour clarifier les choses.\nTout d’abord, nous allons créer un cadre de données appelé dishes où les colonnes lasagna, stovies, poutine et tartiflette sont remplis avec 10 valeurs aléatoires tirées d’un sac (à l’aide de la fonction rnorm() pour tirer des valeurs aléatoires d’une distribution normale avec une moyenne de 0 et un écart type de 1). Nous incluons également un “problème”, que nous devrons résoudre plus tard, en incluant 3 NA dans la fonction poutine (en utilisant rep(NA, 3)).\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\nSupposons que vous souhaitiez multiplier les valeurs des variables stovies et lasagna et créer un nouvel objet appelé stovies_lasagna. Nous pouvons le faire “à la main” en utilisant :\n\nstovies_lasagna &lt;- dishes$stovies * dishes$lasagna\n\nSi c’était tout ce que nous avions à faire, nous pourrions nous arrêter ici. R fonctionne avec des vecteurs, de sorte qu’effectuer ce type d’opérations dans R est en fait beaucoup plus simple que dans d’autres langages de programmation, où ce type de code peut nécessiter des boucles (nous disons que R est un langage vectorisé). Une chose à garder à l’esprit pour plus tard est que faire ce genre d’opérations avec des boucles peut être beaucoup plus lent que la vectorisation.\nMais que se passe-t-il si nous voulons répéter cette multiplication plusieurs fois ? Supposons que nous voulions multiplier les colonnes lasagna et stovies, stovies et tartiflette et poutine et tartiflette. Dans ce cas, nous pouvons copier et coller le code en remplaçant les informations pertinentes.\n\nlasagna_stovies &lt;- dishes$lasagna * dishes$stovies\nstovies_tartiflette &lt;- dishes$stovies * dishes$stovies\npoutine_tartiflette &lt;- dishes$poutine * dishes$tartiflette\n\nBien que cette approche fonctionne, il est facile de faire des erreurs. En fait, ici, nous avons “oublié” de modifier stovies en tartiflette dans la deuxième ligne de code lors du copier-coller. C’est là que l’écriture d’une fonction s’avère utile. Si nous écrivions cela sous forme de fonction, il n’y aurait qu’une seule source d’erreur potentielle (dans la fonction elle-même) au lieu de nombreuses lignes de code copiées-collées (que nous réduisons également en utilisant une fonction).\n\n\n\n\n\n\nAstuce\n\n\n\nEn règle générale, si nous devons faire la même chose (par copier/coller et modifier) 3 fois ou plus, nous créons une fonction pour cela.\n\n\nDans ce cas, nous utilisons un code assez trivial où il est peut-être difficile de faire une véritable erreur. Mais que se passerait-il si nous augmentions la complexité ?\n\ndishes$lasagna * dishes$stovies / dishes$lasagna + (dishes$lasagna * 10^(dishes$stovies))\n-dishes$stovies - (dishes$lasagna * sqrt(dishes$stovies + 10))\n\nImaginez maintenant que vous deviez copier et coller ce code trois fois, et que vous deviez à chaque fois modifier l’élément lasagna et stovies (surtout si nous devions le faire plus de trois fois).\nCe que nous pourrions faire à la place, c’est généraliser notre code pour x et y au lieu de nommer des plats spécifiques. En procédant de la sorte, nous pourrions recycler la x * y code. Chaque fois que nous voulions regrouper plusieurs colonnes, nous assignions une parabole à l’un ou l’autre des éléments suivants x ou y. Nous attribuerons la multiplication aux objets lasagna_stovies et stovies_poutine afin de pouvoir y revenir plus tard.\n\n# Assign x and y values\nx &lt;- dishes$lasagna\ny &lt;- dishes$stovies\n\n# Use multiplication code\nlasagna_stovies &lt;- x * y\n\n# Assign new x and y values\nx &lt;- dishes$stovies\ny &lt;- dishes$poutine\n\n# Reuse multiplication code\nstovies_poutine &lt;- x * y\n\nC’est essentiellement ce que fait une fonction. Appelons notre nouvelle fonction multiply_cols() et définissons-la avec deux arguments, x et y. Une fonction dans R renvoie simplement sa dernière valeur. Toutefois, il est possible de forcer la fonction à renvoyer une valeur antérieure si cela s’avère nécessaire. Pour ce faire, il suffit d’utiliser la fonction return() n’est pas strictement nécessaire dans cet exemple car R retournera automatiquement la valeur de la dernière ligne de code de notre fonction. Nous l’incluons ici pour l’expliciter.\n\nmultiply_cols &lt;- function(x, y) {\n  return(x * y)\n}\n\nMaintenant que nous avons défini notre fonction, nous pouvons l’utiliser. Utilisons la fonction pour multiplier les colonnes lasagna et stovies et assigner le résultat à un nouvel objet appelé lasagna_stovies_func\n\nlasagna_stovies_func &lt;- multiply_cols(x = dishes$lasagna, y = dishes$stovies)\nlasagna_stovies_func\n\n [1]  0.39870077 -0.75520781  0.05353023  0.23102663  0.45329894 -2.78623418\n [7] -0.35813571  0.10336648 -0.10248847  0.14124923\n\n\nSi nous ne nous intéressons qu’à la multiplication dishes$lasagna et dishes$stovies il serait exagéré de créer une fonction pour faire quelque chose une seule fois. Cependant, l’avantage de créer une fonction est que nous avons maintenant cette fonction ajoutée à notre environnement que nous pouvons utiliser aussi souvent que nous le souhaitons. Nous disposons également du code pour créer la fonction, ce qui signifie que nous pouvons l’utiliser dans des projets entièrement nouveaux, réduisant ainsi la quantité de code qui doit être écrite (et testée à nouveau) à partir de zéro à chaque fois.\nPour s’assurer que la fonction a fonctionné correctement, nous pouvons comparer le code lasagna_stovies avec notre nouvelle variable lasagna_stovies_func à l’aide de la fonction identical() fonction. La fonction identical() teste si deux objets sont exactement identiques et renvoie soit un TRUE ou FALSE valeur. Utiliser ?identical pour en savoir plus sur cette fonction.\n\nidentical(lasagna_stovies, lasagna_stovies_func)\n\n[1] TRUE\n\n\nEt nous confirmons que la fonction a produit le même résultat que lorsque nous effectuons le calcul manuellement. Nous vous recommandons de prendre l’habitude de vérifier que la fonction que vous avez créée fonctionne comme vous le pensez.\nUtilisons maintenant notre multiply_cols() pour multiplier les colonnes stovies et poutine. Remarquez maintenant que l’argument x reçoit la valeur dishes$stovieset y la valeur dishes$poutine.\n\nstovies_poutine_func &lt;- multiply_cols(x = dishes$stovies, y = dishes$poutine)\nstovies_poutine_func\n\n [1]            NA            NA            NA  0.0879287405  0.4392946420\n [6]  0.0605509100  0.9761694387  1.2215362724 -0.2825366740 -0.0005892957\n\n\nJusqu’à présent, tout va bien. Tout ce que nous avons fait, c’est envelopper le code x * y dans une fonction, où nous demandons à l’utilisateur de spécifier ce que son x et y sont.\nL’utilisation de la fonction est un peu longue car nous devons retaper le nom de la base de données pour chaque variable. Pour nous amuser un peu, nous pouvons modifier la fonction afin de spécifier le cadre de données en tant qu’argument et les noms des colonnes sans les mettre entre guillemets (comme dans le style tidyverse).\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = prod(.)) %&gt;%\n    pull(xy)\n}\n\nPour cette nouvelle version de la fonction, nous avons ajouté un paramètre data à la ligne 1. A la ligne 3, nous sélectionnons les variables x et y fournies comme arguments. A la ligne 4, nous créons le produit des 2 colonnes sélectionnées et à la ligne 5, nous extrayons la colonne que nous venons de créer. Nous supprimons également la return() puisqu’elle n’était pas nécessaire\nNotre fonction est maintenant compatible avec le pipe (soit en natif |&gt; ou magrittr %&gt;%). Toutefois, étant donné que la fonction utilise désormais le tuyau de magrittr 📦 et dplyr 📦, nous devons charger le paquet 📦 de tidyverse pour qu’elle fonctionne.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\nlasagna_stovies_func &lt;- dishes |&gt; multiply_cols(lasagna, stovies)\n\nAjoutons maintenant un peu plus de complexité. Si vous regardez la sortie de poutine_tartiflette certains des calculs ont produit NA valeurs. Cela s’explique par le fait que les NA que nous avons incluses dans poutine lorsque nous avons créé l’élément dishes cadre de données. Malgré ces NA valeurs, la fonction semble avoir fonctionné, mais elle ne nous a donné aucune indication quant à l’existence d’un problème. Dans ce cas, nous préférerions qu’elle nous avertisse que quelque chose ne va pas. Comment pouvons-nous faire en sorte que la fonction nous informe lorsque NA sont produites ? Voici une solution.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\n\nLe cœur de notre fonction reste le même, mais nous avons maintenant six lignes de code supplémentaires (lignes 6 à 11). Nous avons inclus des instructions conditionnelles, if (lignes 6-8) et else (lignes 9-11), afin de tester si un NAont été produites et, si c’est le cas, nous affichons un message d’avertissement à l’intention de l’utilisateur. La section suivante de ce chapitre explique le fonctionnement et l’utilisation de ces fonctions.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programmation</span>"
    ]
  },
  {
    "objectID": "05-programmation.html#déclarations-conditionnelles",
    "href": "05-programmation.html#déclarations-conditionnelles",
    "title": "5  Programmation",
    "section": "\n5.3 Déclarations conditionnelles",
    "text": "5.3 Déclarations conditionnelles\nx * y n’applique aucune logique. Il prend simplement la valeur de x et la multiplie par la valeur de y. Les instructions conditionnelles permettent d’injecter de la logique dans votre code. L’instruction conditionnelle la plus couramment utilisée est if. Chaque fois que vous voyez un if lisez-le comme * Si X est VRAI, faites une chose”.. Incluant un else permet simplement d’étendre la logique à  Si X est VRAI, faites une chose, ou bien faites quelque chose de différent.*.\nLes deux if et else vous permettent d’exécuter des sections de code, en fonction d’une condition qui est soit TRUE ou FALSE. Le pseudo-code ci-dessous vous montre la forme générale.\n  if (condition) {\n  Code executed when condition is TRUE\n  } else {\n  Code executed when condition is FALSE\n  }\nPour approfondir la question, nous pouvons utiliser une vieille blague de programmeur pour poser un problème.\n\nLe partenaire d’un programmeur dit : * Allez au magasin acheter une brique de lait et, s’il y a des œufs, prenez-en six.*\nLe programmeur est revenu avec 6 briques de lait.\nLorsque le partenaire s’en aperçoit, il s’exclame * Pourquoi diable as-tu acheté six briques de lait ?*\nLe programmeur a répondu “Ils avaient des œufs\n\nAu risque d’expliquer une blague, l’énoncé conditionnel ici est de savoir si le magasin avait ou non des œufs. Si le codage est conforme à la demande initiale, le programmeur doit apporter 6 briques de lait si le magasin a des œufs (condition = VRAI), ou apporter 1 brique de lait s’il n’y a pas d’œufs (condition = FAUX). Dans R, cela est codé comme suit :\n\neggs &lt;- TRUE # Whether there were eggs in the store\n\nif (eggs == TRUE) { # If there are eggs\n  n.milk &lt;- 6 # Get 6 cartons of milk\n} else { # If there are not eggs\n  n.milk &lt;- 1 # Get 1 carton of milk\n}\n\nNous pouvons alors vérifier n.milk le nombre de briques de lait qu’ils ont ramenées.\n\nn.milk\n\n[1] 6\n\n\nEt comme dans la blague, notre code R n’a pas compris que la condition était de déterminer s’il fallait ou non acheter des œufs, et non plus du lait (il s’agit en fait d’un exemple libre du schéma de Winograd conçu pour tester la conditionl’intelligence d’une intelligence artificielle en fonction de sa capacité à raisonner sur le sens d’une phrase).\nNous pourrions coder exactement la même instruction conditionnelle de blague œuf-lait à l’aide d’un ifelse() fonction.\n\neggs &lt;- TRUE\nn.milk &lt;- ifelse(eggs == TRUE, yes = 6, no = 1)\n\nCette ifelse() fait exactement la même chose que la version plus étoffée de tout à l’heure, mais elle est maintenant condensée en une seule ligne de code. Elle présente l’avantage supplémentaire de travailler sur des vecteurs plutôt que sur des valeurs individuelles (nous y reviendrons plus tard lorsque nous introduirons les boucles). La logique est lue de la même manière : “S’il y a des oeufs, assignez une valeur de 6 à n.milk s’il n’y a pas d’oeufs, assigner la valeur 1 à n.milk”.\nNous pouvons vérifier à nouveau que la logique renvoie toujours 6 briques de lait :\n\nn.milk\n\n[1] 6\n\n\nActuellement, nous devrions copier et coller du code si nous voulions changer la présence ou l’absence d’œufs dans le magasin. Nous avons appris plus haut comment éviter de nombreux copier-coller en créant une fonction. Comme avec la simple fonction x * y de notre précédente expression multiply_cols() les déclarations logiques ci-dessus sont simples à coder et se prêtent bien à la transformation en fonction. Et si nous faisions justement cela et enveloppions cette déclaration logique dans une fonction ?\n\nmilk &lt;- function(eggs) {\n  if (eggs == TRUE) {\n    6\n  } else {\n    1\n  }\n}\n\nNous avons créé une fonction appelée milk() dont le seul argument est eggs. L’utilisateur de la fonction spécifie si les œufs sont soit TRUE ou FALSE et la fonction utilisera alors une instruction conditionnelle pour déterminer le nombre de cartons de lait renvoyés.\nEssayons rapidement :\n\nmilk(eggs = TRUE)\n\n[1] 6\n\n\nEt la plaisanterie est maintenue. Remarquez que, dans ce cas, nous avons spécifié que nous remplissons la fonction eggs (eggs = TRUE). Dans certaines fonctions, comme la nôtre ici, lorsqu’une fonction n’a qu’un seul argument, nous pouvons être paresseux et ne pas nommer l’argument que nous remplissons. En réalité, on considère généralement qu’il est préférable d’indiquer explicitement les arguments que l’on remplit afin d’éviter les erreurs potentielles.\nOK, revenons à la fonction multiply_cols() que nous avons créée ci-dessus et expliquons comment nous avons utilisé des instructions conditionnelles pour avertir l’utilisateur si NA sont produites lorsque nous multiplions deux colonnes.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nDans cette nouvelle version de la fonction, nous utilisons toujours x * y comme auparavant, mais cette fois nous avons assigné les valeurs de ce calcul à un vecteur temporaire appelé temp_var afin de pouvoir l’utiliser dans nos instructions conditionnelles. Notez que ce temp_var est locale à notre fonction et n’existera pas en dehors de la fonction en raison de ce que l’on appelle les règles de cadrage de R . [règles de cadrage de R][cadrage] . Nous utilisons ensuite un if pour déterminer si notre temp_var contient des NA valeurs. Pour ce faire, nous utilisons d’abord la fonction is.na() pour vérifier si chaque valeur de notre temp_var est un NA. Les is.na() renvoie TRUE si la valeur est un NA et FALSE si la valeur n’est pas un NA. Nous imbriquons ensuite le is.na(temp_var) à l’intérieur de la fonction any() pour vérifier si une des valeurs retournées par is.na(temp_var) sont TRUE. Si au moins une valeur est TRUE l’une any() renverra une valeur TRUE. Ainsi, s’il existe des NA valeurs dans notre temp_var la condition pour le if() sera TRUE alors que s’il n’y a pas de NA valeurs présentes, la condition sera FALSE. Si la condition est TRUE la warning() génère un message d’avertissement à l’intention de l’utilisateur et renvoie la valeur de la fonction temp_var variable. Si la condition est FALSE le code sous le else est exécuté, ce qui renvoie simplement la valeur temp_var .\nAinsi, si nous exécutons notre multiple_columns() sur les colonnes dishes$stovies et dishes$poutine (qui contient NAs), nous recevrons un message d’avertissement.\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\n\nEn revanche, si nous multiplions deux colonnes qui ne contiennent pas de NA nous ne recevons pas de message d’avertissement\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programmation</span>"
    ]
  },
  {
    "objectID": "05-programmation.html#combinaison-dopérateurs-logiques",
    "href": "05-programmation.html#combinaison-dopérateurs-logiques",
    "title": "5  Programmation",
    "section": "\n5.4 Combinaison d’opérateurs logiques",
    "text": "5.4 Combinaison d’opérateurs logiques\nLes fonctions que nous avons créées jusqu’à présent étaient parfaitement adaptées à nos besoins, même si elles étaient assez simplistes. Essayons de créer une fonction un peu plus complexe. Nous allons créer une fonction permettant de déterminer si la journée d’aujourd’hui sera bonne ou non en fonction de deux critères. Le premier critère dépendra du jour de la semaine (vendredi ou non) et le second sera de savoir si votre code fonctionne ou non (VRAI ou FAUX). Pour ce faire, nous utiliserons if et else et des déclarations. La complexité viendra de if des déclarations qui suivent immédiatement les else pertinente. Nous utiliserons ces instructions conditionnelles quatre fois pour obtenir toutes les combinaisons possibles, qu’il s’agisse d’un vendredi ou non, et pour savoir si votre code fonctionne ou non.\nNous avons également utilisé l’instruction cat() pour produire un texte formaté correctement.\n\ngood.day &lt;- function(code.working, day) {\n  if (code.working == TRUE && day == \"Friday\") {\n    cat(\n  \"BEST.\n  DAY.\n    EVER.\n      Stop while you are ahead and go to the pub!\"\n    )\n  } else if (code.working == FALSE && day == \"Friday\") {\n    cat(\"Oh well, but at least it's Friday! Pub time!\")\n  } else if (code.working == TRUE && day != \"Friday\") {\n    cat(\"\n  So close to a good day...\n  shame it's not a Friday\"\n    )\n  } else if (code.working == FALSE && day != \"Friday\") {\n    cat(\"Hello darkness.\")\n  }\n}\n\n\ngood.day(code.working = TRUE, day = \"Friday\")\n\nBEST.\n  DAY.\n    EVER.\n      Stop while you are ahead and go to the pub!\n\ngood.day(FALSE, \"Tuesday\")\n\nHello darkness.\n\n\nVous avez remarqué que nous n’avons jamais spécifié ce qu’il fallait faire si le jour n’était pas un vendredi ? C’est parce que, pour cette fonction, la seule chose qui compte est de savoir si c’est un vendredi ou non.\nNous avons également utilisé des opérateurs logiques chaque fois que nous avons utilisé la fonction if à chaque fois que nous avons utilisé des instructions. Les opérateurs logiques sont la dernière pièce du puzzle des conditions logiques. Le tableau ci-dessous résume les opérateurs. Les deux premiers sont des opérateurs logiques et les six derniers sont des opérateurs relationnels. Vous pouvez utiliser n’importe lequel de ces opérateurs lorsque vous créez vos propres fonctions (ou boucles).\n\n\n\n\n\n\n\n\nOpérateur\nDescription technique\nCe que cela signifie\nExemple d’application\n\n\n\n&&\nET logique\nLes deux conditions doivent être remplies\nif(cond1 == test && cond2 == test)\n\n\n`\n\n`\nOU logique\n\n\n&lt;\nInférieur à\nX est inférieur à Y\nif(X &lt; Y)\n\n\n&gt;\nSupérieur à\nX est supérieur à Y\nif(X &gt; Y)\n\n\n&lt;=\nInférieur ou égal à\nX est inférieur/égal à Y\nif(X &lt;= Y)\n\n\n&gt;=\nSupérieur ou égal à\nX est supérieur/égal à Y\nif(X &gt;= Y)\n\n\n==\nEgal à\nX est égal à Y\nif(X == Y)\n\n\n!=\nN’est pas égal à\nX n’est pas égal à Y\nif(X != Y)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programmation</span>"
    ]
  },
  {
    "objectID": "05-programmation.html#boucles",
    "href": "05-programmation.html#boucles",
    "title": "5  Programmation",
    "section": "\n5.5 Boucles",
    "text": "5.5 Boucles\nR est très performant dans l’exécution de tâches répétitives. Si nous voulons qu’un ensemble d’opérations soit répété plusieurs fois, nous utilisons ce que l’on appelle une boucle. Lorsque vous créez une boucle, R exécute les instructions qu’elle contient un certain nombre de fois ou jusqu’à ce qu’une condition donnée soit remplie. Il existe trois principaux types de boucles dans R : la boucle for la boucle while et la boucle répéter boucle.\nLes boucles sont l’un des éléments de base de tous les langages de programmation, et pas seulement de R, et peuvent être un outil puissant (bien qu’à notre avis, elles soient utilisées beaucoup trop souvent lors de l’écriture de code R).\n\n5.5.1 Boucle For\nLa structure de boucle la plus couramment utilisée lorsque vous souhaitez répéter une tâche un nombre défini de fois est la boucle for boucle. L’exemple le plus simple de boucle for est le suivant :\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nMais que fait réellement le code ? Il s’agit d’un morceau de code dynamique où un index i est remplacé itérativement par chaque valeur du vecteur 1:5. Décomposons. Parce que la première valeur de notre séquence (1:5) est 1 la boucle commence par remplacer i par 1 et exécute tout ce qui se trouve entre le { }. Les boucles utilisent conventionnellement i comme compteur, abréviation d’itération, mais vous êtes libre d’utiliser ce que vous voulez, même le nom de votre animal de compagnie, cela n’a pas vraiment d’importance (sauf lorsque vous utilisez des boucles imbriquées, auquel cas les compteurs doivent être appelés différemment, comme SenorWhiskers et HerrFlufferkins).\nAinsi, si nous devions effectuer manuellement la première itération de la boucle\n\ni &lt;- 1\nprint(i)\n\n[1] 1\n\n\nUne fois cette première itération terminée, la boucle for boucle revient au début et remplace i par la valeur suivante dans notre 1:5 séquence (2 dans ce cas) :\n\ni &lt;- 2\nprint(i)\n\n[1] 2\n\n\nCe processus est ensuite répété jusqu’à ce que la boucle atteigne la dernière valeur de la séquence (5 dans cet exemple), après quoi elle s’arrête.\nPour renforcer la façon dont les for et vous présenter une caractéristique importante des boucles, nous allons modifier notre compteur à l’intérieur de la boucle. Cela peut être utilisé, par exemple, si nous utilisons une boucle pour parcourir un vecteur mais que nous voulons sélectionner la ligne suivante (ou toute autre valeur). Pour ce faire, nous ajouterons simplement 1 à la valeur de notre index à chaque fois que nous itérons notre boucle.\n\nfor (i in 1:5) {\n  print(i + 1)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n\n\nComme dans la boucle précédente, la première valeur de notre séquence est 1. La boucle commence par remplacer i par 1 mais cette fois, nous avons spécifié qu’une valeur de 1 doit être ajoutée à i dans l’expression résultant en une valeur de 1 + 1.\n\ni &lt;- 1\ni + 1\n\n[1] 2\n\n\nComme précédemment, une fois l’itération terminée, la boucle passe à la valeur suivante de la séquence et remplace i par la valeur suivante (2 dans ce cas), de sorte que i + 1 devient 2 + 1.\n\ni &lt;- 2\ni + 1\n\n[1] 3\n\n\nEt ainsi de suite. Nous pensons que vous comprenez l’idée ! En fait, il s’agit d’une for et rien d’autre.\nBien que nous ayons utilisé une simple addition dans le corps de la boucle, vous pouvez également combiner des boucles avec des fonctions.\nRevenons à notre cadre de données dishes. Précédemment dans le chapitre, nous avons créé une fonction pour multiplier deux colonnes et l’avons utilisée pour créer notre lasagna_stovies, stovies_poutine, et poutine_tartiflette objets. Nous aurions pu utiliser une boucle pour cela. Rappelons-nous à quoi ressemblent nos données et le code de la fonction multiple_columns() fonction.\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nPour utiliser une liste afin d’itérer sur ces colonnes, nous devons d’abord créer une liste vide (vous vous souvenez de Section 3.2.3 ?) que nous appelons temp (abréviation de temporary) qui sera utilisée pour stocker les résultats de la fonction for boucle.\n\ntemp &lt;- list()\nfor (i in 1:(ncol(dishes) - 1)) {\n  temp[[i]] &lt;- multiply_cols(dishes, x = colnames(dishes)[i], y = colnames(dishes)[i + 1])\n}\n\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\n\n\nLorsque nous spécifions notre for remarquez que nous avons soustrait 1 de ncol(dishes). La boucle ncol() renvoie le nombre de colonnes dans notre dishes cadre de données qui est 4 et donc notre boucle s’exécute de i = 1 à i = 4 - 1 qui est i = 3.\nAinsi, lors de la première itération de la boucle i prend la valeur 1. Les multiply_cols() multiplie le dishes[, 1] (lasagna) et dishes[, 1 + 1] (stovies) et le stocke dans la colonne temp[[1]] qui est le premier élément de la colonne temp liste.\nLa deuxième itération de la boucle i prend la valeur 2. Les multiply_cols() multiplie le dishes[, 2] (stovies) et dishes[, 2 + 1] (poutine) et le stocke dans la colonne temp[[2]] qui est le deuxième élément de la colonne temp liste.\nLa troisième et dernière itération de la boucle i prend la valeur 3. Les multiply_cols() multiplie le dishes[, 3] (poutine) et dishes[, 3 + 1] (tartiflette) et le stocke dans la colonne temp[[3]] qui est le troisième élément de la colonne temp liste.\nEncore une fois, il est bon de vérifier que nous obtenons quelque chose de sensé de notre boucle (rappelez-vous, vérifiez, vérifiez et vérifiez encore !). Pour ce faire, nous pouvons utiliser la fonction identical() pour comparer les variables que nous avons créées by hand à chaque itération de la boucle manuellement.\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\ni &lt;- 1\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  lasagna_stovies_func\n)\n\n[1] TRUE\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\ni &lt;- 2\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  stovies_poutine_func\n)\n\nWarning in multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + :\nThe function has produced NAs\n\n\n[1] TRUE\n\n\nSi vous pouvez suivre les exemples ci-dessus, vous serez en bonne position pour commencer à écrire vos propres boucles for. Cela dit, il existe d’autres types de boucles.\n\n5.5.2 Boucle While\nUn autre type de boucle que vous pouvez utiliser (bien que moins fréquemment) est la boucle while boucle. La boucle while est utilisée lorsque vous voulez continuer à tourner en boucle jusqu’à ce qu’une condition logique spécifique soit remplie (à comparer avec la boucle for qui parcourt toujours une séquence entière).\nLa structure de base de la boucle while est la suivante\n\nwhile (logical_condition) {\n  expression\n}\n\nUn exemple simple de boucle while est le suivant :\n\ni &lt;- 0\nwhile (i &lt;= 4) {\n  i &lt;- i + 1\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nIci, la boucle continuera uniquement à transmettre des valeurs au corps principal de la boucle (l’élément expression corps) que lorsque i est inférieur ou égal à 4 (spécifié à l’aide de l’attribut &lt;= dans cet exemple). Une fois que i est supérieur à 4, la boucle s’arrête.\nIl existe un autre type de boucle, très rarement utilisé : la boucle repeat boucle. La boucle repeat n’a pas de contrôle conditionnel et peut donc continuer à itérer indéfiniment (ce qui signifie qu’une pause, ou “stop here”, doit être codée dans la boucle). Cela vaut la peine d’être conscient de son existence, mais pour l’instant nous ne pensons pas qu’il faille s’en préoccuper ; la fonction for et while vous permettront de répondre à la plupart de vos besoins en matière de boucles.\n\n5.5.3 Quand utiliser une boucle ?\nLes boucles sont assez couramment utilisées, bien que parfois un peu trop à notre avis. Des tâches équivalentes peuvent être effectuées avec des fonctions, qui sont souvent plus efficaces que les boucles. La question se pose donc de savoir quand il faut utiliser une boucle.\nEn général, les boucles sont implémentées de manière inefficace dans R et doivent être évitées lorsque de meilleures alternatives existent, en particulier lorsque vous travaillez avec de grands ensembles de données. Cependant, les boucles sont parfois le seul moyen d’obtenir le résultat souhaité.\nVoici quelques exemples de cas où l’utilisation de boucles peut s’avérer appropriée :\n\nCertaines simulations (par exemple le modèle de Ricker peut, en partie, être construit à l’aide de boucles)\nRelations récursives (une relation qui dépend de la valeur de la relation précédente) [“pour comprendre la récursivité, il faut comprendre la récursivité”.] )\nProblèmes plus complexes (par exemple, depuis combien de temps le dernier blaireau a-t-il été vu sur le site ? \\(j\\) étant donné qu’une martre des pins a été vue à l’heure \\(t\\) au même endroit \\(j\\) que le blaireau, lorsque la martre a été détectée au cours d’une période spécifique de 6 heures, mais excluant les blaireaux vus 30 minutes avant l’arrivée de la martre, répétée pour toutes les détections de martres)\nBoucles While (continuez à sauter jusqu’à ce que vous ayez atteint la lune)\n\n5.5.4 Si ce ne sont pas des boucles, alors quoi ?\nEn bref, utilisez la famille de fonctions apply ; apply(), lapply(), tapply(), sapply(), vapply() et mapply(). Les fonctions apply peuvent souvent accomplir les tâches de la plupart des boucles “maison”, parfois plus rapidement (bien que cela ne soit pas vraiment un problème pour la plupart des gens), mais surtout avec un risque d’erreur beaucoup plus faible. Une stratégie à garder à l’esprit et qui peut s’avérer utile est la suivante : pour chaque boucle que vous faites, essayez de la refaire en utilisant une fonction apply (souvent lapply ou sapply fonctionneront). Si vous le pouvez, utilisez la version applicable. Il n’y a rien de pire que de se rendre compte qu’il y avait une petite, minuscule, erreur apparemment insignifiante dans une boucle qui, des semaines, des mois ou des années plus tard, s’est propagée dans un énorme gâchis. Nous recommandons vivement d’essayer d’utiliser les fonctions apply chaque fois que cela est possible.\nlapply\nVotre fonction d’application sera souvent lapply() du moins au début. La façon dont les lapply() et la raison pour laquelle il constitue souvent une bonne alternative aux boucles for, est qu’il passe en revue chaque élément d’une liste et effectue une tâche (c’est-à-dire exécuter une fonction). Il présente l’avantage supplémentaire de produire les résultats sous forme de liste, ce que vous devriez autrement coder vous-même dans une boucle.\nUne fonction lapply() a la structure suivante :\nlapply(X, FUN)\nIci X est le vecteur que nous voulons faire quelque chose quelque chose. FUN Les stands de l’association sont très amusants (je plaisante !). C’est aussi l’abréviation de “fonction”.\nCommençons par une démonstration simple. Utilisons la fonction lapply() pour créer une séquence de 1 à 5 et ajouter 1 à chaque observation (comme nous l’avons fait lorsque nous avons utilisé une boucle for) :\n\nlapply(0:4, function(a) {\n  a + 1\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nRemarquez que nous devons spécifier notre séquence en tant que 0:4 pour obtenir la sortie 1 ,2 ,3 ,4 , 5 puisque nous ajoutons 1 à chaque élément de la séquence. Voyez ce qui se passe si vous utilisez 1:5 à la place.\nDe manière équivalente, nous aurions pu définir la fonction d’abord, puis l’utiliser dans lapply()\n\nadd_fun &lt;- function(a) {\n  a + 1\n}\nlapply(0:4, add_fun)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nLes sapply() fait la même chose que lapply() mais au lieu de stocker les résultats sous forme de liste, elle les stocke sous forme de vecteur.\n\nsapply(0:4, function(a) {\n  a + 1\n})\n\n[1] 1 2 3 4 5\n\n\nComme vous pouvez le voir, dans les deux cas, nous obtenons exactement les mêmes résultats que lorsque nous avons utilisé la boucle for.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programmation</span>"
    ]
  },
  {
    "objectID": "06-quarto.html",
    "href": "06-quarto.html",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "",
    "text": "6.1 Qu’est-ce que R markdown / Quarto ?",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#quest-ce-que-r-markdown-quarto",
    "href": "06-quarto.html#quest-ce-que-r-markdown-quarto",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "",
    "text": "6.1.1 R Markdown\nR markdown est un langage de texte simple et facile à utiliser pour combiner votre code R, les résultats de votre analyse de données (y compris les graphiques et les tableaux) et vos commentaires écrits dans un document unique, bien formaté et reproductible (comme un rapport, une publication, un chapitre de thèse ou une page web comme celle-ci).\nTechniquement, R markdown est une combinaison de trois langages, R, Markdown et YAML (un autre langage de balisage). Markdown et YAML sont tous deux un type de langage de balisage. Un langage de balisage permet simplement de créer un fichier de texte brut facile à lire, qui peut contenir du texte formaté, des images, des en-têtes et des liens vers d’autres documents. Si vous êtes intéressé, vous pouvez trouver plus d’informations sur les langages de balisage [ici][balisage] . En fait, vous êtes exposé à un langage de balisage tous les jours, car la plupart des contenus Internet que vous digérez quotidiennement sont étayés par un langage de balisage appelé HTML (Hypertext Markup Language). Quoi qu’il en soit, le point principal est que R markdown est très facile à apprendre (beaucoup, beaucoup plus facile que HTML) et lorsqu’il est utilisé avec un bon IDE (RStudio ou VS Code), il est ridiculement facile à intégrer dans votre flux de travail pour produire un contenu riche en fonctionnalités (alors pourquoi ne le feriez-vous pas ?!).\n\n6.1.2 Quarto ?\nQuarto est une version multilingue de la nouvelle génération de R Markdown de Posit, avec de nombreuses nouvelles fonctionnalités et capacités, compatible non seulement avec R mais aussi avec d’autres langages comme Python et Julia. Comme R Markdown, Quarto utilise knitr 📦 package pour exécuter le code R, et est donc capable de rendre la plupart des codes R existants. .Rmd existants sans modification. Cependant, il s’accompagne également d’une pléthore de nouvelles fonctionnalités. Plus important encore, il facilite grandement la création de différents types de sorties puisque le codage est homogénéisé pour un format spécifique sans avoir à se fier à différents paquets r ayant chacun leurs propres spécificités (Par exemple bookdown, hugodown, blogdown, thesisdown, rticles, xaringan, …).\nDans la suite de ce chapitre, nous parlerons de Quarto mais beaucoup de choses peuvent être faites avec R markdown. Quarto utilise .qmd alors que R markdown fonctionne avec des fichiers .Rmd mais Quarto peut rendre .Rmd également.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#pourquoi-utiliser-quarto",
    "href": "06-quarto.html#pourquoi-utiliser-quarto",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.2 Pourquoi utiliser Quarto ?",
    "text": "6.2 Pourquoi utiliser Quarto ?\nAu cours des chapitres précédents, nous avons beaucoup parlé de la nécessité de mener vos recherches de manière robuste et reproductible afin de faciliter la science ouverte. En résumé, la science ouverte consiste à faire tout ce qui est en notre pouvoir pour rendre nos données, nos méthodes, nos résultats et nos conclusions transparents et accessibles à tous. Certains des principaux principes de la science ouverte sont décrits ici et comprennent\n\nTransparence de la méthodologie expérimentale, de l’observation, de la collecte des données et des méthodes d’analyse.\nDisponibilité publique et réutilisation des données scientifiques\nAccessibilité au public et transparence de la communication scientifique\nUtilisation d’outils en ligne pour faciliter la collaboration scientifique\n\nÀ l’heure actuelle, vous utilisez tous (avec un peu de chance) R pour explorer et analyser vos données intéressantes. En tant que tel, vous êtes déjà bien avancé dans votre démarche visant à rendre votre analyse plus reproductible, plus transparente et plus facile à partager. Cependant, il se peut que votre flux de travail actuel ressemble à ceci :\n\n\n\n\n\n\n\nFigure 6.1: Non-reproducible workflow\n\n\n\n\nVos données sont importées dans R à partir de votre tableur préféré, vous écrivez votre code R pour explorer et analyser vos données, vous enregistrez les tracés sous forme de fichiers externes, vous copiez les tableaux des résultats d’analyse, puis vous combinez manuellement tous ces éléments et votre prose écrite dans un seul document MS Word (peut-être pour un article ou un chapitre de thèse). Bien qu’il n’y ait rien de particulièrement mauvais dans cette approche (et qu’elle soit certainement meilleure que l’utilisation d’un logiciel “pointer-cliquer” pour analyser vos données), elle présente certaines limites :\n\nElle n’est pas particulièrement reproductible. Parce que ce flux de travail sépare votre code R du document final, il y a de multiples occasions de prendre des décisions non documentées (quels graphiques avez-vous utilisés ? quelles analyses avez-vous incluses ou non ? etc.)\nIl est inefficace. Si vous devez revenir en arrière et modifier quelque chose (créer un nouveau graphique ou mettre à jour votre analyse, etc.), vous devrez créer ou modifier plusieurs documents, ce qui augmente le risque que des erreurs se glissent dans votre flux de travail.\nIl est difficile à maintenir. Si votre analyse change, vous devrez à nouveau mettre à jour plusieurs fichiers et documents.\nIl peut être difficile de décider ce qui doit être partagé avec d’autres. Partagez-vous l’ensemble de votre code (exploration initiale des données, validation du modèle, etc.) ou seulement le code spécifique à votre document final ? Il est assez courant (et mauvais !) pour les chercheurs de maintenir deux scripts R, l’un utilisé pour l’analyse proprement dite et l’autre à partager avec le document final ou le chapitre de la thèse. Cela peut prendre du temps et prêter à confusion et devrait être évité.\n\nUn flux de travail plus efficace et plus robuste pourrait ressembler à ceci :\n\n\n\n\n\n\n\nFigure 6.2: A-reproducible (and more fficient) workflow\n\n\n\n\nVos données sont importées dans R comme précédemment, mais cette fois-ci, tout le code R que vous avez utilisé pour analyser vos données, produire vos graphiques et votre texte écrit (Introduction, Matériel et Méthodes, Discussion, etc.) est contenu dans un seul document Quarto qui est ensuite utilisé (avec vos données) pour créer automatiquement votre document final. C’est exactement ce que Quarto vous permet de faire.\nVoici quelques-uns des avantages de l’utilisation de Quarto :\n\nIl relie explicitement vos données à votre code R et à vos résultats, créant ainsi un flux de travail entièrement reproductible. TOUT du code R utilisé pour explorer, résumer et analyser vos données peut être inclus dans un seul document facile à lire. Vous pouvez décider de ce que vous voulez inclure dans votre document final (comme vous l’apprendrez ci-dessous), mais tout votre code R peut être inclus dans le document Quarto.\nVous pouvez créer une grande variété de formats de sortie (pdf, pages web html, MS Word et bien d’autres) à partir d’un seul document Quarto, ce qui améliore à la fois la collaboration et la communication.\nAméliore la transparence de votre recherche. Vos données et votre fichier Quarto peuvent être joints à votre publication ou au chapitre de votre thèse en tant que matériel supplémentaire ou être hébergés sur un dépôt GitHub (voir Chapitre 7).\nAugmente l’efficacité de votre flux de travail. Si vous avez besoin de modifier ou d’étendre votre analyse actuelle, il vous suffit de mettre à jour votre document Quarto et ces changements seront automatiquement inclus dans votre document final.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#commencer-avec-quarto",
    "href": "06-quarto.html#commencer-avec-quarto",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.3 Commencer avec Quarto",
    "text": "6.3 Commencer avec Quarto\nQuarto s’intègre très bien avec R Studio et VS Code et fournissent à la fois un éditeur de source et un éditeur visuel offrant une expérience proche de votre logiciel d’écriture classique WYSIWYG (ce que vous voyez est ce que vous écrivez) (par exemple Microsoft Word ou LibreOffice writer).\n\n6.3.1 Installation de la solution\nPour utiliser Quarto, vous devez d’abord installer le logiciel Quarto et le logiciel quarto 📦 (avec ses dépendances). Vous trouverez des instructions sur la façon de procéder dans Section 1.1.1 et sur le site web de Quarto. Si vous souhaitez créer des documents PDF (ou des documents MS Word) à partir de votre fichier Quarto, vous devrez également installer une version de {{&lt; latex &gt;}} sur votre ordinateur. Si vous n’avez pas installé {{&lt; latex &gt;} }, nous vous recommandons d’installer TinyTeX . Là encore, des instructions sur la manière de procéder sont disponibles à l’adresse suivante : Section 1.1.1.\n\n6.3.2 Créez un document Quarto, .qmd\n\nIl est temps de créer votre premier document Quarto. Dans RStudio, cliquez sur le menu File -&gt; New File -&gt; Quarto.... Dans la fenêtre qui s’ouvre, donnez un “titre” au document, saisissez les informations relatives à l’“auteur” (votre nom) et sélectionnez HTML comme format de sortie par défaut. Nous pourrons modifier tout cela ultérieurement, ne vous en préoccupez donc pas pour l’instant.\n\n\n\n\n\n\n\nFigure 6.3: Creating a Quarto document\n\n\n\n\nVous remarquerez que lorsque votre nouveau document Quarto est créé, il comprend un exemple de code Quarto. Normalement, vous devriez surligner et supprimer tout ce qui se trouve dans le document, à l’exception de l’information située en haut, entre les boutons --- (c’est ce qu’on appelle l’en-tête YAML dont nous parlerons dans un instant) et commencer à écrire votre propre code. Cependant, pour l’instant, nous allons utiliser ce document pour nous entraîner à convertir Quarto aux formats html et pdf et vérifier que tout fonctionne.\n\n\n\n\n\n\n\nFigure 6.4: A new Quarto document\n\n\n\n\nUne fois que vous avez créé votre document Quarto, c’est une bonne pratique de sauvegarder ce fichier dans un endroit pratique (Section 1.4 et Figure 1.11). Vous pouvez le faire en sélectionnant File -&gt; Save dans le menu de RStudio (ou utilisez le raccourci clavier ctrl + s sous Windows ou cmd + s sur Mac) et entrez un nom de fichier approprié (appelez-le par exemple my_first_quarto). Notez que l’extension de votre nouveau fichier Quarto est .qmd.\nMaintenant, pour convertir votre fichier .qmd en document HTML, cliquez sur le petit triangle noir à côté de l’icône Knit en haut de la fenêtre source et sélectionnez knit to HTML\n\n\n\n\n\n\n\nFigure 6.5: Knitting a Qmd file\n\n\n\n\nRStudio va maintenant “tricoter” (ou rendre) votre fichier .qmd en un fichier HTML. Remarquez qu’il y a un nouveau Quarto dans votre fenêtre de console, qui vous fournit des informations sur le processus de rendu et affiche également les erreurs si quelque chose ne va pas.\nSi tout s’est déroulé sans problème, un nouveau fichier HTML a été créé et enregistré dans le même répertoire que votre fichier .qmd (le nôtre s’appellera my_first_quarto.html). Pour visualiser ce document, il suffit de double-cliquer sur le fichier pour l’ouvrir dans un navigateur (comme Chrome ou Firefox) et afficher le contenu rendu. RStudio affichera également un aperçu du fichier rendu dans une nouvelle fenêtre pour que vous puissiez le vérifier (votre fenêtre peut être légèrement différente si vous utilisez un ordinateur Windows).\n\n\n\n\n\n\n\nFigure 6.6: A my first rendered html\n\n\n\n\nVous venez de rendre votre premier document Quarto. Si vous souhaitez tricoter votre .qmd en un document pdf, il vous suffit de choisir knit to PDF au lieu de knit to HTML lorsque vous cliquez sur le bouton knit l’icône Cela créera un fichier appelé my_first_quarto.pdf que vous pouvez ouvrir en double-cliquant dessus. Essayez-le !\nVous pouvez également tricoter un .qmd en utilisant la ligne de commande dans la console plutôt qu’en cliquant sur l’icône de tricotage. Pour ce faire, il suffit d’utiliser la commande quarto_render() de la fonction quarto 📦 package, comme indiqué ci-dessous. Là encore, vous pouvez modifier le format de sortie à l’aide de la fonction output_format = ainsi que de nombreuses autres options.\nlibrary(quarto)\n\nquarto_render('my_first_quarto.qmd', output_format = 'html_document')\n\n# alternatively if you don't want to load the quarto package\n\nquarto::quarto_render('my_first_quarto.Rmd', output_format = 'html_document')",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#anatomie-du-document-quarto-.qmd",
    "href": "06-quarto.html#anatomie-du-document-quarto-.qmd",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.4 Anatomie du document Quarto (.qmd)",
    "text": "6.4 Anatomie du document Quarto (.qmd)\nMaintenant que vous pouvez rendre un fichier Quarto dans RStudio aux formats HTML et PDF, examinons de plus près les différents composants d’un document Quarto typique. Normalement, chaque document Quarto est composé de 3 éléments principaux :\n\nun en-tête YAML\ntexte formaté\ndes morceaux de code.\n\n\n\n\n\n\n\n\nFigure 6.7: Structure of a qmd file\n\n\n\n\n\n6.4.1 En-tête YAML\nYAML signifie ’YAML Ain’t Markup L anguage” (c’est une blague “in”) [plaisanterie][blague] !) et ce composant optionnel contient les métadonnées et les options pour l’ensemble du document comme le nom de l’auteur, la date, le format de sortie, etc. L’en-tête YAML est entouré avant et après d’une balise --- sur sa propre ligne. Dans RStudio, un en-tête YAML minimal est automatiquement créé pour vous lorsque vous créez un nouveau document Quarto comme nous l’avons fait ci-dessus (Section 6.3.2), mais vous pouvez le modifier à tout moment. Un en-tête YAML simple peut ressembler à ceci :\n---\ntitle: My first Quarto document\nauthor: Jane Doe\ndate: March 01, 2020\nformat: html\n---\nDans l’en-tête YAML ci-dessus, le format de sortie est défini sur HTML. Si vous souhaitez changer le format de sortie au format pdf, vous pouvez le changer de format: html à format: pdf (vous pouvez également définir plusieurs formats de sortie si vous le souhaitez). Vous pouvez également modifier la police et la taille de police par défaut pour l’ensemble du document et même inclure des options fantaisistes telles qu’une table des matières, des références en ligne et une bibliographie. Si vous souhaitez explorer la pléthore d’autres options, voir ici . Attention, la plupart des options que vous pouvez spécifier dans l’en-tête YAML fonctionneront avec les documents au format HTML et PDF, mais pas toutes. Si vous avez besoin de plusieurs formats de sortie pour votre document Quarto, vérifiez si vos options YAML sont compatibles avec ces formats. De plus, l’indentation dans l’en-tête YAML a une signification, soyez donc prudent lorsque vous alignez du texte. Par exemple, si vous souhaitez inclure une table des matières, vous devez modifier l’option output: dans l’en-tête YAML comme suit\n---\ntitle: My first Quarto document\nauthor: Bob Hette\ndate: March 01, 2020\nformat:\n  html:\n    toc: true\n---\n\n6.4.2 Texte formaté\nComme mentionné ci-dessus, l’un des avantages de Quarto est que vous n’avez pas besoin de votre traitement de texte pour rassembler votre code R, votre analyse et votre écriture. Quarto est capable de restituer (presque) tout le formatage de texte dont vous pourriez avoir besoin, comme l’italique, le gras, le barré, l’indice supérieur et l’indice inférieur, ainsi que les listes à puces et numérotées, les en-têtes et les pieds de page, les images, les liens vers d’autres documents ou pages web, et aussi les équations. Cependant, contrairement à ce que vous connaissez, le Ce que vous voyez, c’est ce que vous obtenez ( WYSIWYG ), vous ne voyez pas le texte formaté final dans votre document Quarto (comme vous le feriez avec MS Word), mais vous devez “baliser” le formatage de votre texte pour qu’il soit rendu dans votre document de sortie. À première vue, cela peut sembler une véritable plaie, mais c’est en fait très facile à faire et cela présente de nombreux avantages. avantages (passez-vous plus de temps à embellir votre texte dans MS Word qu’à rédiger un contenu de qualité ?)\nVoici un exemple de marquage du formatage du texte dans un document Quarto\n#### Tadpole sediment experiment\n\nThese data were obtained from a mesocosm experiment which aimed to examine the\neffect of bullfrog tadpoles (*Lithobates catesbeianus*) biomass on sediment\nnutrient (NH~4~, NO~3~ and PO~3~) release.\nAt the start of the experiment 15 replicate mesocosms were filled with\n20 cm^2^ of **homogenised** marine sediment and assigned to one of five \ntadpole biomass treatments.\nqui ressemblerait à ceci dans le document rendu final (pouvez-vous repérer les marques ?)\n\nExpérience de sédimentation sur les têtards\n\n\nCes données proviennent d’une expérience en mésocosme qui visait à examiner les effets de la sédimentation sur les têtards. l’effet des têtards de grenouille-taureau (Lithobates catesbeianus) sur les sédiments nutriments (NH4 NO3 et PO3). Au début de l’expérience, 15 mésocosmes répétés ont été remplis de 20 cm2 de homogénéisé sédiments marins homogénéisés et classés dans l’une des cinq catégories suivantes biomasse de têtards.\n\nL’accent est mis\nLa syntaxe markdown la plus courante pour mettre en valeur et formater du texte est présentée ci-dessous.\n\n\n\n\n\n\n\nObjectif\nQuarto\nproduction\n\n\n\ntexte en gras\n**mytext**\nmon texte\n\n\ntexte en italique\n*mytext*\nmon texte\n\n\nbarré\n~~mytext~~\nmon texte\n\n\nexposant\nmytext^2^\nmon texte2\n\n\n\nindice\nmytext~2~\nmon texte2\n\n\n\n\nIl est intéressant de noter qu’il n’y a pas de soulignement par défaut dans la syntaxe de R markdown, pour des raisons plus ou moins ésotériques (par exemple. un soulignement est considéré comme un élément stylistique (il peut y avoir d’autres raisons). [raisons][souligner] )). Quarto a corrigé ce problème, vous pouvez simplement faire [text to underline]{.underline} pour souligner votre texte.\nEspaces blancs et sauts de ligne\nL’une des choses qui peut être déroutante pour les nouveaux utilisateurs de markdown est l’utilisation des espaces et des retours à la ligne (la touche Entrée de votre clavier). En markdown, les espaces multiples dans le texte sont généralement ignorés, tout comme les retours à la ligne. Par exemple, ce texte en markdown\nThese      data were      obtained from a\nmesocosm experiment which    aimed to examine the\neffect\nof          bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\nsera rendu sous la forme\n\nCes données ont été obtenues à partir d’un expérience en mésocosme qui visait à examiner l’impact de l’utilisation de l’eau sur la santé. l’effet des têtards de grenouille-taureau (Lithobates catesbeianus) biomasse.\n\nC’est généralement une bonne chose (plus d’espaces multiples aléatoires dans votre texte). Si vous voulez que votre texte commence sur une nouvelle ligne, vous pouvez simplement ajouter deux espaces vides à la fin de la ligne précédente.\n\nCes données ont été obtenues à partir d’un\nexpérience en mésocosme qui visait à examiner l’impact de l’utilisation de l’eau sur la qualité de l’air.\neffet des têtards de grenouille-taureau (Lithobates catesbeianus).\n\nSi vous voulez vraiment des espaces multiples dans votre texte, vous pouvez utiliser la fonction Nsur breaking space tag &nbsp;\nThese &nbsp; &nbsp; &nbsp; data were &nbsp; &nbsp; &nbsp; &nbsp; obtained from a  \nmesocosm experiment which &nbsp; &nbsp; aimed to examine the    \neffect &nbsp; &nbsp; &nbsp; &nbsp; bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\n\nCes données ont été obtenues à partir d’un\nd’une expérience en mésocosme qui visait à examiner l’impact de l’utilisation de l’eau sur la santé.\neffet des têtards de grenouille-taureau (Lithobates catesbeianus).\n\nRubriques\nVous pouvez ajouter des titres et des sous-titres à votre document Quarto en utilisant la fonction # en début de ligne. Vous pouvez réduire la taille des titres en ajoutant simplement plus de # symboles. Par exemple, il est possible de réduire la taille des titres en ajoutant simplement des symboles supplémentaires.\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\npermet d’obtenir des titres par ordre de taille décroissante\n\nEn-tête 1\nEn-tête 2\nEn-tête 3\nEn-tête 4\nEn-tête 5\nEn-tête 6\n\nCommentaires\nComme vous pouvez le voir ci-dessus, la signification de la # est différente lorsqu’il s’agit de formater du texte dans un document Quarto par rapport à un script R standard (qui est utilisé pour inclure un commentaire - vous vous souvenez ?!). Vous pouvez cependant utiliser un # pour commenter du code à l’intérieur d’un morceau de code (Section 6.4.3) comme d’habitude (plus d’informations à ce sujet dans un instant). Si vous souhaitez inclure un commentaire dans votre document Quarto en dehors d’un morceau de code qui ne sera pas inclus dans le document rendu final, placez votre commentaire entre les symboles &lt;!-- et --&gt;.\n&lt;!--\nthis is an example of how to format a comment using Quarto.\n--&gt;\nListes\nSi vous souhaitez créer une liste de texte à puces, vous pouvez mettre en forme une liste non ordonnée avec des sous-éléments. Notez que les sous-éléments doivent être indentés.\n- item 1\n- item 2\n   + sub-item 2\n   + sub-item 3\n- item 3\n- item 4\n\n\nélément 1\npoint 2\n\nsous-poste 2\nsous-poste 3\n\n\npoint 3\npoint 4\n\n\nSi vous avez besoin d’une liste ordonnée\n1. item 1\n1. item 2\n    + sub-item 2\n    + sub-item 3\n1. item 3\n1. item 4\n\n\narticle 1\npoint 2\n\n\nsous-poste 2\nsous-poste 3\n\n\npoint 3\npoint 4\n\n\nLiens\nOutre les images, vous pouvez également inclure des liens vers des pages web ou d’autres liens dans votre document. Utilisez la syntaxe suivante pour créer un lien cliquable vers une page web existante. Le texte du lien est placé entre les crochets et l’URL de la page web entre les crochets ronds immédiatement après.\nYou can include a text for your clickable [link](https://www.worldwildlife.org)\nqui vous donne :\n\nVous pouvez inclure un texte pour votre lien cliquable cliquable\n\n\n6.4.3 Morceaux de code\nVenons-en maintenant au cœur du problème. Pour inclure du code R dans votre document Quarto, il vous suffit de placer votre code dans un “morceau de code”. Tous les morceaux de code commencent et se terminent par trois antisèches ```````````. Remarque : ces signes sont également appelés “accents graves” ou “guillemets arrière” et n’ont rien à voir avec une apostrophe ! Sur la plupart des claviers, vous pouvez [trouver la coche arrière][bâton arrière] sur la même touche que le tilde (~).\n```{r}\nAny valid R code goes here\n```\nVous pouvez insérer un morceau de code soit en tapant les délimiteurs du morceau ```{r} et ``````````soit en utilisant l'option de votre IDE (barre d'outils de RStudio (bouton Insérer) ou en cliquant sur le menuCode-&gt;Insert Chunk`. Dans VS Code, vous pouvez utiliser des extraits de code). Le mieux est peut-être de se familiariser avec les raccourcis clavier de votre IDE ou de vos extraits de code.\nIl y a beaucoup de choses que vous pouvez faire avec des morceaux de code : vous pouvez produire du texte à partir de votre analyse, créer des tableaux et des figures et insérer des images, entre autres choses. À l’intérieur du morceau de code, vous pouvez placer des règles et des arguments entre les accolades. {} qui vous permettent de contrôler l’interprétation de votre code et le rendu des résultats. C’est ce qu’on appelle les options du bloc de code. La seule option obligatoire est le premier argument qui spécifie le langage utilisé (r dans notre cas, mais [d’autres][moteurs] langues sont prises en charge). Remarque : les options de blocs peuvent être écrites de deux manières :\n\nsoit toutes les options de blocs doivent être écrites entre les crochets, sur une seule ligne, sans retour à la ligne. 1. soit elles peuvent être écrites en utilisant une notation YAML à l’intérieur du morceau de code en utilisant #| au début de la ligne.\n\nNous utilisons la notation YAML pour les options du bloc de code car nous la trouvons beaucoup plus facile à lire lorsque vous avez plusieurs options de longues légendes.\nVous pouvez également spécifier un nom (ou étiquette) facultatif pour le morceau de code, ce qui peut s’avérer utile en cas de problèmes de débogage et de rendu avancé de documents. Dans le bloc suivant, nous nommons le morceau de code summary-stats, nous chargeons le paquet ggplot2 📦 créer un cadre de données (dataf) avec deux variables x et y, utiliser la méthode summary() pour afficher des statistiques sommaires et tracer un nuage de points des données à l’aide de la fonction ggplot(). Lorsque nous exécutons le bloc de code, le code R et la sortie résultante sont affichés dans le document final.\n```{r, summary-stats, echo = TRUE, fig.cap = \"Caption for a simple figure but making the chunk options long and hard to read\"}\nlibrary(ggplot)\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n```\n```{r}\n#| label: summary-stats\n#| echo: true\n#| fig-cap = \"Caption for a simple figure but making the chunk options long and hard to read\"\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n```\nTous deux produiront\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n\n\n\n\n\n\nFigure 6.8: Caption for a simple figure but making the chunk options long and hard to read\n\n\n\n\nLorsque vous utilisez des noms de morceaux, assurez-vous que vous n’avez pas de noms de morceaux en double dans votre document Quarto et évitez les espaces et les points, car cela posera des problèmes lorsque vous devrez tricoter votre document. - pour séparer les mots dans nos noms de blocs).\nSi nous voulons afficher uniquement la sortie de notre code R (juste les statistiques sommaires par exemple) et non le code lui-même dans notre document final, nous pouvons utiliser l’option chunk. echo=FALSE\n```{r}\n#| label: summary-stats2\n#| echo: false\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\n\nPour afficher le code R mais pas la sortie, utilisez l’option results='hide' l’option chunk.\n```{r}\n#| label: summary-stats\n#| results: 'hide'\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n\nIl peut arriver que vous souhaitiez exécuter un morceau de code sans afficher aucune sortie. Vous pouvez supprimer la totalité de la sortie en utilisant l’option chunk include: false.\n```{r}\n#| label: summary-stats4\n#| include: false\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\nIl existe un grand nombre d’options de morceaux documentées ici avec une version plus condensée ici . Les plus couramment utilisées sont résumées ci-dessous, les valeurs par défaut étant indiquées.\n\n\n\n\n\n\n\nOption d’assemblage\nvaleur par défaut\nFonction\n\n\n\nécho\necho: true\nSi false n’affichera pas le code dans le document final\n\n\nrésultats\nresults: 'markup'\nSi “cacher”, les résultats du code ne seront pas affichés dans le document final.\n\n\nSi “hold”, l’affichage de tous les éléments de sortie sera retardé jusqu’à la fin du morceau.\n\n\n\n\nSi ‘asis’, les résultats seront affichés sans être reformatés.\n\n\n\n\n\n\n\ninclude | include: true | Si false exécute le bloc mais ne l’inclut pas dans le document final.\neval | eval: true | Si false n’exécutera pas le code contenu dans le morceau de code.\nmessage | message: true | Si false n’affichera pas les messages générés par le code.\navertissement | warning: true | Si false n’affichera pas les messages d’avertissement générés par le code.\n\n\n6.4.4 Code R en ligne\nJusqu’à présent, nous avons écrit et exécuté notre code R par morceaux. Une autre bonne raison d’utiliser Quarto est que nous pouvons également inclure notre code R directement dans notre texte. C’est ce qu’on appelle le “code en ligne”. Pour inclure votre code dans votre texte Quarto, il vous suffit d’écrire r write your code here. Cela peut s’avérer très utile lorsque vous souhaitez inclure des statistiques sommaires dans votre texte. Par exemple, nous pourrions décrire le iris comme suit :\nMorphological characteristics (variable names: \n`r names(iris)[1:4]`) were measured from \n`r nrow(iris)` *Iris sp.* plants from \n`r length(levels(iris$Species))` different species.\nThe mean Sepal length was\n`r round(mean(iris$Sepal.Length), digits = 2)` mm.\n  \nqui sera rendu par\n\nCaractéristiques morphologiques (noms de variables : Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) étaient mesurés à partir de 150 l’iris plantes de 3 différentes espèces. La longueur moyenne des sépales était de 5.84 mm.\n\nL’avantage d’inclure du code R en ligne dans votre texte est que ces valeurs seront automatiquement mises à jour si vos données changent.\n\n6.4.5 Images et photos\nLa possibilité d’intégrer des images et des liens vers des pages web (ou d’autres documents) dans votre document Quarto est une fonctionnalité utile. Vous pouvez inclure des images dans votre document Quarto de différentes manières. La méthode la plus simple est sans doute d’utiliser le format markdown de Quarto :\n![Image caption](path/to/you/image){options}\nVoici un exemple avec une image occupant 75 % de la largeur et centrée.\n![Waiting for the eclipse](images/markdown/eclipse_ready.jpg){fig-align=\"center\" width=\"75%\"}\nrésultant en :\n\n\n\n\n\nFigure 6.9: En attendant l’éclipse\n\n\nUne autre façon d’inclure des images dans votre document est d’utiliser la fonction include_graphics() de la fonction knitr du paquet. Le code suivant produira un résultat similaire.\n```{r}\n#| label: fig-knitr\n#| fig-align: center\n#| out-width: 75%\n#| fig-cap: Waiting for the eclipse\nknitr::include_graphics(\"images/markdown/eclipse_ready.jpg\")\n```\nLe code ci-dessus ne fonctionnera que si le fichier image (eclipse_ready.jpg) se trouve au bon endroit par rapport à l’endroit où vous avez enregistré votre fichier .qmd fichier. Dans l’exemple, le fichier image se trouve dans un sous-répertoire (dossier) appelé images/markdown dans le répertoire où nous avons enregistré notre my_first_quarto.qmd fichier. Vous pouvez intégrer des images enregistrées dans de nombreux types de fichiers différents, mais les plus courants sont les suivants .jpg et .png.\n\n6.4.6 Les chiffres\nPar défaut, les figures produites par le code R sont placées immédiatement après le morceau de code à partir duquel elles ont été générées. Par exemple :\n\n```{r}\n#| label: fig-simple-plot\n#| fig-cap: A simple plot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.10: A simple plot\n\n\n\n\nLe fichier fig-cap: permet de fournir une légende de figure reconnue par Quarto et utilisée dans la numérotation des figures et les références croisées (Section 6.4.8).\nSi vous souhaitez modifier les dimensions de l’intrigue dans le document final, vous pouvez utiliser l’option fig-width: et fig-height: (en pouces !). Vous pouvez également modifier l’alignement de la figure à l’aide de la fonction fig-align: option chunk.\n\n```{r}\n#| label: fig-simple-plot2\n#| fig-cap: A shrinked figure\n#| fig-width: 4\n#| fig-height: 3\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.11: A shrinked figure\n\n\n\n\nVous pouvez ajouter une légende à la figure à l’aide de la fonction fig-cap: option.\n\n```{r}\n#| label: fig-simple-plot-cap\n#| class-source: fold-show\n#| fig-cap: A simple plot\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.12: A simple plot\n\n\n\n\nSi vous souhaitez supprimer la figure dans le document final, utilisez l’option fig-show: 'hide' pour supprimer la figure dans le document final.\n\n```{r}\n#| label: fig-simple-plot5\n#| fig-show: hide\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\nSi vous utilisez un logiciel comme ggplot2 📦 pour créer vos parcelles, n’oubliez pas que vous devrez rendre le paquet disponible avec l’option library() dans le morceau de code (ou dans un morceau de code précédent).\n\n```{r}\n#| label: fig-simple-ggplot\n#| fig-cap: A simple ggplot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure 6.13: A simple ggplot\n\n\n\n\nLà encore, il existe un grand nombre d’options spécifiques à la production de graphiques et de figures. Voir ici pour plus de détails.\n\n6.4.7 Tableaux\nDans Quarto, vous pouvez créer des tableaux en utilisant la syntaxe markdown native (il n’est pas nécessaire que ce soit dans un morceau de code).\n|  x  |  y  |\n|:---:|:---:|\n|  1  |  5  | \n|  2  |  4  |\n|  3  |  3  |\n|  4  |  2  |\n|  5  |  1  |\n\n: Caption for a simple markdown table\n\n\n\n\n\n\nx\ny\n\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n2\n\n\n5\n1\n\n\n\nCaption pour un simple tableau markdown {#tbl-simp-md}\nLe :-------: indique à markdown que la ligne du dessus doit être traitée comme un en-tête et les lignes du dessous comme le corps du tableau. L’alignement à l’intérieur du tableau est déterminé par la position de la balise :. Pour centrer l’alignement, utilisez :------: pour aligner à gauche :------ et aligner à droite ------:. Bien qu’il puisse être amusant ( !) de créer des tableaux avec des balises brutes, cela n’est pratique que pour les tableaux très petits et très simples.\nLa façon la plus simple d’inclure des tableaux dans un document Quarto est d’utiliser la fonction kable() de la fonction knitr 📦 package. La fonction kable() permet de créer des tableaux pour les formats HTML, PDF et Word.\nPour créer un tableau des 2 premières lignes par espèce de l’échantillon iris à l’aide de la fonction kable() il suffit d’écrire\nlibrary(knitr)\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\nkable()\nou sans charger knitr 📦 mais en indiquant où trouver le kable() fonction.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable()\n\n\n\nTable 6.1: A simple kable table\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n\n\n\n\n\n\nLa fonction kable() offre de nombreuses options pour modifier la mise en forme du tableau. Par exemple, si nous voulons arrondir les valeurs numériques à une décimale, utilisez la fonction digits = argument. Pour centrer le contenu du tableau, utilisez align = 'c' et pour fournir des en-têtes de colonne personnalisés, utilisez l’argument col.names = argument. Voir ?knitr::kable pour plus d’informations.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable(\n    digits=0,\n    align = 'c',\n    col.names = c(\n      'Sepal length', 'Sepal width',\n      'Petal length', 'Petal width', 'Species'\n    )\n)\n\n\n\nTable 6.2: A nicer kable table\n\n\n\n\nSepal length\nSepal width\nPetal length\nPetal width\nSpecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n7\n3\n5\n1\nversicolor\n\n\n6\n3\n4\n2\nversicolor\n\n\n6\n3\n6\n2\nvirginica\n\n\n6\n3\n5\n2\nvirginica\n\n\n\n\n\n\n\n\nVous pouvez encore améliorer l’aspect de votre kable tables en utilisant la fonction kableExtra 📦 package (n’oubliez pas d’installer le package au préalable !). Voir ici pour plus de détails et un tutoriel utile.\nSi vous voulez encore plus de contrôle et d’options de personnalisation pour vos tables, jetez un coup d’œil à l’option gt 📦 [paquet][gt] . gt est un acronyme pour grammar de tet est basé sur un principe similaire pour les tableaux qui sont utilisés pour les tracés dans ggplot.\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  rename_with(~ gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n\n\nTable 6.3: A nice gt table\n\n\n\n\n\n\nSepal Length\nSepal Width\nPetal Length\nPetal Width\n\n\n\nsetosa\n\n\n5.1\n3.5\n1.4\n0.2\n\n\n4.9\n3.0\n1.4\n0.2\n\n\nversicolor\n\n\n7.0\n3.2\n4.7\n1.4\n\n\n6.4\n3.2\n4.5\n1.5\n\n\nvirginica\n\n\n6.3\n3.3\n6.0\n2.5\n\n\n5.8\n2.7\n5.1\n1.9\n\n\n\n\n\n\n\n\n\nDans la plupart des paquets R développés pour produire des tableaux, il existe des options permettant d’inclure des légendes de tableaux. Cependant, si vous souhaitez ajouter une légende de tableau, nous vous recommandons d’utiliser l’option code chunk dans Quarto tbl-cap: car cela permet des références croisées (Section 6.4.8) et une meilleure intégration dans le document.\n```{r}\n#| label: tbl-gt-table\n#| tbl-cap: A nice gt table\n#| echo: true\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n=2) %&gt;%\n  rename_with(~gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n```\n\n6.4.8 Références croisées\nLes références croisées permettent aux lecteurs de naviguer plus facilement dans votre document en fournissant des références numérotées et des liens hypertextes vers diverses entités telles que les figures et les tableaux. Une fois configurée, la numérotation des tableaux et des figures se fait automatiquement, de sorte qu’il n’est pas nécessaire de renuméroter toutes les figures lorsque vous en ajoutez ou en supprimez une.\nChaque entité pouvant faire l’objet d’une référence croisée nécessite une étiquette (un identifiant unique) précédée d’un type de référence croisée, par exemple #fig-element.\nPour plus de détails, voir le site section sur les références croisées sur le site web de Quarto.\n\n6.4.8.1 Sections du document\nVous pouvez faire des références croisées avec d’autres sections du document. Pour ce faire, vous devez\n\ndéfinir un identifiant pour la section vers laquelle vous souhaitez établir un lien. L’identifiant doit :\n\n\ncommencer par #sec-\n\nen minuscules (figure 6.3)\nn’a pas d’espace, en utilisant - à la place\n\n\nutiliser le @ et l’identifiant pour faire référence à la section\n\n## Cross-referencing sections {#sec-cross-ref-sections}\n\n[...]\n\nAs seen before(@sec-cross-ref-sections)\n\n6.4.8.2 Images, figures et tableaux\nPour les tableaux, les images et les figures, outre l’identifiant, l’élément doit également être accompagné d’une légende pour que les références croisées fonctionnent.\nLe préfixe pour les tableaux est #tbl- et #fig- pour les images et les figures.\nVoici un exemple d’image incluse dans un texte en markdown :\n![Rocking the eclipse](images/markdown/eclipse_ready.jpg){#fig-cute-dog}\n\nSee @fig-cute-dog for an illustration.\n\n\n\n\n\nFigure 6.14: Le rock de l’éclipse\n\n\nVoir Figure 6.14 pour une illustration.\nPour les figures et les tableaux produits avec des morceaux de code R, il suffit de fournir l’identifiant dans le champ label et la légende également en tant qu’option de bloc.\nVoici le code pour une figure et un tableau.\n\n```{r}\n#| label: fig-cr-plot\n#| fig-cap: A nice figure\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure 6.15: A nice figure\n\n\n\n\n\n```{r}\n#| label: tbl-cr-table\n#| tbl-cap: A nice table\n#| warning: false\nlibrary(knitr)\nkable(iris[1:5,], digits=0, align = 'c', col.names = c('sepal length', 'sepal width', 'petal length', 'petal width', 'species'))\n```\n\n\nTable 6.4: A nice table\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\nspecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n2\n0\nsetosa\n\n\n5\n4\n1\n0\nsetosa\n\n\n\n\n\n\n\n\nEn utilisant les références croisées, on peut écrire :\nTel que vu sur @fig-cr-plot et @tbl-cr-table …\nPour obtenir :\nComme vu sur Figure 6.15 et Table 6.4 …\n\n6.4.9 Citations et bibliographie\nPour générer des citations et une bibliographie, Quarto a besoin de :\n\nun document correctement formaté .qmd formaté\nun fichier source bibliographique comprenant toutes les informations pour les citations. Il fonctionne avec une grande variété de formats mais nous suggérons l’utilisation de {{&lt; bibtex &gt;} le format }.\n(optionnel) un fichier CSL qui spécifie le formatage à utiliser lors de la génération des citations et de la bibliographie.\n\nLa source bibliographique et le fichier csl (facultatif) sont spécifiés dans l’en-tête yaml sous la forme :\n---\ntitle: \"My Document\"\nbibliography: references.bib\ncsl: ecology.csl\n---\n\n6.4.9.1 Citations\nQuarto utilise la représentation markdown standard de Pandoc pour les citations (par ex. [@citation]) - les citations sont placées entre crochets et séparées par des points-virgules. Chaque citation doit avoir une clé, composée de ‘@’ + l’identifiant de la base de données, et peut optionnellement avoir un préfixe, un localisateur, et un suffixe. La clé de la citation doit commencer par une lettre, un chiffre ou , et peut contenir des caractères alphanumériques,  et des caractères de ponctuation internes.\n\n\n\n\n\n\nFormat Markdown\nSortie (default)\n\n\n\nLes licornes sont les meilleures [voir @martin1219, pp. 33-35 ; aussi @martin2200, chap. 1]\nLes licornes sont les meilleures (voir Martin 1219, pp. 33-35, aussi Martin 2200 chap. 1)\n\n\n\nLes licornes sont les meilleures [@martin2200 ; @martin1219]\nLes licornes sont les meilleures (Martin 1219, 2200)\n\n\n\nMartin dit que les licornes sont les meilleures [-@martin2200]\nMartin dit que les licornes sont les meilleures (2200)\n\n\n\n\n@martin1219 dit que les licornes sont les meilleures.\n\nMartin (1219) dit que les licornes sont les meilleures.\n\n\n\n@martin1219 [p. 33] dit que les licornes sont ce qu’il y a de mieux.\nLa plupart des gens disent que les licornes sont ce qu’il y a de mieux.\n\n\n\n6.4.9.2 Créer la bibliographie\nPar défaut, la liste des ouvrages cités sera automatiquement générée et placée en fin de document si le style l’exige. Elle sera placée dans une div avec l’id refs s’il y en a une comme\n### Bibliography\n\n::: {#refs}\n:::\nPour plus de détails, voir le site page Citation sur le site de Quarto.\n\n6.4.9.3 Intégration avec Zotero\nQuarto s’intègre très bien avec Zotero si vous utilisez l’éditeur visuel de RStudio ou VS Code.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#sec-tips-tricks",
    "href": "06-quarto.html#sec-tips-tricks",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.5 Quelques conseils et astuces",
    "text": "6.5 Quelques conseils et astuces\nProblème :\nLors du rendu de mon document Quarto au format pdf, mon code sort du bord de la page.\nSolution :\nAjoutez un argument global_options au début de votre fichier .qmd dans un morceau de code :\n```{r}\n#| label: global_options\n#| include: false \nknitr::opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE) \n```\nCe morceau de code ne sera pas affiché dans le document final en raison de l’argument global_options. include: false et vous devez placer le morceau de code immédiatement après l’en-tête YAML pour affecter tout ce qui se trouve en dessous.\ntidy.opts = list(width.cutoff = 60), tidy=TRUE définit le point de coupure de la marge et fait passer le texte à la ligne suivante. Jouez avec cette valeur pour l’obtenir correctement (60-80 devrait convenir à la plupart des documents).\nAvec quarto, vous pouvez également utiliser la fonction globale knitr dans un fichier knitrdans l’en-tête YAML (voir Site web de Quarto pour plus de détails).\n---\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk: \n    message: false\n    tidy.opts: !expr 'list(width.cutoff=60)'\n    tidy: true \n---\nProblème :\nLorsque je charge un paquet dans mon document Quarto, le rendu contient tous les messages de démarrage et/ou les avertissements.\nSolution :\nVous pouvez charger tous vos paquets au début de votre document Quarto dans un morceau de code en même temps que vous définissez vos options globales.\n```{r}\n#| label: global_options\n#| include: false\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning=FALSE,\n  tidy.opts=list(width.cutoff=60)\n) \nsuppressPackageStartupMessages(library(ggplot2))\n```\nL’option message = FALSE et warning = FALSE suppriment les messages et les avertissements. Les suppressPackageStartupMessages(library(ggplot2)) chargera le fichier ggplot2 📦 mais supprimera les messages de démarrage.\nLe problème est le suivant :\nLors de la conversion de mon document Quarto en PDF, mes tableaux et/ou figures sont répartis sur deux pages.\nSolution :\nAjoutez un saut de page à l’aide de la fonction {{&lt; latex &gt;}} \\pagebreak avant le tableau ou la figure incriminé(e)\nproblème :\nLe code dans mon document rendu est laid !\nSolution :\nAjouter l’argument tidy: true à vos arguments globaux. Cependant, cela peut parfois poser des problèmes, notamment en ce qui concerne l’indentation du code. La meilleure solution est d’écrire un code qui a de l’allure (insérer des espaces et utiliser plusieurs lignes)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#informations-complémentaires",
    "href": "06-quarto.html#informations-complémentaires",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.6 Informations complémentaires",
    "text": "6.6 Informations complémentaires\nBien que nous ayons couvert plus qu’il n’en faut pour vous permettre d’aller loin avec Quarto, nous n’avons eu le temps que d’effleurer la surface, comme c’est le cas pour la plupart des choses liées aux technologies de l’information et de la communication. Heureusement, il existe une mine d’informations à votre disposition si vous souhaitez approfondir vos connaissances et votre expérience. Un bon point de départ est l’excellent site web de Quarto ici.\nUn autre guide de référence Quarto utile et concis peut être trouvé ici\nUne feuille de calcul rapide et facile pour R Markdown de R Markdown",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#pratique",
    "href": "06-quarto.html#pratique",
    "title": "6  Rapports reproductibles avec Quarto",
    "section": "\n6.7 Pratique",
    "text": "6.7 Pratique\nNous allons créer un nouveau document Rmarkdown et l’éditer en utilisant les fonctions de base de Rmarkdown. R et Rmarkdown et les fonctions\n\n6.7.1 Le contexte\nNous utiliserons l’awesome palmerpenguins jeu de données 🐧 pour explorer et visualiser les données.\nCes données ont été collectées et partagées par Dr. Kristen Gorman et Station Palmer, Antarctique LTER.\nL’ensemble a été conçu par les Drs Allison Horst et Alison Hill. site officiel.\nLe paquet palmerpenguins comporte deux ensembles de données :\n\n\npenguins_raw a les données brutes des observations des manchots (voir ?penguins_raw pour plus d’informations)\n\npenguins est une version simplifiée des données brutes (voir ?penguins pour plus d’informations)\n\nPour cet exercice, nous allons utiliser la fonction penguins jeu de données.\n\nlibrary(palmerpenguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n6.7.2 Questions\n1) Installer le paquet palmerpenguins.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ninstall.packages(\"palmerpenguins\")\n\n\n\n\n2)\n\nCréez un nouveau document Quarto, nommez-le et enregistrez-le.\nSupprimez tout ce qui se trouve après la ligne 12.\nAjouter un nouveau titre de section, un texte simple et un texte en caractères gras.\nCompiler (“Tricoter”).\n\n3)\n\nAjoutez un morceau dans lequel vous chargez le palmerpenguins. La ligne de code correspondante devrait être cachée dans la sortie.\nChargez également le fichier tidyverse de paquets. Modifier les valeurs par défaut pour supprimer tous les messages.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n```{r}\n#| echo: false\n#| message:false\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n```\n\n\n\n4) Ajoutez un autre morceau dans lequel vous construisez un tableau avec les 10 premières lignes de l’ensemble de données.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n```{r}\npenguins %&gt;%\n  slice(1:10) %&gt;%\n  knitr::kable()\n```\n\n\n\n5) Dans une nouvelle section, affichez le nombre d’individus, d’espèces de manchots et d’îles que nous avons dans l’ensemble de données. Cette information doit apparaître directement dans le texte, vous devez utiliser du code en ligne. 😄. Calculer la moyenne des traits (numériques) mesurés sur les manchots.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Numerical exploration\n\nThere are `r nrow(penguins)` penguins in the dataset,\nand `r length(unique(penguins$species))` different species.\nThe data were collected in `r length(unique(penguins$island))`\nislands of the Palmer archipelago in Antarctica.\n\nThe mean of all traits that were measured on the penguins are:\n```{r}\n#| echo: false\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n```\n\n\n\n6) Dans une autre section, intitulée “Exploration graphique”, construisez une figure avec 3 histogrammes superposés, chacun correspondant à la masse corporelle d’une espèce.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Graphical exploration\n\nA histogram of body mass per species:\n```{r}\n#| fig-cap: Distribution of body mass by species of penguins\n  ggplot(data = penguins) +\n  aes(x = body_mass_g) +\n  geom_histogram(aes(fill = species),\n                 alpha = 0.5,\n                 position = \"identity\") +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_minimal() +\n  labs(x = \"Body mass (g)\",\n       y = \"Frequency\",\n       title = \"Penguin body mass\")\n```\n\n\n\n7) Dans une autre section, intitulée Régression linéaire Ajustez un modèle de la longueur du bec en fonction de la taille du corps (longueur des nageoires), de la masse corporelle et du sexe. Obtenez le résultat et évaluez graphiquement les hypothèses du modèle. Pour rappel, voici comment effectuer une régression linéaire.\n```{r}\nmodel &lt;- lm(Y ~  X1 + X2, data = data)\nsummary(model)\nplot(model)\n```\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Linear regression\n\nAnd here is a nice model with graphical output\n```{r}\n#| fig-cap: \"Checking assumptions of the model\"\nm1 &lt;- lm(bill_length_mm ~  flipper_length_mm + body_mass_g + sex, data = penguins)\nsummary(m1)\npar(mfrow= c(2,2))\nplot(m1)\n```\n\n\n\n8) Ajouter des références manuellement ou à l’aide de citr dans RStudio.\n\nChoisissez une publication récente du chercheur qui a partagé les données, le Dr Kristen Gorman. Importez cette publication dans votre gestionnaire de références favori (nous utilisons Zotero, sans complexe), et créez une référence bibtex que vous ajouterez au fichier mabiblio.bib.\nAjouter bibliography: mabiblio.bib au début de votre document R Markdown (YAML).\nCitez la référence dans le texte en la tapant manuellement ou en utilisant la commande citr. Pour utiliser citr installez-le d’abord ; si tout se passe bien, vous devriez le voir apparaître dans le menu déroulant Addins 💪. Il suffit ensuite d’utiliser Insert citations dans le menu déroulant Addins.\nCompiler.\n\n9) Changez le format de citation par défaut (style Chicago) en format The American Naturalist. Il se trouve ici https://www.zotero.org/styles. Pour ce faire, ajoutez csl: the-american-naturalist.csl dans le YAML.\n10) Créez votre rapport au format html, pdf et docx. 🎉\nExemple de résultats\nVous pouvez voir un exemple de la sortie fichier source Rmarkdown et sortie pdf\n\n\n\n\nHappy coding\n\n\n\n\n\n\n\nA. C. Davison, and D. V. Hinkley. 1997. Bootstrap methods and their\napplications. Cambridge University Press, Cambridge.\n\n\nAdler, D., S. T. Kelly, T. Elliott, and J. Adamson. 2024. vioplot: Violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey,\nA. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nAngelo Canty, and B. D. Ripley. 2024. boot:\nBootstrap r (s-plus) functions.\n\n\nBartoń, K. 2024. MuMIn:\nMulti-model inference.\n\n\nBates, D., M. Mächler, B. Bolker, and S. Walker. 2015. Fitting linear\nmixed-effects models using lme4. Journal\nof Statistical Software 67:1–48.\n\n\nChampely, S. 2020. pwr: Basic functions for power analysis.\n\n\nDouglas, A. 2023. An introduction to\nr.\n\n\nFox, J. 2003. Effect\ndisplays in R for generalised linear models. Journal of\nStatistical Software 8:1–27.\n\n\nFox, J., and J. Hong. 2009. Effect displays in\nR for multinomial and proportional-odds logit models:\nExtensions to the effects package.\nJournal of Statistical Software 32:1–24.\n\n\nFox, J., and S. Weisberg. 2018. Visualizing fit and lack of\nfit in complex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software 87:1–27.\n\n\nFox, J., and S. Weisberg. 2019a. An R companion to\napplied regression. Third. Sage, Thousand Oaks CA.\n\n\nFox, J., and S. Weisberg. 2019b. An\nr companion to applied regression. 3rd edition. Sage, Thousand Oaks\nCA.\n\n\nFriendly, M. 2023. vcdExtra: “vcd” extensions and additions.\n\n\nHorst, A. M., A. P. Hill, and K. B. Gorman. 2020. palmerpenguins: Palmer archipelago (antarctica)\npenguin data.\n\n\nHothorn, T., F. Bretz, and P. Westfall. 2008. Simultaneous inference in\ngeneral parametric models. Biometrical Journal 50:346–363.\n\n\nHvitfeldt, E. 2022. emoji: Data and function to work with emojis.\n\n\nIannone, R., J. Cheng, B. Schloerke, E. Hughes, A. Lauer, J. Seo, K.\nBrevoort, and O. Roy. 2024. gt: Easily create presentation-ready display\ntables.\n\n\nKassambara, A. 2023. ggpubr: “ggplot2” based publication ready plots.\n\n\nLüdecke, D., M. S. Ben-Shachar, I. Patil, P. Waggoner, and D. Makowski.\n2021. performance: An R package for\nassessment, comparison and testing of statistical models. Journal of\nOpen Source Software 6:3139.\n\n\nMartin, J. 1219. Another lasagna recipe from medieval times. Journal of\nLasagna 4:1686.\n\n\nMartin, J. 2200. A silly example. Chapman; Hall/CRC, Boca Raton,\nFlorida.\n\n\nMeyer, D., A. Zeileis, and K. Hornik. 2006. The strucplot framework:\nVisualizing multi-way contingency tables with vcd. Journal of\nStatistical Software 17:1–48.\n\n\nPedersen, T. L. 2024. patchwork: The composer of plots.\n\n\nPeng, R. D. 2024. simpleboot: Simple bootstrap routines.\n\n\nPrunello, M., and G. Mari. 2021. ggcleveland: Implementation of plots from\ncleveland’s visualizing data book.\n\n\nR Core Team. 2024. R:\nA language and environment for statistical computing. R Foundation\nfor Statistical Computing, Vienna, Austria.\n\n\nRodriguez-Sanchez, F., and C. P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen,\nA. Elberg, and J. Crowley. 2024. GGally:\nExtension to “ggplot2”.\n\n\nWheeler, B., and M. Torchiano. 2016. lmPerm: Permutation tests for linear models.\n\n\nWickham, H. 2007. Reshaping\ndata with the reshape package. Journal\nof Statistical Software 21:1–20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François,\nG. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E.\nMiller, S. M. Bache, K. Müller, J. Ooms, D. Robinson, D. P. Seidel, V.\nSpinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019.\nWelcome to the tidyverse. Journal of Open Source Software\n4:1686.\n\n\nWilkinson, L. 2005. The Grammar of Graphics.\nSpringer Science & Business Media.\n\n\nXie, Y. 2014. knitr: A comprehensive tool\nfor reproducible research in R. in V. Stodden, F.\nLeisch, and R. D. Peng, editors. Implementing reproducible computational\nresearch. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with\nR and knitr. 2nd edition. Chapman; Hall/CRC, Boca\nRaton, Florida.\n\n\nXie, Y. 2024. knitr: A general-purpose package for dynamic\nreport generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive\nguide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown\ncookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZeileis, A., and T. Hothorn. 2002. Diagnostic checking in\nregression relationships. R News 2:7–10.\n\n\nZeileis, A., D. Meyer, and K. Hornik. 2007. Residual-based shadings\nfor visualizing (conditional) independence. Journal of Computational\nand Graphical Statistics 16:507–525.\n\n\nZhu, H. 2024. kableExtra: Construct complex table with\n“kable” and pipe syntax.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rapports reproductibles avec Quarto</span>"
    ]
  },
  {
    "objectID": "07-github.html",
    "href": "07-github.html",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "",
    "text": "7.1 Qu’est-ce que le contrôle de version ?\nA Système de contrôle des versions (VCS) conserve un enregistrement de toutes les modifications que vous apportez aux fichiers qui composent un projet particulier et vous permet de revenir à des versions antérieures des fichiers si nécessaire. En d’autres termes, si vous vous trompez ou si vous perdez accidentellement des fichiers importants, vous pouvez facilement revenir à une étape antérieure de votre projet pour régler le problème. Le contrôle de version a été conçu à l’origine pour le développement collaboratif de logiciels, mais il est tout aussi utile pour la recherche scientifique et les collaborations (même s’il est vrai que la plupart des termes, du jargon et des fonctionnalités sont axés sur le développement de logiciels). Il existe actuellement de nombreux systèmes de contrôle de version différents, mais nous nous concentrerons sur l’utilisation de Git parce qu’il est gratuit et open source et qu’il s’intègre bien à RStudio. Cela signifie qu’il peut facilement faire partie de votre flux de travail habituel avec un minimum de frais supplémentaires.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#pourquoi-utiliser-le-contrôle-de-version",
    "href": "07-github.html#pourquoi-utiliser-le-contrôle-de-version",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.2 Pourquoi utiliser le contrôle de version ?",
    "text": "7.2 Pourquoi utiliser le contrôle de version ?\nPourquoi devriez-vous vous préoccuper du contrôle des versions ? Tout d’abord, il permet d’éviter cette situation (familière ?) lorsque vous travaillez sur un projet qui découle généralement de ce scénario (familier ?).\n\n\n\n\n\n\n\nFigure 7.1: Why you need version control (source: PhDComics)\n\n\n\n\nLe contrôle de version se charge automatiquement de conserver une trace de toutes les versions d’un fichier particulier et vous permet de revenir aux versions précédentes si nécessaire. Le contrôle de version vous aide également (en particulier le futur vous) à garder une trace de tous vos fichiers en un seul endroit et il aide les autres (en particulier les collaborateurs) à revoir, contribuer et réutiliser votre travail via le site web GitHub. Enfin, vos fichiers sont toujours disponibles de n’importe où et sur n’importe quel ordinateur, tout ce dont vous avez besoin, c’est d’une connexion internet.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#quest-ce-que-git-et-github",
    "href": "07-github.html#quest-ce-que-git-et-github",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.3 Qu’est-ce que Git et GitHub ?",
    "text": "7.3 Qu’est-ce que Git et GitHub ?\nGit est un système de contrôle de version développé à l’origine par Linus Torvalds qui permet de suivre les modifications apportées à un ensemble de fichiers. Ces fichiers peuvent être de n’importe quel type, y compris la ménagerie de fichiers qui composent généralement un projet axé sur les données (.pdf, .Rmd, .docx, .txt, .jpg, etc.), bien que les fichiers texte simples soient ceux qui fonctionnent le mieux. L’ensemble des fichiers qui composent un projet s’appelle un référentiel (ou simplement repo).\nGitHub est un service d’hébergement de dépôts Git basé sur le web qui vous permet de créer une copie distante de votre projet local contrôlé par version. Cette copie peut servir de sauvegarde ou d’archive de votre projet ou vous permettre, ainsi qu’à vos collègues, d’y accéder pour travailler en collaboration.\nAu début d’un projet, nous créons généralement (mais pas toujours) un dépôt Git. distant sur GitHub, puis cloner (il s’agit d’une copie) de ce dépôt dans notre base de données local local (celui qui se trouve devant vous). Ce clonage est généralement un événement unique et vous ne devriez pas avoir besoin de cloner ce dépôt à nouveau, à moins que vous ne fassiez vraiment n’importe quoi. Une fois que vous avez cloné votre dépôt, vous pouvez travailler localement sur votre projet comme d’habitude, en créant et en sauvegardant des fichiers pour votre analyse de données (scripts, documents R markdown, figures, etc.). En cours de route, vous pouvez prendre des instantanés (appelés commits) de ces fichiers après y avoir apporté des modifications importantes. Nous pouvons alors pousser ces modifications vers le dépôt GitHub distant pour en faire une sauvegarde ou les mettre à la disposition de nos collaborateurs. Si d’autres personnes travaillent sur le même projet (dépôt), ou si vous travaillez sur un autre ordinateur, vous pouvez tirer les modifications vers votre dépôt local afin que tout soit synchronisé.\n\n\n\n\n\n\n\nFigure 7.2: How git works",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-setup-git",
    "href": "07-github.html#sec-setup-git",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.4 Pour commencer",
    "text": "7.4 Pour commencer\nCe chapitre suppose que vous avez déjà installé les dernières versions de R et un IDE (RStudio ou VSCode). Si vous ne l’avez pas encore fait, vous trouverez des instructions dans Section 1.1.1.\n\n7.4.1 Installer Git\nPour commencer, vous devez d’abord installer Git. Si vous avez de la chance, vous avez peut-être déjà installé Git (surtout si vous avez un ordinateur Mac ou Linux). Vous pouvez vérifier si vous avez déjà installé Git en cliquant sur le bouton Terminal tab dans RStudio et en tapant git --version. Si vous voyez quelque chose qui ressemble à git version 2.25.0 (le numéro de version peut être différent sur votre ordinateur), alors vous avez déjà installé Git (jours heureux). Si vous obtenez une erreur (quelque chose comme git: command not found), cela signifie que Git n’est pas (encore) installé.\nVous pouvez également faire cette vérification en dehors de RStudio en ouvrant un terminal séparé si vous le souhaitez. Sous Windows, allez dans le menu “Démarrer” et dans la barre de recherche (ou la boîte d’exécution) tapez cmd et appuyez sur la touche Entrée. Sur Mac, allez dans “Applications” dans le Finder, cliquez sur le dossier “Utilitaires”, puis sur le programme “Terminal”. Sur une machine Linux, ouvrez simplement le Terminal (Ctrl+Alt+T fait souvent l’affaire).\nPour installer Git sur un Windows nous vous recommandons de télécharger et d’installer Git pour Windows (également connu sous le nom de “Git Bash”). Vous trouverez le fichier de téléchargement et les instructions d’installation ici .\nPour ceux qui utilisent un Mac nous vous recommandons de télécharger Git à partir de ici et de l’installer de la manière habituelle (double-cliquez sur le paquet d’installation une fois téléchargé). Si vous avez déjà installé Xcode sur votre Mac et que vous souhaitez utiliser une version plus récente de Git, vous devrez suivre quelques étapes supplémentaires documentées ici . Si vous n’avez jamais entendu parler de Xcode, ne vous inquiétez pas !\nPour ceux d’entre vous qui ont la chance de travailler sur un Linux vous pouvez simplement utiliser le gestionnaire de paquets de votre système d’exploitation pour installer Git à partir du dépôt officiel. Pour Ubuntu Linux (ou ses variantes), ouvrez votre Terminal et tapez\nsudo apt update\nsudo apt install git\nVous aurez besoin de privilèges administratifs pour effectuer cette opération. Pour les autres versions de Linux, voir ici pour de plus amples instructions d’installation.\nQuelle que soit la version de Git que vous installez, une fois l’installation terminée, vérifiez que le processus d’installation s’est bien déroulé en exécutant la commande git --version dans l’onglet Terminal de RStudio (comme décrit ci-dessus). Sur certaines installations de Git (oui, nous vous regardons MS Windows), cela peut encore produire une erreur car vous devrez également configurer RStudio pour qu’il puisse trouver l’exécutable Git (décrit dans Section 7.4.3).\n\n7.4.2 Configurer Git\nAprès avoir installé Git, vous devez le configurer pour pouvoir l’utiliser. Cliquez à nouveau sur l’onglet Terminal dans la fenêtre Console et tapez ce qui suit :\ngit config --global user.email 'you@youremail.com'\n\ngit config --global user.name 'Your Name'\nen remplaçant 'Your Name' votre nom réel et 'you@youremail.com' par votre adresse électronique. Nous vous recommandons d’utiliser l’adresse électronique de votre université (si vous en avez une), car vous l’utiliserez également lors de l’ouverture de votre compte GitHub (voir plus loin).\nSi vous avez réussi, vous ne devriez pas voir de message d’erreur dans ces commandes. Pour vérifier que vous avez configuré Git avec succès, tapez ce qui suit dans le Terminal\ngit config --global --list\nVous devriez voir vos deux user.name et user.email configurés.\n\n7.4.3 Configurer RStudio\nComme vous pouvez le voir ci-dessus, Git peut être utilisé à partir de la ligne de commande, mais il s’intègre également bien à RStudio, en fournissant une interface utilisateur graphique conviviale. Si vous souhaitez utiliser l’intégration Git de RStudio (nous vous le recommandons, au moins au début), vous devez vérifier que le chemin d’accès à l’exécutable Git est correctement spécifié. Dans RStudio, allez dans le menu Tools -&gt; Global Options -&gt; Git/SVN et assurez-vous que ‘Enable version control interface for RStudio projects’ est coché et que le chemin ‘Git executable:’ est correct pour votre installation. Si ce n’est pas le cas, cliquez sur le bouton Browse... et naviguez jusqu’à l’endroit où vous avez installé git et cliquez sur le fichier exécutable. Vous devrez redémarrer RStudio après cette opération.\n\n\n\n\n\n\n\nFigure 7.3: Providing path to git software in RStudio\n\n\n\n\n\n7.4.4 Configurer VSCode\n\npour développer\n\n\n7.4.5 Créer un compte GitHub\nSi tout ce que vous voulez, c’est garder une trace des fichiers et de leurs versions sur votre ordinateur local, alors Git est suffisant. En revanche, si vous souhaitez faire une copie hors site de votre projet ou le mettre à la disposition de vos collaborateurs, vous aurez besoin d’un service d’hébergement en ligne pour vos dépôts Git. C’est là que GitHub entre en jeu (il existe également d’autres services tels que GitLab , Bitbucket et Savannah ). Vous pouvez vous inscrire pour un compte gratuit sur GitHub ici . Vous devrez spécifier un nom d’utilisateur, une adresse email et un mot de passe fort. Nous vous suggérons d’utiliser l’adresse électronique de votre université (si vous en avez une), car elle vous permettra également de demander un compte gratuit d’éducateur ou de chercheur . gratuit pour les éducateurs ou les chercheurs ce qui vous permettra de bénéficier d’un certain nombre d’avantages avantages (ne vous en préoccupez pas pour l’instant). Quand il s’agit de choisir un nom d’utilisateur, nous vous suggérons d’y réfléchir. Choisissez un nom d’utilisateur court plutôt que long, utilisez des minuscules et des traits d’union si vous voulez inclure plusieurs mots, trouvez un moyen d’incorporer votre nom réel et, enfin, choisissez un nom d’utilisateur que vous vous sentirez à l’aise de révéler à votre futur employeur !\nCliquez ensuite sur “Sélectionnez un plan” (il se peut que vous deviez d’abord résoudre une énigme simple pour vérifier que vous êtes un être humain) et choisissez l’option “Plan gratuit”. Github vous enverra un courriel à l’adresse que vous avez fournie pour que vous puissiez vérifier.\nUne fois que vous avez terminé toutes ces étapes, vous devriez avoir installé Git et GitHub, prêts à l’emploi (enfin !).",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#mise-en-place-dun-projet",
    "href": "07-github.html#mise-en-place-dun-projet",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.5 Mise en place d’un projet",
    "text": "7.5 Mise en place d’un projet\n\n7.5.1 dans RStudio\nMaintenant que tout est prêt, créons notre premier projet RStudio à version contrôlée. Pour ce faire, il existe plusieurs approches différentes. Vous pouvez d’abord créer un dépôt GitHub distant, puis connecter un projet RStudio à ce dépôt (c’est ce que nous appellerons l’option 1). Une autre option consiste à créer d’abord un dépôt local, puis à lier un dépôt GitHub distant à ce dépôt (Option 2). Vous pouvez également connecter un projet existant à un dépôt GitHub, mais nous n’aborderons pas cette option ici. Si vous êtes totalement novice en matière de Git et de GitHub, nous vous suggérons d’utiliser l’option 1, car cette approche met en place votre dépôt Git local de manière satisfaisante et vous pouvez pousser et tirer immédiatement. L’option 2 nécessite un peu plus de travail et offre donc plus de possibilités de se tromper. Nous aborderons ces deux options ci-dessous.\n\n7.5.2 Option 1 - GitHub d’abord\nPour utiliser l’approche GitHub first, vous devez d’abord créer un fichier dépôt (repo) sur GitHub. Accédez à votre page GitHub et connectez-vous si nécessaire. Cliquez sur l’onglet “Dépôts” en haut et ensuite sur le bouton vert “Nouveau” à droite.\n\n\n\n\n\n\n\nFigure 7.4: Creating a new repository on Github\n\n\n\n\nDonnez un nom à votre nouveau dépôt (appelons-le first_repo pour ce chapitre), sélectionnez ‘Public’, cochez la case ‘Initialize this repository with a README’ (c’est important) et cliquez sur ‘Create repository’ (ignorez les autres options pour l’instant).\n\n\n\n\n\n\n\nFigure 7.5: Configuring a new repository on Github\n\n\n\n\nVotre nouveau dépôt GitHub est maintenant créé. Notez que le README a été rendu dans GitHub et qu’il est au format markdown (.md) (voir Chapitre 6 sur R markdown si cela ne vous dit rien). Cliquez ensuite sur le bouton vert “Cloner ou télécharger” et copiez le fichier https//... URL qui s’affiche pour plus tard (mettez tout en surbrillance et copiez ou cliquez sur l’icône de copie dans le presse-papiers à droite).\n\n\n\n\n\n\n\nFigure 7.6: Getting the cloning path for a directory on github\n\n\n\n\nOk, nous allons maintenant nous intéresser à RStudio. Dans RStudio, cliquez sur le bouton File -&gt; New Project le menu Dans la fenêtre qui s’ouvre, sélectionnez Version Control.\n\n\n\n\n\n\n\nFigure 7.7: Setting a new Github project in RStudio\n\n\n\n\nCollez maintenant l’URL que vous avez précédemment copiée de GitHub dans le champ Repository URL: dans la boîte de dialogue. Cela devrait remplir automatiquement le champ Project Directory Name: avec le nom correct du dépôt (il est important que le nom de ce répertoire soit le même que celui du dépôt que vous avez créé sur GitHub). Vous pouvez ensuite sélectionner l’endroit où vous souhaitez créer ce répertoire en cliquant sur le bouton Browse en face du bouton Create project as a subdirectory of: en face de l’option Naviguez jusqu’à l’endroit où vous souhaitez créer le répertoire et cliquez sur OK. Nous cochons également l’option Open in new session option.\n\n\n\n\n\n\n\n\nFigure 7.8\n\n\n\n\nRStudio va maintenant créer un nouveau répertoire portant le même nom que votre référentiel sur votre ordinateur local et va ensuite cloner votre référentiel distant dans ce répertoire. Le répertoire contiendra trois nouveaux fichiers ; first_repo.Rproj (ou quel que soit le nom que vous avez donné à votre référentiel), README.md et .gitignore. Vous pouvez vérifier cela dans le Files qui se trouve généralement dans le volet inférieur droit de RStudio. Vous disposerez également d’un Git dans le volet supérieur droit avec deux fichiers listés (nous y reviendrons plus tard dans le chapitre). C’est tout pour l’option 1, vous avez maintenant un dépôt GitHub distant qui est lié à votre dépôt local géré par RStudio. Toutes les modifications que vous apportez aux fichiers de ce répertoire seront contrôlées par Git.\n\n\n\n\n\n\n\nFigure 7.9\n\n\n\n\n\n7.5.3 Option 2 - RStudio d’abord\nUne autre approche consiste à créer d’abord un projet RStudio local, puis à établir un lien avec un dépôt Github distant. Comme nous l’avons mentionné précédemment, cette option est plus complexe que l’option 1. N’hésitez donc pas à sauter cette étape et à y revenir plus tard si vous êtes intéressé. Cette option est également utile si vous souhaitez simplement créer un projet RStudio local lié à un dépôt Git local (i.e. GitHub n’est pas impliqué). Dans ce cas, suivez les instructions ci-dessous en omettant la partie GitHub.\nDans RStudio, cliquez sur le bouton File -&gt; New Project et sélectionnez l’option New Directory option.\n\n\n\n\n\n\n\nFigure 7.10\n\n\n\n\nDans la fenêtre qui s’ouvre, sélectionnez l’option New Project l’option\n\n\n\n\n\n\n\nFigure 7.11\n\n\n\n\nDans la fenêtre Nouveau projet, spécifiez un Directory name (choisissez second_repo pour ce chapitre) et sélectionnez l’endroit où vous souhaitez créer ce répertoire sur votre ordinateur (cliquez sur le bouton Browse (cliquez sur le bouton ). Assurez-vous que le répertoire Create a git repository est cochée\n\n\n\n\n\n\n\nFigure 7.12\n\n\n\n\nCela créera un répertoire à version contrôlée appelé second_repo sur votre ordinateur qui contient deux fichiers, second_repo.Rproj et .gitignore (il peut également y avoir un fichier .Rhistory mais n’en tenez pas compte). Vous pouvez le vérifier en consultant le fichier Files dans RStudio (généralement dans le volet inférieur droit).\n\n\n\n\n\n\n\nFigure 7.13\n\n\n\n\nOK, avant de créer un dépôt sur GitHub, nous devons faire une dernière chose - nous devons placer notre fichier second_repo.Rproj et .gitignoresous contrôle de version. Malheureusement, nous n’avons pas encore abordé ce sujet en détail, alors suivez les quelques instructions suivantes (à l’aveugle !) et nous y reviendrons dans Section 7.6 de ce chapitre.\nPour placer nos deux fichiers sous contrôle de version, cliquez sur l’onglet “Git” qui se trouve généralement dans le panneau supérieur de RStudio.\n\n\n\n\n\n\n\nFigure 7.14\n\n\n\n\nVous pouvez voir que les deux fichiers sont listés. Ensuite, cochez les cases de la colonne “Staged” pour les deux fichiers et cliquez sur le bouton “Commit”.\n\n\n\n\n\n\n\nFigure 7.15\n\n\n\n\nVous accédez alors à la fenêtre “Examiner les modifications”. Saisissez le message de validation “Première validation” dans la fenêtre “Message de validation” et cliquez sur le bouton “Valider”. Une nouvelle fenêtre apparaît avec des messages que vous pouvez ignorer pour l’instant. Cliquez sur “Fermer” pour fermer cette fenêtre ainsi que la fenêtre “Examiner les modifications”. Les deux fichiers devraient maintenant avoir disparu du panneau Git dans RStudio, ce qui indique que la validation a été effectuée avec succès.\n\n\n\n\n\n\n\nFigure 7.16\n\n\n\n\nOK, ces deux fichiers sont maintenant sous contrôle de version. Nous devons maintenant créer un nouveau dépôt sur GitHub. Dans votre navigateur, allez sur votre page GitHub et connectez-vous si nécessaire. Cliquez sur l’onglet “Dépôts” et cliquez sur le bouton vert “Nouveau” à droite. Donnez à votre nouveau dépôt le nom second_repo (identique au nom de votre répertoire de contrôle de version) et sélectionnez “Public”. Cette fois-ci ne pas cocher la case ‘Initialize this repository with a README’ (c’est important) et cliquer sur ‘Create repository’.\n\n\n\n\n\n\n\nFigure 7.17\n\n\n\n\nCela vous amènera à une page de configuration rapide qui vous fournira du code pour différentes situations. Le code qui nous intéresse est celui qui se trouve sous ...or push an existing repository from the command line l’en-tête.\n\n\n\n\n\n\n\nFigure 7.18\n\n\n\n\nSurlignez et copiez la première ligne de code (note : la vôtre sera légèrement différente car elle inclura votre nom d’utilisateur GitHub et non le mien).\ngit remote add origin https://github.com/alexd106/second_repo.git\nPassez à RStudio, cliquez sur l’onglet “Terminal” et collez la commande dans le terminal. Retournez ensuite sur GitHub et copiez la deuxième ligne de code\ngit push -u origin master\net collez-la dans le terminal de RStudio. Vous devriez voir quelque chose comme ceci\n\n\n\n\n\n\n\nFigure 7.19\n\n\n\n\nSi vous jetez un coup d’œil à votre repo sur GitHub (cliquez sur l’icône /second_repo en haut de la page), vous verrez le second_repo.Rproj et .gitignore ont été remplacés par les fichiers poussés sur GitHub depuis votre dépôt local.\n\n\n\n\n\n\n\nFigure 7.20\n\n\n\n\nLa dernière chose à faire est de créer et d’ajouter un fichier README à votre dépôt. Un fichier README décrit votre projet et est écrit en utilisant le même langage Markdown que vous avez appris dans Chapitre 6. Un bon fichier README permet aux autres (ou au futur vous !) d’utiliser votre code et de reproduire votre projet. Vous pouvez créer un fichier README dans RStudio ou dans GitHub. Utilisons la seconde option.\nDans votre dépôt sur GitHub, cliquez sur le bouton vert Add a README vert.\n\n\n\n\n\n\n\nFigure 7.21\n\n\n\n\nRédigez maintenant une brève description de votre projet dans la rubrique &lt;&gt; Edit new file puis cliquez sur le bouton vert Commit new file vert.\n\n\n\n\n\n\n\nFigure 7.22\n\n\n\n\nVous devriez maintenant voir l’écran README.md dans votre référentiel. Il n’existera pas encore sur votre ordinateur car vous devrez tirer ces changements dans votre dépôt local, mais nous y reviendrons dans la section suivante.\nQue vous ayez suivi l’option 1 ou l’option 2 (ou les deux), vous avez maintenant configuré avec succès un projet RStudio à version contrôlée (et un répertoire associé) et l’avez lié à un dépôt GitHub. Git va maintenant surveiller ce répertoire pour toutes les modifications que vous apportez aux fichiers et aussi si vous ajoutez ou supprimez des fichiers. Si les étapes ci-dessus vous semblent un peu difficiles, rappelez-vous que vous n’avez à le faire qu’une seule fois pour chaque projet et que cela devient de plus en plus facile avec le temps.\n\n7.5.4 dans VSCode\n\npour développer",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use-git",
    "href": "07-github.html#sec-use-git",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.6 Utiliser Git avec RStudio",
    "text": "7.6 Utiliser Git avec RStudio\nMaintenant que nous avons mis en place notre projet et nos dépôts (locaux et distants), il est enfin temps d’apprendre à utiliser Git dans votre IDE !\nTypiquement, lorsque vous utilisez Git, votre flux de travail se déroule comme suit :\n\nVous créez/supprimez et modifiez les fichiers dans le répertoire de votre projet sur votre ordinateur comme d’habitude (en sauvegardant les modifications au fur et à mesure).\nUne fois que vous avez atteint un “point d’arrêt” naturel dans votre progression (c.-à-d. vous seriez triste si vous perdiez ce progrès), vous étape ces fichiers\nEnsuite, vous engager les modifications que vous avez apportées à ces fichiers mis en scène (avec un message de validation utile), ce qui crée un instantané permanent de ces modifications.\nVous continuez ce cycle jusqu’à ce que vous arriviez à un point où vous souhaitez pousser ces changements sur GitHub\nSi vous travaillez avec d’autres personnes sur le même projet, vous pouvez également avoir besoin de tirer leurs modifications sur votre ordinateur local\n\n\n\n\n\n\n\n\nFigure 7.23\n\n\n\n\nPrenons un exemple pour clarifier ce flux de travail.\nOuvrez le first_repo.Rproj que vous avez créé précédemment dans l’option 1. Utilisez soit l’outil File -&gt; Open Project ou cliquer sur l’icône de projet en haut à droite et sélectionner le projet approprié.\n\n\n\n\n\n\n\nFigure 7.24\n\n\n\n\nCréez un document R markdown à l’intérieur de ce projet en cliquant sur l’icône File -&gt; New File -&gt; R markdown menu (vous vous souvenez de Chapitre 6 ?).\nUne fois créé, nous pouvons supprimer tout le code R markdown de l’exemple (à l’exception de l’en-tête YAML) comme d’habitude et écrire du texte R markdown intéressant et inclure un tracé. Nous utiliserons la fonction cars pour ce faire. Enregistrez ce fichier (cmd + s pour Mac ou ctrl + s sous Windows). Votre document R markdown devrait ressembler à ce qui suit (ce n’est pas grave si ce n’est pas exactement la même chose).\n\n\n\n\n\n\n\nFigure 7.25\n\n\n\n\nJetez un coup d’œil à l’onglet ‘Git’ qui devrait contenir votre nouveau document R markdown (first_doc.Rmd dans cet exemple) ainsi que first_repo.Rproj et .gitignore (vous avez créé ces fichiers précédemment en suivant l’option 1).\n\n\n\n\n\n\n\nFigure 7.26\n\n\n\n\nEn suivant notre flux de travail, nous devons maintenant mettre en scène ces fichiers. Pour ce faire, cochez les cases de la colonne “Etagé” pour tous les fichiers. Notez qu’une icône d’état se trouve à côté de la case, ce qui vous donne une indication sur la façon dont les fichiers ont été modifiés. Dans notre cas, tous les fichiers doivent être ajoutés (A majuscule) car nous venons de les créer.\n\n\n\n\n\n\n\nFigure 7.27\n\n\n\n\nAprès avoir mis en scène les fichiers, l’étape suivante consiste à engager les fichiers. Pour ce faire, cliquez sur le bouton “Commit”.\n\n\n\n\n\n\n\nFigure 7.28\n\n\n\n\nAprès avoir cliqué sur le bouton “Valider”, vous accéderez à la fenêtre “Revoir les modifications”. Vous devriez voir les trois fichiers que vous avez mis en scène à l’étape précédente dans le panneau de gauche. Si vous cliquez sur le nom du fichier first_doc.Rmd vous verrez les modifications que vous avez apportées à ce fichier en surbrillance dans le volet inférieur. Le contenu que vous avez ajouté est surligné en vert et le contenu supprimé est surligné en rouge. Comme vous venez de créer ce fichier, tout le contenu est surligné en vert. Pour valider ces fichiers (prendre un instantané), saisissez d’abord un message de validation obligatoire dans le champ “Message de validation”. Ce message doit être relativement court et informatif (pour vous et vos collaborateurs) et indiquer pourquoi vous avez effectué les modifications, et non ce que vous avez modifié. Ceci est logique car Git garde une trace de ce que a changé et qu’il est donc préférable de ne pas utiliser les messages de livraison à cette fin. Il est traditionnel de saisir le message “First commit” (ou “Initial commit”) lorsque vous livrez des fichiers pour la première fois. Cliquez maintenant sur le bouton “Commit” pour valider ces modifications.\n\n\n\n\n\n\n\nFigure 7.29\n\n\n\n\nUn résumé de la validation que vous venez d’effectuer s’affiche. Cliquez ensuite sur le bouton “Fermer” pour revenir à la fenêtre “Revoir les modifications”. Notez que les fichiers mis à disposition ont été supprimés.\n\n\n\n\n\n\n\nFigure 7.30\n\n\n\n\nMaintenant que vous avez validé vos modifications, l’étape suivante consiste à pousser ces changements sur GitHub. Avant de pousser vos modifications, il est conseillé de commencer par tirer les modifications depuis GitHub. Ceci est particulièrement important si vous et vos collaborateurs travaillez sur les mêmes fichiers, car cela permet de garder votre copie locale à jour et d’éviter tout conflit potentiel. Dans ce cas, votre dépôt sera déjà à jour, mais c’est une bonne habitude à prendre. Pour ce faire, cliquez sur le bouton “Tirer” en haut à droite de la fenêtre “Examiner les modifications”. Une fois que vous avez retiré les modifications, cliquez sur le bouton vert “Pousser” pour transférer vos modifications. Vous verrez un résumé de l’opération que vous venez d’effectuer. Cliquez sur le bouton “Fermer” et fermez la fenêtre “Revoir les modifications”.\n\n\n\n\n\n\n\nFigure 7.31\n\n\n\n\nPour confirmer que les modifications que vous avez apportées au projet ont été transférées sur GitHub, ouvrez votre page GitHub, cliquez sur le lien Dépôts, puis sur l’icône first_repo dépôt. Vous devriez voir quatre fichiers listés, dont le fichier first_doc.Rmd que vous venez de pousser. À côté du nom du fichier, vous verrez votre dernier message de validation (“Première validation” dans ce cas) et la date de la dernière validation.\n\n\n\n\n\n\n\nFigure 7.32\n\n\n\n\nPour voir le contenu du fichier, cliquez sur le bouton first_doc.Rmd nom du fichier.\n\n\n\n\n\n\n\nFigure 7.33\n\n\n\n\n\n7.6.1 Suivi des modifications\nAprès avoir suivi les étapes décrites ci-dessus, vous aurez réussi à modifier un projet RStudio en créant un nouveau document R markdown, à mettre en scène puis à valider ces changements et enfin à pousser les changements vers votre dépôt GitHub. Maintenant, apportons d’autres modifications à votre fichier R markdown et suivons à nouveau le flux de travail, mais cette fois, nous verrons comment identifier les modifications apportées aux fichiers, examiner l’historique des livraisons et comment restaurer une version précédente du document.\nDans RStudio, ouvrez le fichier first_repo.Rproj que vous avez créé précédemment (s’il n’est pas déjà ouvert), puis ouvrez le fichier first_doc.Rmd (cliquez sur le nom du fichier dans la fenêtre Files dans RStudio).\nApportez quelques modifications à ce document. Supprimez la ligne commençant par “Ma première version contrôlée…” et remplacez-la par quelque chose de plus informatif (voir figure ci-dessous). Nous allons également changer les symboles de tracé en rouge et donner des étiquettes aux axes de tracé. Enfin, ajoutons un tableau récapitulatif du cadre de données à l’aide de la commande kable() et summary() (il se peut que vous ayez besoin d’installer le programme knitr si vous ne l’avez pas fait auparavant pour utiliser le paquet kable() ) et enfin rendre ce document au format pdf en changeant l’option YAML en output: pdf_document.\n\n\n\n\n\n\n\nFigure 7.34\n\n\n\n\nSauvegardez ces modifications, puis cliquez sur le bouton knit pour effectuer le rendu au format pdf. Un nouveau fichier pdf nommé first_doc.pdf sera créé et vous pourrez l’afficher en cliquant sur le nom du fichier dans la fenêtre Files dans RStudio.\nNotez que ces deux fichiers ont été ajoutés à la base de données Git dans RStudio. Les icônes d’état indiquent que le fichier first_doc.Rmd a été modifié (M majuscule) et que le fichier first_doc.pdf n’est pas suivi (point d’interrogation).\n\n\n\n\n\n\n\nFigure 7.35\n\n\n\n\nPour mettre en scène ces fichiers, cochez la case “Mis en scène” pour chaque fichier et cliquez sur le bouton “Valider” pour accéder à la fenêtre “Examiner les modifications”.\n\n\n\n\n\n\n\nFigure 7.36\n\n\n\n\nAvant de valider vos modifications, notez l’état des fichiers first_doc.pdf est passé de non suivi à ajouté (A). Vous pouvez consulter les modifications que vous avez apportées à l’élément first_doc.Rmd en cliquant sur le nom du fichier dans le volet supérieur gauche, ce qui vous donnera un résumé utile des modifications dans le volet inférieur (techniquement appelé diffs). Les lignes qui ont été supprimées sont surlignées en rouge et les lignes qui ont été ajoutées sont surlignées en vert (notez que du point de vue de Git, une modification de ligne est en fait deux opérations : la suppression de la ligne d’origine suivie de la création d’une nouvelle ligne). Une fois que vous êtes satisfait, validez ces modifications en rédigeant un message de validation approprié et cliquez sur le bouton “Valider”.\n\n\n\n\n\n\n\nFigure 7.37\n\n\n\n\nPour transférer les modifications sur GitHub, cliquez d’abord sur le bouton “Pull” (rappelez-vous qu’il s’agit d’une bonne pratique, même si vous ne collaborez qu’avec vous-même pour l’instant), puis cliquez sur le bouton “Push”. Accédez à votre dépôt GitHub en ligne et vous verrez vos nouveaux commits, y compris le bouton first_doc.pdf que vous avez créé lorsque vous avez rendu votre document R markdown.\n\n\n\n\n\n\n\nFigure 7.38\n\n\n\n\nPour voir les changements dans first_doc.Rmd cliquez sur le nom de ce fichier.\n\n\n\n\n\n\n\nFigure 7.39\n\n\n\n\n\n7.6.2 Historique des engagements\nL’un des avantages de Git et de GitHub est que vous pouvez consulter l’historique de tous les commits que vous avez effectués, ainsi que les messages de commits associés. Vous pouvez le faire localement en utilisant RStudio (ou la ligne de commande Git) ou si vous avez poussé vos commits sur GitHub, vous pouvez les consulter sur le site web de GitHub.\nPour consulter l’historique des livraisons dans RStudio, cliquez sur le bouton ” History ” (celui qui ressemble à une horloge) dans le volet Git pour afficher l’historique dans la fenêtre ” Review Changes “. Vous pouvez également cliquer sur les boutons”Commit” ou “Diff” pour accéder à la même fenêtre (il vous suffit de cliquer en plus sur le bouton “History” dans la fenêtre “Review Changes”).\n\n\n\n\n\n\n\nFigure 7.40\n\n\n\n\nLa fenêtre d’historique est divisée en deux parties. Le volet supérieur répertorie toutes les livraisons que vous avez effectuées dans ce dépôt (avec les messages de livraison associés), en commençant par la plus récente en haut et la plus ancienne en bas. Vous pouvez cliquer sur chacune de ces livraisons et le volet inférieur vous montre les modifications que vous avez apportées ainsi qu’un résumé de l’historique. Date à laquelle la validation a été effectuée, l’auteur du commit et le message du commit (Sujet). Il existe également un identifiant unique pour l’engagement (SHA - Secure Hash Algorithm) et un identifiant Parent qui identifie la livraison précédente. Ces identifiants SHA sont très importants car vous pouvez les utiliser pour visualiser et revenir à des versions antérieures de fichiers (détails ci-dessous Section 7.6.3). Vous pouvez également consulter le contenu de chaque fichier en cliquant sur le lien “Voir le fichier @ clé SHA” (dans notre cas, “Voir le fichier @ 2b4693d1”).\n\n\n\n\n\n\n\nFigure 7.41\n\n\n\n\nVous pouvez également consulter l’historique de vos commits sur le site GitHub, mais cette consultation sera limitée aux commits que vous avez déjà transférés sur GitHub. Pour consulter l’historique des livraisons, accédez au dépôt et cliquez sur le lien “livraisons” (dans notre cas, le lien sera intitulé “3 livraisons” car nous avons effectué 3 livraisons).\n\n\n\n\n\n\n\nFigure 7.42\n\n\n\n\nVous verrez une liste de tous les commits que vous avez faits, avec les messages de commit, la date du commit et l’identifiant SHA (ce sont les mêmes identifiants SHA que vous avez vus dans l’historique de RStudio). Vous pouvez même parcourir le dépôt à un moment donné en cliquant sur le bouton &lt;&gt; lien. Pour visualiser les modifications apportées aux fichiers associés au commit, il suffit de cliquer sur le lien du commit concerné dans la liste.\n\n\n\n\n\n\n\nFigure 7.43\n\n\n\n\nLes modifications seront affichées selon le format habituel : vert pour les ajouts et rouge pour les suppressions.\n\n\n\n\n\n\n\nFigure 7.44\n\n\n\n\n\n7.6.3 Annulation des modifications\nL’un des avantages de l’utilisation de Git est la possibilité de revenir à des versions antérieures des fichiers si vous avez fait une erreur, si vous avez cassé quelque chose ou si vous préférez simplement une approche plus ancienne. La façon de procéder dépend du fait que les modifications que vous souhaitez supprimer ont été mises à disposition, validées ou poussées sur GitHub. Nous allons passer en revue quelques scénarios courants ci-dessous, en utilisant principalement RStudio, mais nous aurons parfois besoin d’utiliser le Terminal (toujours dans RStudio cependant).\nModifications sauvegardées mais non mises à jour, validées ou poussées\nSi vous avez enregistré des modifications dans votre ou vos fichiers mais que vous ne les avez pas mis en page, livrés ou poussés sur GitHub, vous pouvez cliquer avec le bouton droit de la souris sur le fichier incriminé dans le panneau Git et sélectionner “Revert …” (revenir en arrière). Cela ramènera toutes les modifications que vous avez faites au même état que votre dernier commit. Sachez qu’il n’est pas possible d’annuler cette opération, alors utilisez-la avec précaution.\n\n\n\n\n\n\n\nFigure 7.45\n\n\n\n\nVous pouvez également annuler les modifications apportées à une partie seulement d’un fichier en ouvrant la fenêtre “Diff” (cliquez sur le bouton “Diff” dans le panneau Git). Sélectionnez la ligne que vous souhaitez annuler en double-cliquant dessus, puis cliquez sur le bouton “Annuler la ligne”. De la même manière, vous pouvez supprimer des morceaux de code en cliquant sur le bouton “Supprimer le morceau”.\n\n\n\n\n\n\n\nFigure 7.46\n\n\n\n\nStagé mais non validé et non poussé\nSi vous avez mis en scène vos fichiers, mais que vous ne les avez pas validés, décochez-les simplement en cliquant sur la case “Mis en scène” dans le panneau Git (ou dans la fenêtre “Examiner les modifications”) pour supprimer la coche. Vous pouvez alors revenir sur tout ou partie du fichier comme décrit dans la section ci-dessus.\nStagé et validé mais non poussé\nSi vous avez fait une erreur ou avez oublié d’inclure un fichier dans votre dernier commit que vous n’avez pas encore poussé sur GitHub, vous pouvez simplement corriger votre erreur, enregistrer vos modifications, puis modifier votre précédent commit. Vous pouvez le faire en mettant en scène votre fichier, puis en cochant la case “Modifier la livraison précédente” dans la fenêtre “Examiner les modifications” avant de livrer.\n\n\n\n\n\n\n\nFigure 7.47\n\n\n\n\nSi nous consultons l’historique des livraisons, nous pouvons voir que notre dernière livraison contient les deux modifications apportées au fichier plutôt que deux livraisons distinctes. Nous utilisons souvent l’approche “amend commit”, mais il est important de comprendre que vous devez ne pas faire si vous avez déjà poussé votre dernier commit sur GitHub car vous réécrivez l’histoire et toutes sortes de mauvaises choses peuvent arriver !\n\n\n\n\n\n\n\nFigure 7.48\n\n\n\n\nSi vous repérez une erreur qui s’est produite plusieurs fois ou si vous souhaitez simplement revenir à une version précédente d’un document, plusieurs options s’offrent à vous.\nOption 1 - (probablement la plus simple mais très peu Git - mais bon, peu importe !) est de regarder dans votre historique de commit dans RStudio, de trouver le commit sur lequel vous souhaitez revenir et de cliquer sur le bouton ‘View file @’ pour afficher le contenu du fichier.\n\n\n\n\n\n\n\nFigure 7.49\n\n\n\n\nVous pouvez alors copier le contenu du fichier dans le presse-papiers et le coller dans votre fichier actuel pour remplacer le code ou le texte défectueux. Vous pouvez également cliquer sur le bouton “Enregistrer sous” et enregistrer le fichier sous un autre nom. Une fois que vous avez enregistré votre nouveau fichier, vous pouvez supprimer votre fichier indésirable actuel et continuer à travailler sur votre nouveau fichier. N’oubliez pas de mettre en scène et de valider ce nouveau fichier.\n\n\n\n\n\n\n\nFigure 7.50\n\n\n\n\nOption 2 - (Git like) Allez dans votre historique Git, trouvez le commit sur lequel vous souhaitez revenir et notez (ou copiez) son identifiant SHA.\n\n\n\n\n\n\n\nFigure 7.51\n\n\n\n\nAllez maintenant au Terminal dans RStudio et tapez git checkout &lt;SHA&gt; &lt;filename&gt;. Dans notre cas, la clé SHA est 2b4693d1 et le nom du fichier est first_doc.Rmd notre commande ressemblerait donc à ceci :\ngit checkout 2b4693d1 first_doc.Rmd\nLa commande ci-dessus copiera la version du fichier sélectionné dans le passé et la placera dans le présent. RStudio peut vous demander si vous souhaitez recharger le fichier tel qu’il a été modifié - sélectionnez oui. Vous devrez également mettre en scène et valider le fichier comme d’habitude.\nSi vous souhaitez ramener tous vos fichiers au même état qu’une livraison précédente plutôt qu’un seul fichier, vous pouvez utiliser (le seul “point . est important, sinon votre HEAD se détachera) :\ngit rm -r .\ngit checkout 2b4693d1 .\n\nNotez que cela supprimera tous les fichiers que vous avez créés depuis que vous avez effectué ce commit, alors soyez prudent !\nMise en scène, livrée et poussée\nSi vous avez déjà transféré vos modifications sur GitHub, vous pouvez utiliser la commande git checkout décrite ci-dessus, puis commiter et pousser pour mettre à jour GitHub (bien que cela ne soit pas vraiment considéré comme une “meilleure” pratique). Une autre approche serait d’utiliser git revert (Note : pour autant que nous puissions en juger git revert n’est pas la même chose que l’option ‘Revert’ dans RStudio). L’option revert dans Git crée essentiellement un nouveau commit basé sur un commit précédent et préserve donc tout l’historique des commits. Pour revenir à un état antérieur (commit), vous devez d’abord identifier la ZSD du commit auquel vous souhaitez revenir (comme nous l’avons fait ci-dessus), puis utiliser la commande revert dans le terminal. Supposons que nous voulions revenir à notre “Premier commit” qui a un identifiant SHA d27e79f1.\n\n\n\n\n\n\n\nFigure 7.52\n\n\n\n\nNous pouvons utiliser le revert comme indiqué ci-dessous dans le terminal. La commande --no-commit est utilisée pour éviter d’avoir à gérer chaque livraison intermédiaire.\ngit revert --no-commit d27e79f1..HEAD\nVotre first_doc.Rmd va maintenant revenir au même état que celui dans lequel il se trouvait lorsque vous avez effectué votre “premier commit”. Notez également que l’élément first_doc.pdf a été supprimé, car il n’était pas présent lorsque nous avons effectué notre première livraison. Vous pouvez maintenant mettre en scène et livrer ces fichiers avec un nouveau message de livraison et enfin les pousser sur GitHub. Remarquez que si nous regardons notre historique des livraisons, toutes les livraisons que nous avons faites sont toujours présentes.\n\n\n\n\n\n\n\nFigure 7.53\n\n\n\n\net notre repo sur GitHub reflète également ces changements\n\n\n\n\n\n\n\nFigure 7.54",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use-git_vsc",
    "href": "07-github.html#sec-use-git_vsc",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.7 Utiliser Git avec VSCode",
    "text": "7.7 Utiliser Git avec VSCode\nMaintenant que nous avons mis en place notre projet et nos dépôts (locaux et distants), il est enfin temps d’apprendre à utiliser Git dans VSCode !\nTypiquement, lorsque vous utilisez Git, votre flux de travail se déroule comme suit :\n\nVous créez/supprimez et modifiez les fichiers dans le répertoire de votre projet sur votre ordinateur comme d’habitude (en sauvegardant les modifications au fur et à mesure).\nUne fois que vous avez atteint un “point d’arrêt” naturel dans votre progression (c.-à-d. vous seriez triste si vous perdiez ce progrès), vous étape ces fichiers\nEnsuite, vous engager les modifications que vous avez apportées à ces fichiers mis en scène (avec un message de validation utile), ce qui crée un instantané permanent de ces modifications.\nVous continuez ce cycle jusqu’à ce que vous arriviez à un point où vous souhaitez pousser ces changements sur GitHub\nSi vous travaillez avec d’autres personnes sur le même projet, vous pouvez également avoir besoin de tirer leurs modifications sur votre ordinateur local\n\n\n\n\n\n\n\n\nFigure 7.55\n\n\n\n\nPrenons un exemple pour clarifier ce flux de travail.\nSuivi des modifications\nHistorique des modifications\nAnnulation des modifications",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-collab",
    "href": "07-github.html#sec-collab",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.8 Collaborer avec Git",
    "text": "7.8 Collaborer avec Git\nGitHub est un excellent outil de collaboration. Il peut sembler effrayant et compliqué au début, mais cela vaut la peine d’investir un peu de temps pour apprendre comment il fonctionne. Ce qui rend GitHub si bon pour la collaboration, c’est qu’il s’agit d’un système de système distribué Cela signifie que chaque collaborateur travaille sur sa propre copie du projet et que les modifications sont ensuite fusionnées dans le dépôt distant. Il existe deux façons principales de mettre en place un projet collaboratif sur GitHub. L’une est le flux de travail que nous avons décrit ci-dessus, où chacun connecte son dépôt local au même dépôt distant ; ce système fonctionne bien avec les petits projets où différentes personnes travaillent principalement sur différents aspects du projet, mais il peut rapidement devenir lourd si de nombreuses personnes collaborent et travaillent sur les mêmes fichiers (misère de la fusion !). La seconde approche consiste à ce que chaque collaborateur crée une copie (ou fork) du dépôt principal, qui devient leur dépôt distant. Chaque collaborateur doit alors envoyer une demande (une demande d’extraction) au propriétaire du référentiel principal afin d’incorporer les modifications dans le référentiel principal, ce qui inclut un processus de révision avant l’intégration des modifications. Plus de détails sur ces sujets peuvent être trouvés dans Section 7.10.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#conseils-git",
    "href": "07-github.html#conseils-git",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.9 Conseils Git",
    "text": "7.9 Conseils Git\nD’une manière générale, vous devriez commiter souvent (y compris les commits modifiés) mais pousser beaucoup moins souvent. Cela facilite la collaboration et rend le processus de retour aux versions précédentes des documents beaucoup plus simple. En général, nous n’envoyons des modifications sur GitHub que lorsque nous sommes satisfaits que nos collaborateurs (ou le reste du monde) puissent voir notre travail. Cependant, cela dépend entièrement de vous, du projet (et des personnes avec lesquelles vous travaillez) et de vos priorités dans l’utilisation de Git.\nSi vous ne voulez pas suivre un fichier dans votre dépôt (peut-être s’agit-il de fichiers trop volumineux ou transitoires), vous pouvez faire en sorte que Git ignore le fichier en l’ajoutant à la balise .gitignore pour qu’il soit ignoré par Git. Sur RStudio, dans le panneau git, vous pouvez faire un clic droit sur le nom du fichier à exclure et sélectionner ‘Ignore…’\n\n\n\n\n\n\n\nFigure 7.56\n\n\n\n\nCela ajoutera le nom du fichier à la base de données .gitignore fichier. Si vous souhaitez ignorer plusieurs fichiers ou un type de fichier particulier, vous pouvez également inclure des caractères génériques dans la commande .gitignore dans le fichier. Par exemple, pour ignorer tous les fichiers png vous pouvez inclure l’expression *.png dans votre .gitignore et enregistrer.\nSi tout va de travers et que vous finissez par détruire complètement votre dépôt Git, ne désespérez pas (nous sommes tous passés par là !). Tant que votre dépôt GitHub est bon, tout ce que vous avez à faire est de supprimer le répertoire du projet en question sur votre ordinateur, de créer un nouveau projet RStudio et de le lier à votre dépôt GitHub distant en utilisant l’option 2 (-Section 7.5.3). Une fois que vous avez cloné le dépôt distant, vous devriez être prêt à partir.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-resources",
    "href": "07-github.html#sec-resources",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.10 Autres ressources",
    "text": "7.10 Autres ressources\nIl existe de nombreux guides en ligne pour en savoir plus sur git et GitHub et, comme pour tout logiciel open source, il existe une vaste communauté qui peut être d’une grande aide :\n\nLe guide de la British Ecological Society sur Code reproductible\nLes guides guides GitHub\nLe laboratoire scientifique de Mozilla Guide GitHub pour la collaboration sur les projets ouverts\nJenny Bryan’s Joyeux Git et GitHub . Nous avons emprunté l’idée (mais avec un contenu différent) de RStudio d’abord, RStudio ensuite dans la section “Mise en place d’un projet à version contrôlée dans RStudio”.\nL’article de Melanie Frazier GitHub : Un guide du débutant pour remonter le temps (et réparer les erreurs) . Nous avons suivi cette structure (avec des modifications et un contenu différent) dans la section “Revenir sur les modifications”.\nSi vous avez fait quelque chose de terriblement mauvais et que vous ne savez pas comment le réparer, essayez Oh Shit, Git ou si vous êtes facilement offensé Dangit, Git\n\nCe ne sont que quelques exemples, il vous suffit de faire une recherche sur “contrôle de version avec git et GitHub” pour voir à quel point la communauté autour de ces projets open source est importante et combien de ressources gratuites sont disponibles pour que vous deveniez un expert en contrôle de version.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#git_practical",
    "href": "07-github.html#git_practical",
    "title": "7  Contrôle de version avec Git et GitHub",
    "section": "\n7.11 Pratique",
    "text": "7.11 Pratique\n\n7.11.1 Contexte\nNous allons configurer Rstudio pour qu’il fonctionne avec notre compte github, puis nous créerons un nouveau projet et commencerons à utiliser github. Pour avoir des données, je suggère d’utiliser l’outil génial palmerpenguins dataset 🐧.\n\n7.11.2 Informations sur les données\nCes données ont été collectées et partagées par Dr. Kristen Gorman et Station Palmer, Antarctique LTER.\nL’ensemble a été conçu par les Drs Allison Horst et Alison Hill. site officiel.\nLe paquet palmerpenguins comporte deux ensembles de données.\n\nlibrary(palmerpenguins)\n\nL’ensemble de données penguins est une version simplifiée des données brutes ; voir ?penguins pour plus d’informations :\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nL’autre jeu de données penguins_raw contient les données brutes ; voir ?penguins_raw pour plus d’informations :\n\nhead(penguins_raw)\n\n# A tibble: 6 × 17\n  studyName `Sample Number` Species          Region Island Stage `Individual ID`\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          \n1 PAL0708                 1 Adelie Penguin … Anvers Torge… Adul… N1A1           \n2 PAL0708                 2 Adelie Penguin … Anvers Torge… Adul… N1A2           \n3 PAL0708                 3 Adelie Penguin … Anvers Torge… Adul… N2A1           \n4 PAL0708                 4 Adelie Penguin … Anvers Torge… Adul… N2A2           \n5 PAL0708                 5 Adelie Penguin … Anvers Torge… Adul… N3A1           \n6 PAL0708                 6 Adelie Penguin … Anvers Torge… Adul… N3A2           \n# ℹ 10 more variables: `Clutch Completion` &lt;chr&gt;, `Date Egg` &lt;date&gt;,\n#   `Culmen Length (mm)` &lt;dbl&gt;, `Culmen Depth (mm)` &lt;dbl&gt;,\n#   `Flipper Length (mm)` &lt;dbl&gt;, `Body Mass (g)` &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   `Delta 15 N (o/oo)` &lt;dbl&gt;, `Delta 13 C (o/oo)` &lt;dbl&gt;, Comments &lt;chr&gt;\n\n\nPour cet exercice, nous allons utiliser l’outil penguins jeu de données.\n\n7.11.3 Questions\n1) Créer un compte github si ce n’est pas encore fait.\n2) Configurez Rstudio avec votre compte github en utilisant l’option usethis paquet.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nusethis::git_sitrep()\nusethis::use_git_config(\n  user.name = \"your_username\",\n  user.email = \"your_email@address.com\"\n)\n\n\n\n\n3) Créez et stockez votre jeton d’autorisation personnel GITHUB\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nusethis::create_github_token()\ngitcreds::gitcreds_set()\n\n\n\n\n4) Créer un nouveau projet R Markdown, l’initialiser pour git, et créer un nouveau dépôt git\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#create R project\nusethis::use_git()\n\n#restart R\nusethis::use_github()\nusethis::git_vaccinate()\n\n\n\n\n5) Créez un nouveau document Rmarkdown dans votre projet. Sauvegardez ensuite le fichier et mettez-le en scène.\n6) Créer un nouveau commit incluant le nouveau fichier et le pousser sur github (Vérifier sur github que cela fonctionne).\n7) Modifiez le fichier. Supprimez tout ce qui se trouve après la ligne 12. Ajoutez un nouveau titre de section, un texte simple et un texte en caractères gras. Puis tricotez et compilez.\n8) Faire un nouveau commit (avec un message significatif), et pousser sur github.\n9) Créez une nouvelle branche, et ajoutez une nouvelle section au fichier rmarkdown dans cette branche. Ce que vous voulez. Je suggérerais un graphique des données.\n10) Créer un commit et le pousser sur la branche.\n11) Sur github, créer une pull request pour fusionner les 2 branches différentes.\n12) Vérifier et accepter la pull request pour fusionner les 2 branches.\nVous avez utilisé avec succès tous les outils essentiels de git 🎉 . Vous êtes prêt à explorer 🕵️ et découvrir sa puissance 💪\n\n\n\n\nHappy git(hub)-ing\n\n\n\n\n7.11.4 Solution\n2)\n\nusethis::git_sitrep()\nusethis::use_git_config(\n  user.name = \"your_username\",\n  user.email = \"your_email@address.com\"\n)\n\n3)\n\nusethis::create_github_token()\ngitcreds::gitcreds_set()\n\n4)\n\n#create R project\nusethis::use_git()\n\n#restart R\nusethis::use_github()\nusethis::git_vaccinate()",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrôle de version avec Git et GitHub</span>"
    ]
  },
  {
    "objectID": "25-puissance.html",
    "href": "25-puissance.html",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "",
    "text": "8.1 La théorie",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-puissance.html#la-théorie",
    "href": "25-puissance.html#la-théorie",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "",
    "text": "8.1.1 Qu’est-ce que la puissance?\nLa puissance est la probabilité de rejeter l’hypothèse nulle quand elle est fausse.\n\n8.1.2 Pourquoi faire une analyse de puissance?\nÉvaluer l’évidence\nL’analyse de puissance effectuée après avoir accepté une hypothèse nulle permet de calculer la probabilité que l’hypothèse nulle soit rejetée si elle était fausse et que la taille d’effet était d’une valeur donnée. Ce type d’analyse a posteriori est très commun.\nPlanifier de meilleures expériences\nL’analyse de puissance effectuée avant de réaliser une expérience (le plus souvent après une expérience préliminaire cependant), permet de déterminer le nombre d’observations nécessaires pour détecter un effet d’une taille donnée à un niveau fixe de probabilité (la puissance). Ce type d’analyse a priori devrait être réalisé plus souvent.\nEstimer la “limite de détection” statistique\nL’effort d’échantillonnage est souvent déterminé à l’avance (par exemple lorsque vous héritez de données récoltées par quelqu’un d’autre), ou très sévèrement limité (lorsque les contraintes logistiques prévalent). Que ce soit a priori ou a posteriori l’analyse de puissance vous permet d’estimer, pour un effort d’échantillonnage donné et un niveau de puissance fixe, quelle est la taille minimale de l’effet qui peut être détecté (comme étant statistiquement significatif).\n\n8.1.3 Facteurs qui affectent la puissance\nIl y a 3 facteurs qui affectent la puissance d’un test statistique.\nLe critère de décision\nLa puissance dépend de \\(\\alpha\\), le seuil de probabilité auquel on rejette l’hypothèse nulle. Si ce seuil est très strict (i.e. si \\(\\alpha\\) est fixé à une valeur très basse, comme 0.1% ou p = 0.001), alors la puissance sera plus faible que si le seuil était moins strict.\nLa taille de l’échantillon\nPlus l’échantillon est grand, plus la puissance est élevée. La capacité d’un test à détecter de petites différences comme étant statistiquement significatives augmente avec une augmentation du nombre d’observations.\nLa taille d’effet\nPlus la taille d’effet est grande, plus un test a de puissance. Pour un échantillon de taille fixe, la capacité d’un test à détecter un effet comme étant statistiquement significatif est plus élevée si l’effet est grand que s’il est petit. La taille d’effet est en fait une mesure du degré de fausseté de l’hypothèse nulle.",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-puissance.html#quest-ce-que-gpower",
    "href": "25-puissance.html#quest-ce-que-gpower",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "\n8.2 Qu’est ce que G*Power?",
    "text": "8.2 Qu’est ce que G*Power?\nG*Power est un programme gratuit, développé par des psychologues de l’Université de Dusseldorf en Allemagne. Le programme existe en version Mac et Windows. Il peut cependant être utilisé sous linux via Wine ou une machine virtuelle.\nG*Power vous permettra d’effectuer une analyse de puissance pour la majorité des tests que nous verrons au cours de la session sans avoir à effectuer des calculs complexes ou farfouiller dans des tableaux ou des figures décrivant des distributions ou des courbes de puissance. G*power est vraiment un outil très utile que vous devrez maîtriser.\nIl est possible de faire toutes les analyses de G*power avec R, mais cela est un peu plus complexes et nécessite un peu plus de code et donc une meilleure compréhension du processus. Dans les cas les plus simple le code R est aussi fourni.\n\n\n\n\n\n\nExercice\n\n\n\nTéléchargez le programme ici et installez-le sur votre ordi et votre station de travail au laboratoire (si ce n’est déjà fait).",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-puissance.html#comment-utiliser-gpower",
    "href": "25-puissance.html#comment-utiliser-gpower",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "\n8.3 Comment utiliser G*Power",
    "text": "8.3 Comment utiliser G*Power\n\n8.3.1 Principe général\nL’utilisation de G*Power implique généralement 3 étapes:\n\nChoisir le test approprié\nChoisir l’un des 5 types d’analyses de puissance disponibles\nInscrire les valeurs des paramètres requis et cliquer sur Calculate\n\n\n8.3.2 Types d’analyses de puissance disponibles\nA priori\nCalcule l’effectif requis pour une valeur de \\(\\alpha\\), \\(\\beta\\) et de taille d’effet donnée. Ce type d’analyse est utile à l’étape de planification des expériences.\nCompromis\nCalcule \\(\\boldsymbol{\\alpha}\\) et \\(\\boldsymbol{\\beta}\\) pour un rapport \\(\\beta / \\alpha\\) donné, un effectif fixe, et une taille d’effet donnée. Ce type d’analyse est plus rarement utilisé (je ne l’ai jamais fait), mais peut être utile lorsque le rapport \\(\\beta / \\alpha\\) est d’intérêt, par exemple lorsque le coût d’une erreur de type I et de type II peut être quantifié.\nCritère\nCalcule \\(\\boldsymbol{\\alpha}\\) pour \\(\\beta\\), effectif et taille d’effet donné. En pratique, je vois peu d’utilité pour ce type de calcul. Contactez-moi si vous en voyez une!\nPost-hoc\nCalcule la puissance (1 - \\(\\boldsymbol{\\beta}\\)) pour \\(\\alpha\\), une taille d’effet et un effectif donné. Très utilisée pour interpréter les résultats d’une analyse statistique non-significative, mais seulement si l’on utilise une taille d’effet biologiquement significative (et non la taille d’effet observée). Peu pertinente lorsque le test est significatif.\nSensitivité\nCalcule la taille d’effet détectable pour une valeur d’\\(\\alpha\\), \\(\\beta\\) et un effectif donné. Très utile également au stade de planification des expériences.\n\n8.3.3 Comment calculer la taille d’effet ?\nG*Power permet de faire une analyse de puissance pour de nombreux tests statistiques.\nL’indice de la taille d’effet qui est utilisé par G*Power pour les calculs dépend du test. Notez que d’autres logiciels peuvent utiliser des indices différents et il est important de vérifier que l’indice que l’on utilise est celui qui convient. G*Power vous facilite la tâche et permet de calculer la taille d’effet en inscrivant seulement les valeurs pertinentes dans la fenêtre de calcul. Le tableau suivant donne les indices utilisés par G*Power pour les différents tests.\n\n\n\n\n\n\n\nTest\nTaille d’effet\nFormule\n\n\n\ntest de t sur des moyennes\nd\n\\(d = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{({s_1}^2 + {s_2}^2)/2}}\\)\n\n\ntest de t pour des corrélations\nr\n\n\n\nautres tests de t\nd\n\\(d = \\frac{\\mu}{\\sigma}\\)\n\n\ntest F (ANOVA)\nf\n\\(f = \\frac{\\frac{\\sqrt{\\sum_{i=1}^k (\\mu_i - \\mu)^2}}{k}}{\\sigma}\\)\n\n\nautres test F\n\\(f^2\\)\n\\(f^2 = \\frac{{R_p}^2}{1-{R_p}^2}\\)\n\n\n\n\n\n\\({R_p}\\) est le coefficient de corrélation partiel\n\n\ntest Chi-carré\nw\n\\(w = \\sqrt{ \\sum_{i=1}^m \\frac{(p_{0i} - p_{1i})^2 }{ p_{0i}} }\\)\n\n\n\n\n\n\\(p_{0i}\\) \\(p_{1i}\\) sont les proportions de la catégorie i prédites par l’hypothèse nulle et alternative respectivement",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-puissance.html#puissance-pour-un-test-de-t-comparant-deux-moyennes",
    "href": "25-puissance.html#puissance-pour-un-test-de-t-comparant-deux-moyennes",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "\n8.4 Puissance pour un test de t comparant deux moyennes",
    "text": "8.4 Puissance pour un test de t comparant deux moyennes\n\n\n\n\n\n\nImportant\n\n\n\nL’ensemble des analyses de puissance décrites après peuvent être réalisé avec 2 fonctions dans R.\n\n\npwr.t.test(): lorsque les tailles d’échantillons sont identiques\n\npwr.t2n.test(): lorsque les échantillons ont des tailles différentes\n\n\n\nL’objectif de cette séance de laboratoire est de vous familiariser avec G*Power et de vous aider à comprendre comment les quatre paramètres des analyses de puissance (\\(\\alpha\\), \\(\\beta\\), effectif et taille d’effet) sont reliés entre eux. On examinera seulement l’un des nombreux tests : le test de t permettant de comparer deux moyennes indépendantes. C’est le test le plus communément utilisé par les biologistes, vous l’avez tous déjà utilisé, et il conviendra très bien pour les besoins de la cause. Ce que vous apprendrez aujourd’hui s’appliquera à toutes les autres analyses de puissance que vous effectuerez à l’avenir.\nJaynie Stephenson a étudié la productivité des ruisseaux de la région d’Ottawa. Elle a, entre autres, quantifié la biomasse des poissons dans 18 ruisseaux sur le Bouclier Canadien d’une part, et dans 18 autres ruisseaux de la vallée de la rivière des Outaouais et de la rivière Rideau d’autre part. Elle a observé une biomasse plus faible dans les ruisseaux de la vallée (2.64 \\(g/m^2\\), écart-type=3.28) que dans ceux du Bouclier (3.31 \\(g/m^2\\), écart-type=2.79). En faisant un test de t pour éprouver l’hypothèse nulle que la biomasse des poissons est la même dans les deux régions, elle obtient:\nPooled-Variance Two-Sample t-Test\nt = -0.5746, df = 34, p-value = 0.5693\nElle accepte l’hypothèse nulle (puisque p est plus élevé que 0.05) conclue donc que la biomasse moyenne des poissons est la même dans ces deux régions.\n\n8.4.1 Analyse post-hoc - Calculer la puissance du test\nCompte tenu des valeurs des moyennes observées et de leur écart- type, on peut utiliser G*Power pour calculer la puissance du test de t bilatéral pour deux moyennes indépendantes et pour la taille d’effet (i.e. la différence entre la biomasse entre les deux régions, pondérée par les écarts-type) à \\(\\alpha\\) = 0.05.\nDémarrer G*Power.\n\nÀ Test family, choisir: t tests\nÀ Statistical test, choisir: Means: Difference between two independent means (two groups)\nÀ Type of power analysis, choisir: Post hoc: Compute achieved power - given \\(\\alpha\\), sample size, and effect size\nDans Input Parameters,\n\n\nà la boîte Tail(s), choisir: Two,\nvérifier que \\(\\boldsymbol{\\alpha}\\) err prob est égal à 0.05\ninscrire 18 pour Sample size group 1 et 2\npour calculer la taille d’effet (Effect size d), cliquer sur le bouton Determine =&gt;\n\n\n\nDans la fenêtre qui s’ouvre à droite, sélectionner \\(\\boldsymbol{n1 = n2}\\)\n\n\n\nentrer les moyennes (Mean group 1 et 2)\nentrer les écarts types (SD \\(\\boldsymbol{\\sigma}\\) group 1 et 2)\ncliquer sur le bouton Calculate and transfer to main window\n\n\n\nCliquer sur le bouton Calculate dans la fenêtre principale et vous devriez obtenir ceci:\n\n\n\n\n\n\n\n\nFigure 8.1: Analyse post-hoc avec la taille d’effet estimée\n\n\n\n\nLa même analyse peut être faites en utlisant R. D’abord, il faut calculer la taille d’effet d pour un test de t comparant deux moyennes, puis utiliser la fonction pwr.t.test du paquet pwr 📦. Le plus simple comme nous allons estimer la taille d’effet d plusieurs fois et de créer une petite fonction qui estime d basé sur les paramètres nécessaires.\n\n# charger le paquet pwr\nlibrary(pwr)\n# définir une fonction pour d\nd &lt;- function(u1, u2, sd1, sd2) {\n  abs(u1 - u2) / sqrt((sd1^2 + sd2^2) / 2)\n}\n\n# analyse de puissance\n\nd_cohen &lt;- d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79)\n\npwr.t.test(\n  n = 18, d = d_cohen, sig.level = 0.05,\n  type = \"two.sample\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.09833902\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# graphique comme g*Power\nx &lt;- seq(-4, 4, length = 200)\ny0 &lt;- dt(x, 34)\ny1 &lt;- dt(x, 34, ncp = d_cohen * sqrt(36) / 2)\nplot(x, y0, type = \"l\", col = \"red\", lwd = 2)\nqc &lt;- qt(0.025, 34)\nabline(v = qc, col = \"green\")\nabline(v = -qc, col = \"green\")\nlines(x, y1, type = \"l\", col = \"blue\", lwd = 2)\n\n# l'erreur de type 2 correspond à la zone bleue\npolygon(\n  c(x[x &lt;= -qc], -qc), c(y1[x &lt;= -qc], 0),\n  col = rgb(red = 0, green = 0.2, blue = 1, alpha = 0.5)\n)\npolygon(\n  c(-qc, x[x &gt;= -qc]), c(0, y0[x &gt;= -qc]),\n  col = rgb(red = 1, green = 0, blue = 0.2, alpha = 0.5)\n)\npolygon(\n  c(x[x &lt;= qc], qc), c(y0[x &lt;= qc], 0),\n  col = rgb(red = 1, green = 0, blue = 0.2, alpha = 0.5)\n)\n\n\n\n\n\n\nFigure 8.2: Graphique de l’analyse de puissance dans R\n\n\n\n\nVoici le code pour faire la figure avec ggplot.\n\nqc &lt;- qt(0.025, 34)\nncp &lt;- d_cohen * sqrt(36) / 2\ndat &lt;- data.frame(\n  x = seq(-4, 4, length = 200),\n  y0 = dt(x, (n - 1) * 2),\n  y1 = dt(x, (n - 1) * 2, ncp = ncp)\n) %&gt;%\n  mutate(\n    beta = ifelse(x &lt;= -qc, y1, 0),\n    alpha = ifelse(x &lt;= qc | x &gt;= -qc, y0, 0)\n  )\n\nggplot(dat, aes(x = x)) +\n  geom_line(aes(y = y0), color = \"red\") +\n  geom_line(aes(y = y1), color = \"blue\") +\n  geom_vline(xintercept = qcl, color = \"green\") +\n  geom_area(\n    aes(x = x, y = beta),\n    fill = rgb(red = 0, green = 0.2, blue = 1, alpha = 0.5)\n  ) +\n  geom_area(\n    aes(x = x, y = alpha),\n    fill = rgb(red = 1, green = 0, blue = 0.2, alpha = 0.5)\n  ) +\n  theme_classic() +\n  ylab(\"dt(x)\")\n\nÉtudions un peu la figure Figure 8.1.\n\nLa courbe de gauche, en rouge, correspond à la distribution de la statistique t si \\(\\boldsymbol{H_0}\\) est vraie (i.e si les deux moyennes étaient égales) compte tenu de l’effectif (18 dans chaque région) et des écarts- types observés.\nLes lignes verticales vertes correspondent aux valeurs critiques de t pour \\(\\boldsymbol{\\alpha = 0.05}\\) et un effectif total de 36 (2x18).\nLes régions ombrées en rose correspondent aux zones de rejet de \\(\\boldsymbol{H_0}\\) (\\({\\alpha/2}\\)) . Si Jaynie avait obtenu une valeur de t en dehors de l’intervalle délimité par les valeurs critiques allant de -2.03224 à 2.03224, alors elle aurait rejeté \\(H_0\\), l’hypothèse nulle d’égalité des deux moyennes. En fait, elle a obtenu une valeur de t égale à -0.5746 et conclu que la biomasse est la même dans les deux régions.\nLa courbe de droite, en bleu, correspond à la distribution de la statistique t si \\(\\boldsymbol{H_1}\\) est vraie (ici \\(H_1\\) correspond à une différence de biomasse entre les deux régions de \\(3.33-2.64=0.69g/m^2\\), compte tenu des écarts-types observés). Cette distribution correspond à ce qu’on devrait s’attendre à observer si \\(H_1\\) était vraie et que l’on répétait un grand nombre de fois les mesures dans des échantillons aléatoires de 18 ruisseaux des deux régions en calculant la statistique t à chaque fois. En moyenne, on observerait une valeur de t d’environ 0.6.\nNotez que la distribution de droite chevauche considérablement celle de gauche, et une bonne partie de la surface sous la courbe de droite se retrouve à l’intérieur de l’intervalle d’acceptation de \\(H_0\\), délimité par les deux lignes vertes et allant de -2.03224 à 2.03224. Cette proportion, correspondant à la partie ombrée en bleu sous la courbe de droite et dénoté par \\(\\beta\\) correspond au risque d’erreur de type II qui est d’accepter \\(H_0\\) quand \\(H_1\\) est vraie.\nLa puissance est simplement \\(\\boldsymbol{1-\\beta}\\), et est ici de 0.098339 (Résultats du test d’estimation de puissance) . Donc, si la biomasse différait de 0.69\\(g/m^2\\) entre les deux régions, Jaynie n’avait que 9.8% des chances d’être capable de détecter une différence statistiquement significative à \\(\\alpha=5%\\) en échantillonnant 18 ruisseaux de chaque région.\n\nRécapitulons: La différence de biomasse entre les deux régions n’est pas statistiquement significative d’après le test de t. C’est donc que cette différence est relativement petite compte tenu de la précision des mesures. Il n’est donc pas très surprenant que la puissance, i.e. la probabilité de détecter une différence significative, soit faible. Toute cette analyse ne nous informe pas beaucoup.\nUne analyse de puissance post hoc avec la taille d’effet observé n’est pas très utile. On la fera plutôt pour une taille d’effet autre que celle observée quand \\(H_0\\) est acceptée. Quelle taille d’effet utiliser? C’est la biologie du système étudié qui peut nous guider. Par exemple, en ce qui concerne la biomasse des poissons, on pourrait s’attendre à ce qu’une différence de biomasse du simple au double (disons de 2.64 à 5.28 \\(g/m^2\\)) ait des conséquences écologiques. On voudrait s’assurer que Jaynie avait de bonnes chances de détecter une différence aussi grande que celle-là avant d’accepter ses conclusions que la biomasse est la même entre les deux régions. Quelles étaient les chances de Jaynie de détecter une différence de 2.64 \\(g/m^2\\) entre les deux régions? G*Power peut nous le dire.\n\n\n\n\n\n\nExercice\n\n\n\nChanger la moyenne du groupe 2 à 5.28, recalculer la taille d’effet, et cliquer sur Calculate pour obtenir Figure 8.3.\n\n\n\n\n\n\n\n\n\nFigure 8.3: Analyse post-hoc avec une taille d’effet différente\n\n\n\n\n\npwr.t.test(\n  n = 18, d = d(u1 = 2.64, sd1 = 3.28, u2 = 5.28, sd2 = 2.79),\n  sig.level = 0.05, type = \"two.sample\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.8670313\n      sig.level = 0.05\n          power = 0.7146763\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\n\nFigure 8.4\n\n\n\n\nLa puissance est de 0.71, donc Jaynie avait une chance raisonnable (71%) de détecter une différence du simple au double avec 18 ruisseaux dans chaque région.\nNotez que cette analyse de puissance post hoc pour une taille d’effet jugée biologiquement significative est bien plus informative que l’analyse précédente pour la taille d’effet observée (qui est celle effectuée par défaut par bien des néophytes et de trop nombreux logiciels qui essaient de penser pour nous). En effet, Jaynie n’a pu détecter de différences significatives entre les deux régions. Cela pourrait être pour deux raisons: soit qu’il n’y a pas de différences entre les régions, ou alors parce que la précision des mesures est si faible et l’effort d’échantillonnage si limité qu’il était très peu probable de détecter même d’énormes différences. La deuxième analyse de puissance permet d’éliminer cette seconde possibilité puisque Jaynie avait 71% des chances de détecter une différence du simple au double.\n\n8.4.2 Analyse à priori - Calculer la taille de l’effectif à échantillonner\nSupposons qu’on puisse défendre la position qu’une différence de biomasse observée par Jaynie entre les deux régions, \\(3.31- 2.64=0.67g/m^2\\), soit écologiquement significative. On devrait donc planifier la prochaine saison d’échantillonnage de manière à avoir de bonnes chances de détecter une différence de cette taille. Combien de ruisseaux Jaynie devrait-elle échantillonner pour avoir 80% des chances de la détecter (compte tenu de la variabilité observée)?\n\n\n\n\n\n\nExercice\n\n\n\nChanger le type d’analyse de puissance dans G*Power à A priori: Compute sample size - given \\(\\alpha\\), power, and effect size. Assurez-vous que les valeurs pour les moyennes et les écarts-type soient celles qu’a obtenu Jaynie, recalculez la taille de l’effet, et inscrivez 0.8 pour la puissance Figure 8.5.\n\n\n\n\n\n\n\n\n\nFigure 8.5: Analyse à priori\n\n\n\n\n\npwr.t.test(\n  power = 0.8, d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79),\n  sig.level = 0.05, type = \"two.sample\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 325.1723\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOuch! Il faudrait échantillonner 326 ruisseaux dans chaque région! Cela coûterait une fortune et exigerait de nombreuses équipes de travail. Sans cela, on ne pourrait échantillonner que quelques dizaines de ruisseaux, et il serait peu probable que l’on puisse détecter une si faible différence de biomasse entre les deux régions. Ce serait vraisemblablement en vain et pourrait être considéré comme une perte de temps: pourquoi tant d’efforts et de dépenses si les chances de succès sont si faibles.\nSi on refait le même calcul pour une puissance de 95%, on obtient 538 ruisseaux par région. Augmenter la puissance ça demande plus d’effort.\n\npwr.t.test(\n  power = 0.95, d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79),\n  sig.level = 0.05, type = \"two.sample\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 537.7286\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n8.4.3 Analyse de sensitivité - Calculer la taille d’effet détectable\nCompte tenu de la variabilité observée, d’un effort d’échantillonnage de 18 ruisseaux par région, et en conservant \\(\\alpha=0.05\\), quelle est la taille d’effet que Jaynie pouvait détecter avec 80% de chances (\\(\\beta=0.2\\)) ?\n\n\n\n\n\n\nExercice\n\n\n\nChangez le type d’analyse dans G*Power à Sensitivity: Compute required effect size - given \\(\\alpha\\), power, and sample size et assurez-vous que la taille des échantillons est de 18 dans chaque région.\n\n\n\n\n\n\n\n\n\nFigure 8.6: Analyse de sensitivité\n\n\n\n\n\npwr.t.test(power = 0.8, n = 18,\n           sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.9612854\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nLa taille d’effet détectable pour cette taille d’échantillon, \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (ou une puissance de 80%) est de 0.961296.\n\n\n\n\n\n\nAvertissement\n\n\n\ncette valeur est l’indice d de la taille d’effet et est pondérée par la variabilité des mesures.\n\n\nDans ce cas ci, d est approximativement égal à \\[ d = \\frac{| \\bar{X_1} \\bar{X_2} |} {\\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}}\\] Pour convertir cette valeur de d sans unités en une valeur de différence de biomasse détectable (i.e \\(| \\bar{X_1} \\bar{X_2} |\\)), il suffit de multiplier d par le dénominateur de l’équation. \\[\n| \\bar{X_1} \\bar{X_2} | = d * \\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}\n\\] Dans R, on peut estimer cela avec:\n\npwr.t.test(\n  power = 0.8, n = 18, sig.level = 0.05,\n  type = \"two.sample\")$d * sqrt((3.28^2 + 2.79^2) / 2)\n\n[1] 2.926992\n\n\nDonc, avec 18 ruisseaux dans chaque région, pour \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (une puissance de 80%), Jaynie pouvait détecter une différence de biomasse de 2.93\\(g/m^2\\) entre les régions, un peu plus que du simple au double.",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-puissance.html#points-à-retenir",
    "href": "25-puissance.html#points-à-retenir",
    "title": "\n8  Analyse de puissance avec R et G*Power\n",
    "section": "\n8.5 Points à retenir",
    "text": "8.5 Points à retenir\n\nL’analyse de puissance post hoc n’est pertinente que lorsque l’on a accepté l’hypothèse nulle. Il est en effet impossible de faire une erreur de type II quand on rejette \\(H_0\\).\nAvec de très grands échantillons, on a une puissance quasi infinie et on peut détecter statistiquement de très petites différences qui ne sont pas nécessairement biologiquement significatives.\nEn utilisant un critère de signification plus strict (\\(\\alpha &lt; 0.05\\)) on diminue notre puissance.\nEn voulant maximiser la puissance, on augmente l’effort requis, à moins d’utiliser une valeur critique plus libérale (\\(\\alpha&gt;0.05\\))\nLe choix de \\(\\beta\\) est quelque peu arbitraire. On considère que \\(\\beta=0.2\\) (puissance de 80%) est relativement élevé.",
    "crumbs": [
      "Données",
      "Principes de statistique",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html",
    "href": "31-reg_lin.html",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "",
    "text": "9.1 Paquets R et données requises\nPour ce laboratoire vous aurez besoin de :\nIl ne faut pas oublier de charge les paquets avec library() (et de les installer au besoin avec install.packages()). Pour les données, il faut les charger et les assigner à un objet R avec la fonction read.csv().\nlibrary(car)\nlibrary(lmtest)\nlibrary(boot)\nlibrary(ggplot2)\nlibrary(pwr)\nlibrary(performance)\n\nesturgeon &lt;- read.csv(\"data/sturgeon.csv\")",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#set-lm",
    "href": "31-reg_lin.html#set-lm",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "",
    "text": "paquets R :\n\n\ncar 📦\n\n\nlmtest 📦\n\n\nboot 📦\n\n\npwr 📦\n\n\nggplot 📦\n\n\n\nJeu de données :\n\n“sturgeon.csv”\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotez que la ligne de code pour charger les données considère que le fichier de données se trouve dans un dossier data au sein de votre répertoire de travail. Si ce n’est pas le cas veuillez ajuster la ligne de commande selon votre répertoire de travail.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#diagrammes-de-dispersion",
    "href": "31-reg_lin.html#diagrammes-de-dispersion",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.2 Diagrammes de dispersion",
    "text": "9.2 Diagrammes de dispersion\nLes analyses de corrélation et de régression devraient toujours commencer par un examen des données; c’est une étape critique qui sert à évaluer si ce type d’analyse est approprié pour un ensemble de données. Supposons que nous voulons évaluer si la longueur d’esturgeons mâles dans la région de The Pas covarie avec leur poids. Pour répondre à cette question, regardons d’abord la corrélation entre la longueur et le poids. Souvenez-vous qu’une des conditions d’application de l’analyse de corrélation est que la relation entre les deux variables est linéaire. Pour évaluer cela, commençons par faire un diagramme de dispersion.\n\nLes données sur les esturgeons son disponibles dans le fichier sturgeon.csv. Après avoir chargé les données en les assignant à un objet esturgeon, faites un diagramme de dispersion avec une droite de régression linéaire et une courbe de Lowess de la longueur en fonction du poids.\n\n\nesturgeon &lt;- read.csv(\"data/sturgeon.csv\")\nstr(esturgeon)\n\n'data.frame':   186 obs. of  9 variables:\n $ fklngth : num  37 50.2 28.9 50.2 45.6 ...\n $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...\n $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...\n $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...\n $ age     : int  11 24 7 23 20 23 20 7 23 19 ...\n $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...\n $ sex     : chr  \"MALE\" \"FEMALE\" \"MALE\" \"FEMALE\" ...\n $ location: chr  \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" ...\n $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...\n\n\n\nmongraph &lt;- ggplot(\n  data = esturgeon[!is.na(esturgeon$rdwght), ], # origine des données\n  aes(x = fklngth, y = rdwght)\n)\n# Représentez les données sous forme de Points, Régression linéaire, \"Lowess\"\nmongraph &lt;- mongraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # Ajoutez une régression linéaire, mais sans l'erreur-type (en vert)\n  stat_smooth(color = \"red\", se = FALSE) + # Ajoutez la \"lowess\" (en rouge)\n  geom_point() + # Ajoutez les données sous forme de points\n  labs(x = \"Longueur\", y = \"Poids\") # Modifiez les noms des axes pour rendre le graphique plus lisible\n\nmongraph # Affichez le graphique\n\n\n\n\n\n\nFigure 9.1: Graphique du poids en fonction de la longueur des esturgeons.\n\n\n\n\n\n\n\nEst-ce que la dispersion des points suggère une bonne corrélation entre les deux variables? Est-ce que la relation semble linéaire?\n\nCe graphique suggère une tendance plus curvilinéaire que linéaire. Malgré tout, il semble y avoir une forte corrélation entre les deux variables.\n\nRefaites le diagramme de dispersion avec des transformations logarithmiques sur les deux axes.\n\n\n# Appliquez une transformation log sur le graphique déjà défini\nmongraph + scale_x_log10() + scale_y_log10()\n\n\n\n\n\n\nFigure 9.2: Graphique poids-longueur avec une échelle log.\n\n\n\n\nComparez les diagrammes de dispersion avant et après transformation (Figure 9.1 et Figure 9.2). Sachant que l’analyse de corrélation présuppose une relation linéaire entre les variables, on devrait donc privilégier l’analyse sur les données log-transformées.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#transformations-de-données-et-coefficient-de-corrélation",
    "href": "31-reg_lin.html#transformations-de-données-et-coefficient-de-corrélation",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.3 Transformations de données et coefficient de corrélation",
    "text": "9.3 Transformations de données et coefficient de corrélation\nUne autre condition préalable à l’analyse de corrélation est que les deux variables concernées suivent une distribution normale bivariée. On peut aisément vérifier la normalité de chacune des 2 variables séparément tel que décrit dans plus bas dans la section Section 9.6.1. Si les deux variables sont normalement distribuées, on présume généralement que la distribution commune des deux variables est également normale (notez que ce n’est pas toujours le cas cependant).\n\nExaminez la distribution des quatre variables (les deux variables originales et les variables transformées). Que concluez-vous de l’inspection visuelle de ces graphiques ?\n\nLes figures ci-dessous sont les 4 diagrammes Q(uantile)-Q(uantile) (qqplot()). Le code pour produire des graphiques multiples sur une seule page avec 2 lignes et 2 colonnes, comme on voit ci-dessous, est :\n\npar(mfrow = c(2, 2)) # divise le graphique en 4 sections\nqqnorm(esturgeon$fklngth, ylab = \"fklngth\")\nqqline(esturgeon$fklngth)\nqqnorm(log10(esturgeon$fklngth), ylab = \"log10(fklngth)\")\nqqline(log10(esturgeon$fklngth))\nqqnorm(esturgeon$rdwght, ylab = \"rdwght\")\nqqline(esturgeon$rdwght)\nqqnorm(log10(esturgeon$rdwght), ylab = \"log10(rdwgth)\")\nqqline(log10(esturgeon$rdwght))\npar(mfrow = c(1, 1)) # redéfinie la zone de graphique par défaut\n\n\n\n\n\n\nFigure 9.3: Diagrammes Q-Q des quatres variables étudiées (Poids, Longueur, Originale et Log-transformées).\n\n\n\n\n\nIl n’y a pas grand-chose à redire: aucune des distributions n’est parfaitement normale, mais les déviations semblent mineures.\n\nGénérez une matrice de graphiques de dispersion de toutes les paires de variables (avec régression linéaires et “lowess”) en utilisant la fonction scatterplotMatrix du paquet car 📦.\n\n\nscatterplotMatrix(\n  ~ fklngth + log10(fklngth) + rdwght + log10(rdwght),\n  data = esturgeon,\n  smooth = TRUE, diagonal = \"density\"\n)\n\n\n\n\n\n\nFigure 9.4: Corrélations entre chaque paires de variables\n\n\n\n\n\n\nEnsuite, calculez le coefficient de corrélation de Pearson entre chaque paire (variables originales et log-transformées) en utilisant la fonction cor(). Avant de commencer, on va cependant ajouter les variables transformées au jeu de données esturgeon :\n\n\nesturgeon$lfklngth &lt;- with(esturgeon, log10(fklngth))\nesturgeon$lrdwght &lt;- log10(esturgeon$rdwght)\n\nVous pouvez ensuite obtenir la matrice de corrélation par :\n\ncor(esturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\nFréquemment, il y a des données manquantes dans un échantillon. En précisant use=\"complete.obs\", toutes les lignes du fichier pour lesquelles les variables ne sont pas toutes mesurées sont éliminées. Dans ce cas, toutes les corrélations seront calculées avec le même nombre de cas. Par contre, en utilisant use=\"pairwise.complete.obs\", R élimine une observation seulement lorsque l’un des deux membres de la paire a une valeur manquante. Dans ce cas, si les données manquantes pour différentes variables se retrouvent dans un groupe différent d’observation, les corrélations ne seront pas nécessairement calculées sur le même nombre de cas ni sur le même sous-ensemble de cas. En général, vous devriez utiliser l’option use=\"complete.obs\" à moins que vous ayez un très grand nombre de données manquantes et que cette façon de procéder élimine la plus grande partie de vos observations.\nPourquoi la corrélation entre les variables originales est-elle plus faible qu’entre les variables transformées ?\n\ncor(esturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\n           fklngth  lfklngth   lrdwght    rdwght\nfklngth  1.0000000 0.9921435 0.9645108 0.9175435\nlfklngth 0.9921435 1.0000000 0.9670139 0.8756203\nlrdwght  0.9645108 0.9670139 1.0000000 0.9265513\nrdwght   0.9175435 0.8756203 0.9265513 1.0000000\n\n\nIl y a plusieurs choses à noter ici.\n\nPremièrement, la corrélation entre la longueur (“fklngth”) et le poids (“rdwght”) est élevée, peu importe la transformation: les poissons lourds ont tendance à être longs.\nDeuxièmement, la corrélation est plus forte pour les données transformées que pour les données brutes.\n\nPourquoi? Parce que le coefficient de corrélation est inversement proportionnel au bruit autour de la relation linéaire. Si la relation est curvilinéaire (comme dans le cas des données non transformées), le bruit est plus grand que si la relation est parfaitement linéaire. Par conséquent, la corrélation est plus faible.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#matrices-de-corrélations-et-correction-de-bonferroni",
    "href": "31-reg_lin.html#matrices-de-corrélations-et-correction-de-bonferroni",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.4 Matrices de corrélations et correction de Bonferroni",
    "text": "9.4 Matrices de corrélations et correction de Bonferroni\nUne pratique courante est d’examiner la matrice de corrélation à la recherche d’associations significatives. Comme exemple, essayons de tester si la corrélation entre lfklngth et rdwght est significative (le plus faible coefficient de corrélation de cette matrice).\n\nEstimez la corrélation entre la longueur (fklngth et le poids (rdwght) des esturgeons:\n\n\ncor.test(\n  esturgeon$lfklngth, esturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"pearson\"\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  esturgeon$lfklngth and esturgeon$rdwght\nt = 24.322, df = 180, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8367345 0.9057199\nsample estimates:\n      cor \n0.8756203 \n\n\nOn voit ici que la corrélation est hautement significative (\\(p &lt; 2.2e-16\\)), ce qui n’est pas surprenant étant donné la valeur du coefficient de corrélation (0.8756 = 87.6%). Il est important de réaliser que si une matrice contient un grand nombre de corrélations, il n’est pas surprenant d’en trouver au moins une qui soit “significative”. En effet, on s’attend à en trouver 5% en moyenne lorsqu’il n’y a en fait aucune corrélation entre les paires de moyennes (correspondant au risque d’erreur de type I \\(\\alpha\\)) . Une façon de corriger pour cette tendance est d’ajuster le niveau \\(\\alpha\\) critique auquel on attribue une signification statistique en divisant \\(\\alpha\\) par le nombre \\(k\\) de corrélations qui sont examinées :\n\\(\\alpha' = \\alpha / k\\) (ajustement de Bonferroni).\nSi initialement \\(\\alpha = 0.05\\) et qu’il y a 10 corrélations qui sont examinées, alors \\(\\alpha'= 0.005\\). Donc, afin de rejeter l’hypothèse nulle, la valeur de p devra être plus petite que \\(\\alpha'\\), en l’occurrence 0.005. Dans l’exemple qui précède, on devrait donc ajuster \\(\\alpha\\) critique en divisant par le nombre total de corrélations dans la matrice (6 dans ce cas, donc \\(\\alpha'=0.00833\\)). Cette correction modifie-t-elle votre conclusion quant à la corrélation entre lkflngth et rdwght?",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#corrélations-non-paramétriques-r-de-spearman-et-tau-de-kendall",
    "href": "31-reg_lin.html#corrélations-non-paramétriques-r-de-spearman-et-tau-de-kendall",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.5 Corrélations non paramétriques: r de Spearman et \\(\\tau\\) de Kendall",
    "text": "9.5 Corrélations non paramétriques: r de Spearman et \\(\\tau\\) de Kendall\nL’analyse faite dans la section précédente avec les esturgeons suggère que l’une des conditions préalables à l’analyse de corrélation, soit la distribution normale bivariée de données, pourrait ne pas être remplie pour fklngth et rdwght, ni pour les paires de variables transformées. La recherche d’une transformation appropriée peut parfois être difficile. Pire encore, pour certaines distributions il n’existe pas de transformation qui va normaliser les données. Dans ces cas-là, la meilleure option est de faire une analyse non paramétrique qui ne présume ni de la normalité ni de la linéarité. Ces analyses sont basées sur les rangs. Les deux plus communes sont le coefficient de rang de Spearman et le \\(\\tau\\) (tau) de Kendall.\n\nDans R, testez la corrélation entre fklngth et rdwght en utilisant Spearman et Kendall’s .\n\n\ncor.test(\n  esturgeon$lfklngth, esturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"spearman\"\n)\n\nWarning in cor.test.default(esturgeon$lfklngth, esturgeon$rdwght, alternative =\n\"two.sided\", : Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  esturgeon$lfklngth and esturgeon$rdwght\nS = 47971, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9522546 \n\n\n\ncor.test(\n  esturgeon$lfklngth, esturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"kendall\"\n)\n\n\n    Kendall's rank correlation tau\n\ndata:  esturgeon$lfklngth and esturgeon$rdwght\nz = 16.358, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.8208065 \n\n\nComparer les résultats de cette analyse à l’analyse paramétrique. Pourquoi y-a-t’il une différence ?\nCalculez les corrélations non paramétriques sur les paires de variables transformées. Vous devriez voir tout de suite que les corrélations des données transformées et non transformées sont identiques puisque dans les deux cas la corrélation est calculée à partir des rangs qui ne sont pas affectés par la transformation.\nNotez que les corrélations obtenues avec le \\(\\tau\\) de Kendall (0.820) sont plus faibles que celles du coefficient de Spearman (0.952). Le \\(\\tau\\) de Kendall pondère un peu plus les grandes différences entre les rangs alors que le coefficient de Spearman donne le même poids à chaque paire d’observations. En général, on préfère le \\(\\tau\\) de Kendall lorsqu’il y a plus d’incertitude quant aux rangs qui sont près les uns des autres.\nLes esturgeons de cet échantillon ont été capturés à l’aide de filets et d’hameçons d’une taille fixe. Quel impact cette méthode de capture peut-elle avoir eu sur la forme de la distribution de fklngth et rdwght? Compte tenu de ces circonstances, l’analyse de corrélation est-elle appropriée ?\nRappelez-vous que l’analyse de corrélation présume aussi que chaque variable est échantillonnée aléatoirement. Dans le cas de nos esturgeons, ce n’est pas le cas: les hameçons appâtés et les filets ne capturent pas de petits esturgeons (et c’est pourquoi il n’y en a pas dans l’échantillon). Il faut donc réaliser que les coefficients de corrélation obtenus dans cette analyse ne reflètent pas nécessairement ceux de la population totale des esturgeons.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#sec-simple-lm",
    "href": "31-reg_lin.html#sec-simple-lm",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.6 Régression linéaire simple",
    "text": "9.6 Régression linéaire simple\nL’analyse de corrélation vise à décrire comment deux variables covarient. L’analyse de régression vise plutôt à produire un modèle permettant de prédire une variable (la variable dépendante) par l’autre (la variable indépendante).\nComme pour l’analyse de corrélation, on devrait commencer en examinant des graphiques. Puisque l’on veut quantifier la relation entre deux variables, un graphique de la variable dépendante (Y) en fonction de la variable indépendante (X) est tout à fait approprié.\n\nLe fichier sturgeon.csv contient les données d’un inventaire d’esturgeons récoltés entre 1978 et 1980 à Cumberland House en Saskatchewan et à The Pas au Manitoba. Faites un diagramme de dispersion de fklngth (la variable dépendante, Y) en fonction de l’age (la variable indépendante, X) pour les esturgeons mâles uniquement et ajoutez-y une régression linéaire et une “lowess”. Que concluez-vous de ce diagramme de dispersion ?\n\n\nesturgeon.male &lt;- subset(esturgeon, subset = sex == \"MALE\")\nmongraph &lt;- ggplot(\n  data = esturgeon.male, # origine des données\n  aes(x = age, y = fklngth)\n) # aesthetics: y=fklngth, x=rdwght\n# Représentez les données sous forme de Points, Régression linéaire, \"Lowess\"\nmongraph &lt;- mongraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # Ajoutez une régression linéaire, mais sans l'erreur-type (en vert)\n  stat_smooth(color = \"red\") + # Ajoutez la \"lowess\" (en rouge)\n  geom_point() +# Ajoutez les données sous forme de points\n  labs(x = \"Age\", y = \"Longueur\") # Modifiez les noms des axes pour rendre le graphique plus lisible\nmongraph # Affichez le graphique\n\n\n\n\n\n\nFigure 9.5: Graphique de la longueur en fonction de l’âge des esturgeons males.\n\n\n\n\nCe graphique suggère que la relation n’est pas linéaire.\nSupposons que nous désirions estimer le taux de croissance des esturgeons mâles. Un estimé (peut-être pas terrible…) du taux de croissance peut être obtenu en calculant la pente de la régression de la longueur en fonction de l’âge.\nAjustons d’abord une régression avec la commande lm() et sauvons ces résultats dans un objet appelé RegModel.1.\n\nRegModele.1 &lt;- lm(fklngth ~ age, data = esturgeon.male)\n\nRien n’apparait à l’écran, c’est normal ne vous inquiétez pas, tout a été sauvegardé en mémoire (nouvel objet visible dans l’onglet environnement) . Pour voir les résultats, tapez:\n\nsummary(RegModele.1)\n\n\nCall:\nlm(formula = fklngth ~ age, data = esturgeon.male)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4936 -2.2263  0.1849  1.7526 10.8234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.50359    1.16873   24.39   &lt;2e-16 ***\nage          0.70724    0.05888   12.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.307 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.664, Adjusted R-squared:  0.6594 \nF-statistic: 144.3 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nla sortie R donne:\n\n\nCall: Un petit rappel du modèle qui a été ajusté et des données utilisées.\n\nResiduals: Un sommaire statistique des résidus autour du modèle estimé.\n\nCoefficients: Valeurs estimées des paramètres du modèle, erreurs-types, statistiques t et probabilités associées.\n\nResidual standard error: Racine carrée de la variance résiduelle.\n\nMultiple R-squared: Coefficient de détermination. Il correspond à la proportion de la variabilité de la variable dépendante qui peut être expliquée par la régression.\n\nAdjusted R-squared: Le R-carré ajusté tient compte du nombre de paramètres du modèle. Si vous voulez comparer différents modèles qui n’ont pas le même nombre de paramètres, c’est ce qu’il faut utiliser.\n\nF-statistic: C’est le test de signification omnibus du modèle. Dans le cas de la régression simple, il est équivalent au test sur la pente de la régression.\n\nLa régression estimée est donc:\n\\[ Fklngth = 28.50359 + 0.70724 * age\\]\nÉtant donné la valeur significative du test de F (ainsi que pour le test de t pour la pente de la droite), on rejette l’hypothèse nulle qu’il n’y a pas de relation entre la taille et l’âge.\n\n9.6.1 Vérifier les conditions d’application de la régression\nLa régression simple de type I a quatre conditions préalables :\n\nil n’y a pas d’erreur de mesure sur la variable indépendante (X)\nla relation entre Y et X est linéaire\n\nles résidus sont normalement distribués\n\nla variance des résidus est constante pour toutes les valeurs de la variable indépendante (homoscedasticité)\n\nProcédons maintenant à l’examen post-mortem. La première condition est rarement remplie avec des données biologiques ; il y presque toujours de l’erreur sur X et sur Y. Cela veut dire qu’en général les pentes estimées sont biaisées, mais que les valeurs prédites ne le sont pas. Toutefois, si l’erreur de mesure sur X est petite par rapport à l’étendue des valeurs de X, le résultat de l’analyse n’est pas dramatiquement influencé. Par contre, si l’erreur de mesure est relativement grande (toujours par rapport à l’étendue des valeurs de X), la droite de régression obtenue par la régression de modèle I est un piètre estimé de la relation fonctionnelle entre X et Y. Dans ce cas, il est préférable de passer à la régression de modèle II, malheureusement au-delà du contenu de ce cours. Les autres conditions préalables à l’analyse de régression de modèle I peuvent cependant être vérifiées, ou du moins évaluées visuellement. La fonction plot() (appliqué directement sur un objet de type model) permet de visualiser des graphiques diagnostiques pour des modèles linéaires.\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModele.1)\n\nLa fonction par() est utilisée pour dire à R de tracer 2 rangées et 2 colonnes de graphiques par page (il y a quatre graphiques diagnostiques qui sont générés automatiquement pour les modèles linéaires), et l’argument las = indique à R d’effectuer une rotation des légendes des axes Y pour qu’elles soient perpendiculaires à l’axe (oui. Je sais. Rien de tout ça n’est évident.)\nVous obtiendrez:\n\n\n\n\n\n\n\nFigure 9.6: Graphiques diagnostiques du modèle.\n\n\n\n\n\n\nLe graphique en haut à gauche, permet d’évaluer la linéarité, la normalité, et l’homoscédasticité des résidus. Il illustre les déviations autour de la régression en fonction des valeurs prédites. Rappelez-vous que le graphique de fklngth vs age suggère que la relation entre la longueur et l’âge n’est pas linéaire. Les très jeunes et très vieux esturgeons sont sous la droite en général, alors que les esturgeons d’âge moyen sont retrouvés généralement au-dessus de la droite de régression. C’est exactement ce que le graphique des résidus en fonction des valeurs prédites illustre. La ligne en rouge est une trace “lowess” au travers de ce nuage de points. Si la relation était linéaire, la trace “lowess” serait presque plate et près de 0. La dispersion des résidus permet d’évaluer visuellement leur normalité et hétéroscédasticité; mais ce graphique n’est pas optimal pour évaluer ces propriétés. Les deux graphiques suivants sont mieux pour cela.\nLe graphique en haut à droite permet d’évaluer la normalité des résidus. C’est un graphique QQ des résidus (QQ plot). Des résidus distribués normalement tomberaient exactement sur la diagonale. Ici, on voit que c’est presque le cas, sauf dans les queues de la distribution.\n\nLe graphique en bas à gauche, intitulé Scale-Location, permet d’évaluer l’homoscédasticité. On y retrouve sur l’ordonnée (l’axe des y) la racine carrée de la valeur absolue des résidus standardisés (résidus divisés par l’écart-type des résidus) en fonction des valeurs prédites. Le graphique aide à déterminer si la variation des résidus est constante ou non. Si les résidus sont homoscédastiques, la valeur moyenne sur l’axe des y ne va pas changer en fonction de la valeur prédite. Ici, il y a une certaine tendance, mais pas une tendance monotone puisqu’il y a d’abord une baisse puis une hausse..; bref, rien qui soit une forte évidence contre la supposition d’homoscédasticité.\n\nLe graphique en bas à droite, montre les résidus en fonction du “leverage” et permet de détecter certaines valeurs extrêmes qui ont une grande influence sur la régression. Le leverage d’un point mesure sa distance des autres points, mais seulement en ce qui concerne les variables indépendantes. Dans le cas d’une régression simple, cela revient à la distance entre le point sur l’axe des x et la moyenne de tous les points sur cet axe. Vous devriez porter une attention particulière aux observations qui ont un leverage plus grand que \\(2(k+1)/n\\), où k est le nombre de variables indépendantes (ici, 1) et n est le nombre d’observations. Dans cet exemple, il y a 75 observations et une variable indépendante et donc les points ayant un leverage plus grand que \\(4 / 75 =  0.053\\) devrait être considérés avec attention. Le graphique indique également comment la régression changerait si on enlevait un point. Ce changement est mesuré par la distance de Cook, illustrée par les bandes en rouge sur le graphique. Un point ayant une distance de Cook supérieure à 1 a une grande influence.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nNotez que R identifie automatiquement les cas les plus extrêmes sur chacun de ces 4 graphiques. Le fait qu’un point soit identifié ne signifie pas nécessairement que c’est une valeur réellement extrême, ou que vous devez vous en préoccuper. Dans tous les ensembles de données il y aura toujours un résidu plus grand que les autres…\n\n\nIl est possible d’obtenir des graphiques d’évaluations des conditions d’applications, qui sont plus simple à interpréter et plus joli (avec des couleurs). On peut utiliser la fonction check_model() du paquet performance 📦.\n\ncheck_model(RegModele.1)\n\n\n\n\n\n\nFigure 9.7: Graphiques diagnostiques du modèle avec la fonction check_model().\n\n\n\n\nFinalement, quel est le verdict concernant la régression linéaire entre fklngth et age ? Elle viole la condition de linéarité, possiblement celle de normalité, remplit la condition d’homoscédasticité, et ne semble pas être influencée outre mesure par des valeurs bizarres ou extrêmes.\n\n9.6.2 Tests formels des conditions d’application pour la régression\nPersonnellement, je n’utilise jamais les tests formels des conditions d’application de la régression et me contente des graphiques des résidus pour guider mes décisions. C’est ce que la plupart des praticiens font. Cependant, lors de mes premières analyses, je n’étais pas toujours certain de bien interpréter les graphiques et j’aurais aimé un indice plus formel ou un test permettant de détecter les violations des conditions d’application de la régression.\nLe package lmtest 📦 (disponible sur CRAN), permet de faire plusieurs tests de linéarité et d’homoscédasticité. Et on peut tester la normalité avec le test Shapiro-Wilk vu précédemment.\nCharger le package lmtest de CRAN (et installer le si besoin).\n\nlibrary(lmtest)\n\n\n\n\n\n\n\nExercice\n\n\n\nExécutez les commandes suivantes\n\n\n\nbptest(RegModele.1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModele.1\nBP = 1.1765, df = 1, p-value = 0.2781\n\n\nLe test Breusch-Pagan examine si la variabilité des résidus est constantes lorsque les valeurs prédites changent. Une faible valeur de p suggère de l’hétéroscédasticité. Ici, la valeur p est élevée et suggère que la condition d’application d’homoscédasticité est remplie avec ces données.\n\ndwtest(RegModele.1)\n\n\n    Durbin-Watson test\n\ndata:  RegModele.1\nDW = 2.242, p-value = 0.8489\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nLe test Durbin-Watson permet de détecter l’autocorrélation sérielle des résidus. En l’absence d’autocorrélation (i.e. d’indépendance des résidus) la valeur attendue de la statistique D est 2. Ce test permet d’éprouver l’hypothèse d’indépendance des résidus, mais ne permet de détecter qu’un type particulier de dépendance. Ici, le test ne permet pas de rejeter l’hypothèse d’indépendance.\n\nresettest(RegModele.1)\n\n\n    RESET test\n\ndata:  RegModele.1\nRESET = 14.544, df1 = 2, df2 = 71, p-value = 5.082e-06\n\n\nLe test RESET permet d’éprouver la linéarité. Si la relation est linéaire, alors la statistique RESET sera d’environ 1. Ici, la statistique est beaucoup plus élevée (14.54) et hautement significative. Le test confirme la tendance que nous avons détectée visuellement plus haut: la relation n’est pas linéaire.\n\nshapiro.test(residuals(RegModele.1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModele.1)\nW = 0.98037, p-value = 0.2961\n\n\nLe test de normalité Shapiro-Wilk sur les résidus confirme que la déviation par rapport à une distribution normale des résidus n’est pas grande.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#transformation-des-données-en-régression",
    "href": "31-reg_lin.html#transformation-des-données-en-régression",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.7 Transformation des données en régression",
    "text": "9.7 Transformation des données en régression\nLa relation entre fklngth et age n’étant pas linéaire, on devrait donc essayer de transformer les données pour tenter de les linéariser :\n\nVoyons ce qu’une transformation log donne:\n\n\npar(mfrow = c(1, 1), las = 1)\nggplot(\n  data = esturgeon.male,\n  aes(x = log10(age), y = log10(fklngth))\n) +\n  geom_smooth(color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  geom_point()\n\n\n\n\n\n\nFigure 9.8: Graphique du log de la longueur en fonction du log de l’âge des esturgeons males\n\n\n\n\nAjustons maintenant une régression simple sur ces données transformées.\n\nRegModele.2 &lt;- lm(log10(fklngth) ~ log10(age), data = esturgeon.male)\nsummary(RegModele.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = esturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nExaminons maintenant les graphiques diagnostiques:\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModele.2)\n\n\n\n\n\n\nFigure 9.9: Graphiques diagnostiques du modèle avec les données log-transformées.\n\n\n\n\nIl y a une certaine amélioration, mais ce n’est pas encore parfait (la perfection n’est pas de ce monde….). Le graphique des résidus en fonction des valeurs prédites suggère encore une certaine non linéarité. Sur le graphique Q-Q les points se retrouvent plus près de la droite diagonale qu’avant, indiquant que les résidus sont encore plus près de la normalité après la transformation log-log. Il n’y a pas d’indice d’hétéroscédasticité. Finalement, même si il reste quelques points avec plus d’influence (leverage) que les autres, aucun n’a une distance de Cook au-delà de 0.5. En résumé, la transformation log a amélioré les choses: la relation est plus linéaire, les résidus sont plus normaux, et il y a moins de points avec une influence relativement élevée. Est-ce que les tests formels supportent cette évaluation ?\n\nbptest(RegModele.2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModele.2\nBP = 0.14282, df = 1, p-value = 0.7055\n\ndwtest(RegModele.2)\n\n\n    Durbin-Watson test\n\ndata:  RegModele.2\nDW = 2.1777, p-value = 0.6134\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModele.2)\n\n\n    RESET test\n\ndata:  RegModele.2\nRESET = 4.4413, df1 = 2, df2 = 71, p-value = 0.01523\n\nshapiro.test(residuals(RegModele.2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModele.2)\nW = 0.98998, p-value = 0.8246\n\n\nOui, les conclusions sont les mêmes: les résidus sont encore homoscédastiques (test Breusch-Pagan), ne sont pas autocorrélés (test Durbin-Watson), sont normaux (test Shapiro-Wilk), et sont plus linéaires (la valeur de P du test RESET est maintenant 0.015, au lieu de 0.000005). Donc la linéarité a augmenté, mais cette condition d’application semble encore légèrement violée.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#traitement-des-valeurs-extrêmes",
    "href": "31-reg_lin.html#traitement-des-valeurs-extrêmes",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.8 Traitement des valeurs extrêmes",
    "text": "9.8 Traitement des valeurs extrêmes\nDans cet exemple, il n’y a pas de valeur vraiment extrême. Oui, je sais, R a quand même identifié les observations 8, 24, et 112 dans le dernier graphique diagnostique. Mais ces valeurs sont encore dans la fourchette de valeurs que je juge “acceptables”. Mais comment déterminer objectivement ce qui est acceptable ? À quel moment juge t’on qu’une valeur extrême est vraiment trop invraisemblable pour ne pas l’exclure ? Il n’y a malheureusement pas de règle absolue là-dessus. Les opinions varient, mais je penche vers le conservatisme sur cette question.\nMa position est que, à moins que la valeur soit biologiquement impossible ou clairement une erreur d’entrée de données, je n’élimine pas les valeurs extrêmes et j’utilise toutes mes données dans leur analyse. Pourquoi?\nParce que je veux que mes données reflètent bien la variabilité naturelle ou réelle. C’est d’ailleurs parfois cette variabilité qui est intéressante.\nL’approche conservatrice qui consiste à conserver toutes les valeurs extrêmes possibles est possiblement la plus honnête, mais elle peut causer certains problèmes. Ces valeurs extrêmes sont souvent la cause des violations des conditions d’application des tests statistiques. La solution suggérée à ce dilemme est de faire l’analyse avec et sans les valeurs extrêmes et de comparer les conclusions. Dans bien des cas, les conclusions seront qualitativement les mêmes et les tailles d’effet ne seront pas très différentes. Toutefois, dans certains cas, la présence des valeurs extrêmes change complètement les conclusions. Dans ces cas, il faut simplement accepter que les conclusions dépendent entièrement de la présence des valeurs extrêmes et sont donc peu concluantes.\nSuivant cette approche comparative, refaisons donc l’analyse après avoir enlevé les observations 8, 24, et 112.\n\nRegModele.3 &lt;- lm(log10(fklngth) ~ log10(age), data = esturgeon.male, subset = !(rownames(esturgeon.male) %in% c(\"8\", \"24\", \"112\"))) # On enlève les observations 8, 24 et 112 du jeu de données\nsummary(RegModele.3)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = esturgeon.male, \n    subset = !(rownames(esturgeon.male) %in% c(\"8\", \"24\", \"112\")))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069163 -0.017390  0.000986  0.018590  0.047647 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.22676    0.02431   50.46   &lt;2e-16 ***\nlog10(age)   0.31219    0.01932   16.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02554 on 70 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7855 \nF-statistic:   261 on 1 and 70 DF,  p-value: &lt; 2.2e-16\n\n\nL’ordonnée à l’origine (Intercept), la pente, et le R carré sont presque les mêmes, et la valeur de p est encore astronomiquement petite. Enlever les valeurs extrêmes a peu d’effet dans ce cas.\nLes graphiques diagnostiques des résidus et les tests formels des conditions d’application sur ce sous-ensemble de données donnent :\n\npar(mfrow = c(2, 2))\nplot(RegModele.3)\nbptest(RegModele.3)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModele.3\nBP = 0.3001, df = 1, p-value = 0.5838\n\ndwtest(RegModele.3)\n\n\n    Durbin-Watson test\n\ndata:  RegModele.3\nDW = 2.0171, p-value = 0.5074\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModele.3)\n\n\n    RESET test\n\ndata:  RegModele.3\nRESET = 3.407, df1 = 2, df2 = 68, p-value = 0.0389\n\nshapiro.test(residuals(RegModele.3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModele.3)\nW = 0.98318, p-value = 0.4502\n\n\n\n\n\n\n\nFigure 9.10: Graphiques diagnostiques du modèle RegModele.3 excluant les données aberrantes.\n\n\n\n\nIl n’y a pas vraiment de différence ici non plus avec l’analyse des données en entier. Bref, tout pointe vers la conclusion que les valeurs les plus extrêmes de cet ensemble de donnée n’influencent pas indûment les résultats statistiques.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#quantifier-la-taille-deffet-et-analyse-de-puissance-en-régression",
    "href": "31-reg_lin.html#quantifier-la-taille-deffet-et-analyse-de-puissance-en-régression",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.9 Quantifier la taille d’effet et analyse de puissance en régression",
    "text": "9.9 Quantifier la taille d’effet et analyse de puissance en régression\nL’interprétation biologique des résultats n’est pas la même chose que l’interprétation statistique. Dans l’analyse qui précède, on conclue statistiquement que la taille augmente avec l’âge (puisque la pente est positive et et \\(p&lt;0.05\\)). Mais cette augmentation “statistique” de la taille avec l’âge ne donne pas d’informations sur la différence de taille entre les jeunes et vieux individus. La pente et un graphique sont plus informatifs à ce sujet que la valeur p. La pente (dans l’espace log-log) est de 0.34. Cela veut dire que pour chaque unité d’accroissement de X (log10(age)), il y a une augmentation de 0.34 unités de log10(fklngth). En d’autres mots, quand l’âge est multiplié par 10, la longueur est multipliée environ par 2 (100.34 = 2.19). Donc la longueur des esturgeons augmente plus lentement que leur âge. La valeur de la pente (0.34) est un estimé de la taille de l’effet de l’âge sur la longueur.\nIl est aussi important d’estimer l’intervalle de confiance sur la pente pour pouvoir estimer si l’intervalle n’inclus ou non que des valeurs biologiquement importantes. Cela peut être fait simplement avec la fonction confint().\n\nconfint(RegModele.2)\n\n                2.5 %   97.5 %\n(Intercept) 1.1377151 1.246270\nlog10(age)  0.2976433 0.384068\n\n\nL’intervalle de confiance à 95% de la pente est 0.29-0.38. L,intervalle de confiance est assez étroit et éloigné de zéro.\n\n9.9.1 Puissance de détecter une pente donnée\nPour les calculs de puissance avec G*Power vous devrez cependant utiliser une autre métrique de la taille de l’effet, calculée à partir de la pente, de son erreur-type, et de la taille de l’échantillon (ce qui facilite les calculs pour G*Power, mais malheureusement pas pour vous) La métrique (d) est calculée comme: \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} \\] où \\(b\\) est l’estimé de la pente, \\(s_b\\) est l’erreur type de la pente, \\(n\\) est le nombre d’observations, et \\(k\\) est le nombre de variables indépendantes (1 pour la régression linéaire simple).\nVous pouvez calculer approximativement la puissance avec G*Power pour une valeur de pente que vous jugez assez grande pour mériter d’être détectée. Choisissez Tests: Means: One group: difference from constant, là, vous devrez remplacer la valeur de \\(b\\) dans l’équation pour la taille d’effet (d) par la pente que vous voudriez détecter, mais utiliser l’erreur type calculée à partir de vos données.\nPar exemple, supposons que les ichthyologues considèrent qu’une pente de 0.1 pour la relation entre log10(fklngth) et log10(age) est signifiante biologiquement, et qu’ils désirent estimer la puissance de détecter une telle pente à partir d’un échantillon de 20 esturgeons. Les résultats de la régression log-log nous fournissent ce dont on a besoin:\n\nsummary(RegModele.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = esturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nL’erreur-type de la pente est 0.02168. Il y avait 75 poissons (n=75) dans l’échantillon de départ. On peut donc calculer la métrique de taille d’effet pour G*Power \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} = \\frac{0.1}{0.02168\\sqrt{74-1-1}}=0.54\\]\nArmés de cette taille d’effet (une pente présumée de 0.1 et une variabilité autour de la régression similaire à la régression de fklngth vs age), choisissez Tests: Means: One group: difference from constant, et entrez la valeur calculée de d, alpha, et l’effectif de l’échantillon pour calculer la puissance.\n\n\n\n\n\n\n\nFigure 9.11: Analyse de puissance pour N = 20 et pente = 0.1\n\n\n\n\nDans R, il est possible de faire cette analyse avec le code suivant:\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(n = 20, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20\n              d = 0.54\n      sig.level = 0.05\n          power = 0.6299804\n    alternative = two.sided\n\n\nLa puissance de détecter une pente comme étant statistiquement significative (au niveau alpha), si la pente est 0.1, que la variabilité résiduelle autour de la régression est semblable à celle de notre échantillon (ce qui revient à une taille d’effet de 0.54, pour un échantillon de 20 esturgeons et alpha=0.05) est de 0.629. Seulement environ 2/3 des échantillons de cette taille détecteraient un effet significatif de l’âge sur fklngth.\n\n9.9.2 Effectif requis pour atteindre une puissance désirée (test A-priori)\nPour estimer la taille d’échantillon (effectif) requis pour avoir une puissance de 99% de détecter un effet de l’âge si la pente est 0.1 (sur une échelle log-log), avec alpha=0.05, on utilise la même valeur de d (0.54):\n\n\n\n\n\n\n\nFigure 9.12: Analyse à priori pour déterminer la taille d’échantillon pour une puissance de 0.99\n\n\n\n\nDans R, il est possible de faire cette analyse avec le code suivant:\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(n = 65, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 65\n              d = 0.54\n      sig.level = 0.05\n          power = 0.9900297\n    alternative = two.sided\n\n\nEn augmentant la taille de l’échantillon à 65, selon le même scénario que précédemment, la puissance augmente à 99%.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#bootstrap-en-régression-simple-avec-r",
    "href": "31-reg_lin.html#bootstrap-en-régression-simple-avec-r",
    "title": "\n9  Corrélation et régression linéaire simple\n",
    "section": "\n9.10 Bootstrap en régression simple avec R",
    "text": "9.10 Bootstrap en régression simple avec R\nUn test non paramétrique pour l’ordonnée à l’origine et la pente d’une régression simple peut être effectué par bootstrap.\n\n# charger le paquet boot\nlibrary(boot)\n# obtenir les poids de régression\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # Permets à boot de sélectionner les échantillons\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrap avec 1000 réplications\nresults &lt;- boot(\n  data = esturgeon.male,\n  statistic = bs,\n  R = 1000, formula = log10(fklngth) ~ log10(age)\n)\n# Résultats\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = esturgeon.male, statistic = bs, R = 1000, formula = log10(fklngth) ~ \n    log10(age))\n\n\nBootstrap Statistics :\n     original        bias    std. error\nt1* 1.1919926  0.0010297189  0.03344522\nt2* 0.3408557 -0.0006201354  0.02643693\n\n\nPour chaque paramètre du modèle (ici l’ordonnée à l’origine est appelée t1* et la pente de la régression t2*), R imprime :\n\n\noriginal la valeur estimée sur tout l’échantillon\n\nbias la différence entre la valeur moyenne des estimés par bootstrap et la valeur originale sur tout l’échantillon\n\nstd. error l’erreur-type de l’estimé bootstrap\n\n\npar(mfrow = c(2, 2))\nplot(results, index = 1) # intercept\nplot(results, index = 2) # log10(age)\n\n\n\n\n\n\nFigure 9.13: Résultats du test de bootstrap non paramétrique\n\n\n\n\n\n\n\n\n\nFigure 9.14: Résultats du test de bootstrap non paramétrique\n\n\n\n\nLa distribution des estimés obtenus par bootstrap est assez normale dans cet exemple, avec de petites déviations dans les queue de la distribution (là où ça compte pour les intervalles de confiance…). On pourrait utiliser l’erreur-type des estimés bootstrap pour calculer un intervalle de confiance symétrique (moyenne +- t E.T.). Cependant, comme R peut facilement calculer des intervalles de confiance qui corrigent pour le biais (BCa: “Bias-Corrected Adjusted”) ou encore des intervalle empiriques à partir des distributions simulées (méthode Percentile) il peut être aussi simple de les calculer selon les 3 méthodes:\n\n# interval de confiance pour l'ordonnée à l'origine\nboot.ci(results, type = \"all\", index = 1)\n\nWarning in boot.ci(results, type = \"all\", index = 1): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 1)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 1.125,  1.257 )   ( 1.126,  1.254 )  \n\nLevel     Percentile            BCa          \n95%   ( 1.130,  1.258 )   ( 1.123,  1.253 )  \nCalculations and Intervals on Original Scale\n\n\n\n# intervalle de confiance pour la pente\nboot.ci(results, type = \"all\", index = 2)\n\nWarning in boot.ci(results, type = \"all\", index = 2): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 2)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.2897,  0.3933 )   ( 0.2896,  0.3923 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.2894,  0.3921 )   ( 0.2941,  0.3955 )  \nCalculations and Intervals on Original Scale\n\n\nIci, les 4 types d’intervalles de confiance que R a calculé sont essentiellement semblables. Si les données avaient violé plus sévèrement les conditions d’application de la régression (normalité, homoscedasticité), alors les différentes méthodes (Normal, Basic, Percentile, et BCa) auraient divergé un peu plus. Lequel choisir alors? BCa est celui qui est préféré de la majorité des praticiens, présentement.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "32-t_test.html",
    "href": "32-t_test.html",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "",
    "text": "10.1 Paquets R et données requises\nPour ce laboratoire, vous aurez besoin de :\nlibrary(car)\nlibrary(lmtest)\nlibrary(boot)\nlibrary(lmPerm)\nlibrary(ggplot2)\nesturgeon &lt;- read.csv(\"data/sturgeon.csv\")\ncrane &lt;- read.csv(\"data/skulldat_2020.csv\")",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#set-t",
    "href": "32-t_test.html#set-t",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "",
    "text": "Paquets R:\n\n\ncar 📦\n\n\nlmtest 📦\n\n\nboot 📦\n\n\nlmPerm 📦\n\n\n\nJeux de données\n\n“sturgeon.csv”\n“skulldat_2020.csv”",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#examen-visuel-des-données",
    "href": "32-t_test.html#examen-visuel-des-données",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "\n10.2 Examen visuel des données",
    "text": "10.2 Examen visuel des données\nUne des premières étapes dans toute analyse de données est l’examen visuel des données par des graphiques et statistiques sommaires pour avoir une idée des distributions sous-jacentes, des valeurs extrêmes et des tendances dans vos données. Cela commence souvent avec des graphiques de vos données (histogrammes, diagrammes de probabilité, boîte à moustache, etc.) qui vous permettent d’évaluer si vos données sont distribuées normalement (c-à-d, suivent une distribution normale), si elles sont corrélées les unes aux autres, ou s’il y a des valeurs suspectes dans le jeu de données.\nSupposons que l’on veuille comparer la distribution des taille des esturgeons de The Pas et Cumberland House. La variable fklngth dans le jeu de données sturgeon.csv représente la longueur (en cm) de chaque poisson (mesurée de l’extrémité de la tête à la base de la fourche de la nageoire caudale). Pour commencer, examinons si cette variable est normalement distribuée. On ne va pas tester pour la normalité à ce stade-ci ; la présomption de normalité dans les analyses paramétriques s’applique aux résidus et non aux données brutes. Cependant, si les données brutes ne sont pas normales, vous avez, en général, une très bonne raison de soupçonner que les résidus ne suivront pas non plus une distribution normale.\nUne excellente façon de comparer visuellement une distribution à la distribution normale est de superposer un histogramme des données observées à une courbe normale. Pour ce faire, il faut procéder en deux étapes :\n\nIndiquer à R que nous voulons créer un histogramme superposé à une courbe normale\nSpécifier qu’on veut que les graphiques soient faits pour les deux sites\n\n\nEn utilisant les données de sturgeon.csv, générez les histogrammes et les approximations des distributions normales ajustées aux données de fklngth à The Pas et Cumberland House.\n\n\n# Utilisez le jeu de données \"sturgeon\" pour créer le graphique appelé \"mongraph\".\n# Et définissez l'axe x, correspondant à \"fklngth\".\nmongraph &lt;- ggplot(data = esturgeon, \n                   aes(x = fklngth)) +\n  xlab(\"Longueur à la fourche (cm)\")\n\n# Ajoutez des éléments au graphique (ggplot) \"mongraph\".\nmongraph &lt;- mongraph +\n  geom_density() + # Ajoutez la densité des données, lissée.\n  geom_rug() + # Ajoutez un \"tapis\" (barres en bas du graphe).\n  geom_histogram(aes(y = ..density..),\n                 color = \"black\", alpha = 0.3) +  # Ajoutez un histogramme noir, semi transparent.\n  stat_function(fun = dnorm,\n                args = list(mean = mean(esturgeon$fklngth),\n                            sd = sd(esturgeon$fklngth)),\n                color = \"red\") # Ajoutez une courbe normale en rouge, à partir de la moyenne et écart-type de \"fklngth\".\n\nmongraph + facet_grid(. ~ location) # Affichez le graphe, par site.\n\n\n\n\n\n\nFigure 10.1: Distribution de la longueur des esturgeons par site.\n\n\n\n\nExaminez ce graphique et essayez de déterminer si ces deux échantillons sont normalement distribués. À mon avis, cette variable est approximativement normalement distribuée dans les deux échantillons.\nPuisque ce qui nous intéresse est de comparer la taille des poissons de deux sites différents, c’est probablement une bonne idée de créer un graphique qui compare les deux groupes de données. Une boîte à moustache (“Box plot”) convient très bien pour cette tâche.\n\nTracez un boxplot de fklngth groupé par location. Que concluez-vous quant à la différence entre les deux sites?\n\n\nggplot(data = esturgeon, aes(x = location, \n                            y = fklngth)) +\n  geom_boxplot(notch = TRUE)\n\n\n\n\n\n\nFigure 10.2: Boxplot de la longueur des esturgeons par site.\n\n\n\n\nIl n’y a pas de grande différence de taille entre les deux sites, mais la taille des poissons à The Pas est plus variable, ayant une plus large étendue de taille et des valeurs extrêmes (définies par les valeurs qui sont &gt; 1.5 * l’étendue interquartile) à chaque bout de la distribution.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-indépendants",
    "href": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-indépendants",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "\n10.3 Comparer les moyennes de deux échantillons indépendants",
    "text": "10.3 Comparer les moyennes de deux échantillons indépendants\nÉprouvez l’hypothèse nulle (H0) : La longueur à la fourche n’est pas différente entre The Pas et Cumberland House de 3 manières différentes :\n\nTest paramétriques supposant des variances égales\nTest paramétriques supposant des variances différentes\nTest non-paramétrique (pas de conditions d’applications sur la distribution et la variance)\n\nQue concluez-vous?\n\n# Test t supposant des variances égales.\nt.test(fklngth ~ location,\n       data = esturgeon,\n       alternative = \"two.sided\",\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  fklngth by location\nt = 2.1359, df = 184, p-value = 0.03401\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1308307 3.2982615\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# Test t supposant des variances différentes.\nt.test(fklngth ~ location,\n       data = esturgeon,\n       alternative = \"two.sided\",\n       var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  fklngth by location\nt = 2.2201, df = 169.8, p-value = 0.02774\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1900117 3.2390804\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# Test non paramétrique.\nwilcox.test(fklngth ~ location,\n            data = esturgeon,\n            alternative = \"two.sided\")\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  fklngth by location\nW = 4973, p-value = 0.06296\nalternative hypothesis: true location shift is not equal to 0\n\n\nEn se fiant au test de t, on rejette donc l’hypothèse nulle. Il y a une différence significative entre les deux moyennes des longueurs à la fourche selon le site.\nNotez que si l’on se fie au test de Wilcoxon, on ne peut pas rejeter l’hypothèse nulle. Les deux tests mènent donc à des conclusions contradictoires. La différence significative obtenue par le test de t peut provenir en partie d’une violation des conditions d’application du test (normalité et homoscédasticité). D’un autre côté, l’absence de différence significative selon le test de Wilcoxon pourrait être due au fait que, pour un effectif donné, la puissance du test non paramétrique est inférieure à celle du test paramétrique correspondant. Compte tenu 1) des valeurs de p obtenues pour les deux tests, et 2) le fait que pour des grands échantillons (des effectifs de 84 et 101 sont considérés grands) le test de t est considéré robuste, il est raisonnable de rejeter l’hypothèse nulle.\nAvant d’accepter les résultats du test de t et de rejeter l’hypothèse nulle qu’il n’y a pas de différences de taille entre les deux sites, il est important de déterminer si les données remplissent les conditions de normalité des résidus et d’égalité des variances. L’examen préliminaire suggérait que les données sont à peu près normales mais qu’il y avait peut-être des problèmes avec les variances (puisque l’étendue des données pour The Pas était beaucoup plus grande que celle pour Cumberland). On peut examiner ces conditions d’application plus en détail en examinant les résidus d’un modèle linéaire et en utilisant les graphiques diagnostiques:\n\nm1 &lt;- lm(fklngth ~ location, data = esturgeon)\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\n\n\nFigure 10.3: Examen visuel des conditions d’application du modèle linéaire.\n\n\n\n\nLe premier graphique ci-dessus montre comment les résidus se distribuent autour des valeurs prédites (les moyennes) pour chaque site et permet de juger si il semble y avoir un problème de normalité ou d’homoscédasticité. Si les variances étaient égales dans les deux sites, l’étendue verticale des résidus tendrait à être la même. Sur le graphique, on voit que l’étendue des résidus est plus grande à gauche (le site où la taille moyenne est la plus faible), ce qui suggère un possible problème d’homogénéité des variances. On peut tester cela plus formellement en comparant la moyenne de la valeur absolue des résidus.(on y reviendra; c’est le test de Levene).\nLe deuxième graphique est un graphique de probabilité (graphique Q-Q) des résidus. Comme ici, les points tombent près de la diagonale, il ne semble pas y avoir de problème important avec la normalité. On peut faire un test formel de la condition de normalité : le test de Shapiro-Wilk:\n\nshapiro.test(residuals(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1)\nW = 0.97469, p-value = 0.001857\n\n\nHummm… Ce test indique que les résidus ne sont pas normaux, ce qui contredit notre évaluation visuelle. Cependant, puisque (a) la distribution des résidus ne s’éloigne pas beaucoup de la normalité et (b) le nombre d’observations à chaque site est raisonnablement grand (i.e. &gt;30), nul besoin d’être trop inquiet quant à l’impact de cette violation de normalité sur la fiabilité du test.\nQu’en est-il de l’égalité des variances?\n\nlibrary(car)\nleveneTest(m1)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1  11.514 0.0008456 ***\n      184                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nbptest(m1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m1\nBP = 8.8015, df = 1, p-value = 0.00301\n\n\nLes résultats qui précédents proviennent de 2 des tests disponibles en R (dans les package car 📦 et lmtest 📦) qui éprouvent l’hypothèse de l’égalité des variances dans des tests de t ou des modèles linéaires ayant uniquement des variables indépendantes discontinues ou catégoriques. Il est inutile de faire les 2 tests. Si ils sont présentés ici, c’est que ces 2 tests sont usuels et qu’il n’y a pas consensus quant au meilleur des deux. Le test de Levene est le plus connu et utilisé. Il compare la moyenne des valeurs absolues des résidus dans les deux groupes. Le test Breusch-Pagan a l’avantage d’être applicable à une plus large gamme de modèles linéaires (il peut être utilisé également avec des variables indépendantes continues, comme en régression). Ici, les deux tests mènent à la même conclusion: la variance diffère entre les deux sites.\nSur la base de ces résultats, on peut conclure qu’il y a des éléments (même si faibles) pour rejeter l’hypothèse nulle qu’il n’y a pas de différence dans la taille de poissons entre les deux sites. On a utilisé une modification du test de t pour tenir compte du fait que les variances ne sont pas égales et nous sommes satisfaits que la condition de normalité des résidus a été remplie. Alors, “fklngth” à Cumberland est plus grande que “fklngth” à The Pas.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#bootstrap-et-tests-de-permutation-pour-comparer-deux-moyennes",
    "href": "32-t_test.html#bootstrap-et-tests-de-permutation-pour-comparer-deux-moyennes",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "\n10.4 Bootstrap et tests de permutation pour comparer deux moyennes",
    "text": "10.4 Bootstrap et tests de permutation pour comparer deux moyennes\n\n10.4.1 Bootstrap\nLe bootstrap et les tests de permutation peuvent être utilisés pour comparer les moyennes (ou d’autres statistiques). Le principe général est simple et peut être effectué de diverses façons. Ici on utilise certains des outils disponibles et le fait qu’une comparaison de moyenne peut être représentée par un modèle linéaire. On pourra utiliser un programme similaire plus tard quand on ajustera des modèles plus complexes (mais plus amusants !).\n\nlibrary(boot)\n\nLa première section sert à définir une fonction (ici appelée bs) qui extraie les coefficients d’un modèle ajusté :\n\n# Fonction pour extraire les coefficients d'un modèle pour chaque itérations.\nbs &lt;- function(formule, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formule, data = d)\n  return(coef(fit))\n}\n\nLa deuxième section avec la commande boot() fait le gros du travail: on prend les données dans “sturgeon”, on les bootstrap \\(R = 1000\\) fois, et chaque fois on ajuste le modèle fklngth vs location et on garde les valeurs calculées par la fonction bs.\n\n# bootstrap avec 1000 réplications.\nresultats &lt;- boot(data = esturgeon, statistic = bs, R = 1000,\n                formule = fklngth ~ location)\n# Affichez les résultats.\nresultats\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = esturgeon, statistic = bs, R = 1000, formule = fklngth ~ \n    location)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 45.084391 -0.010974135   0.4225382\nt2* -1.714546  0.004510085   0.7645004\n\n\nOn obtient les estimés originaux pour les deux coefficients du modèle: la moyenne pour le premier (alphabétiquement) site soit Cumberland, et la différence entre les deux moyennes à Cumberland et The Pas. C’est ce second paramètre, la différence entre les moyennes, qui nous intéresse.\n\nplot(resultats, index = 2)\n\n\n\n\n\n\nFigure 10.4: Normalité des estimés de la différence des moyennes par bootstrap.\n\n\n\n\n\n# Calculez l'intervalle de confiance à 95%.\nboot.ci(resultats, type = \"bca\", index = 2)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = resultats, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   (-3.220, -0.129 )  \nCalculations and Intervals on Original Scale\n\n\nComme l’intervalle de confiance n’inclue pas 0, on conclue que les moyennes ne sont pas les mêmes.\n\n10.4.2 Permutation\nLes tests de permutation pour les modèles linéaires peuvent être effectués à l’aide du package lmPerm 📦 :\n\nm1Perm &lt;- lmp(fklngth ~ location,\n              data = esturgeon,\n              perm = \"Prob\")\n\n[1] \"Settings:  unique SS \"\n\n\nLa fonction lmp() fait tout le travail pour nous. Ici, cette fonction est effectuée avec l’option perm pour choisir la règle utilisée pour stopper les calculs. L’option “Prob” arrête les permutations quand la déviation standard estimée pour la p-valeur tombe sous un seuil déterminé. C’est l’une des nombreuses règles qui peuvent possiblement être utilisées pour ne faire les permutations que sur un sous-ensemble des permutations possibles (ce qui prendrait souvent trèèèèès longtemps).\n\nsummary(m1Perm)\n\n\nCall:\nlmp(formula = fklngth ~ location, data = sturgeon, perm = \"Prob\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-18.40921  -3.75370  -0.08439   3.76598  23.48055 \n\nCoefficients:\n          Estimate Iter Pr(Prob)  \nlocation1   0.8573 3150   0.0308 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.454 on 184 degrees of freedom\nMultiple R-Squared: 0.02419,    Adjusted R-squared: 0.01889 \nF-statistic: 4.562 on 1 and 184 DF,  p-value: 0.03401 \n\n\n\n\nIter: la règle a limité le calcul à 5000 permutations. Notez que ce nombre va varier à chaque fois que vous ferez tourner ce code. Ce sont des résultats obtenus par permutations aléatoires, donc vous devez vous attendre à de la variabilité. .\n\nPr(Prob): La p-valeur estimée pour H0 est 0.0172. La différence observée pour “fklngth” entre les deux sites était plus grande que les valeurs permutées pour environ (1 - 0.0172= 98.3%) des 5000 permutations. Notez que 5000 permutations ce n’est pas un si grand nombre de permutations que ça, et donc les faibles valeurs de p ne sont pas très précises. Si vous voulez des valeurs précises de p, vous devrez faire plus de permutations. Vous pouvez ajuster 2 paramètres: maxIter, le nombre maximal de permutations (défaut 5000) et Ca, le seuil de précision désiré qui arrête les permutations quand l’erreur-type de p est plus petite que Ca*p (défaut=0.1)\n\nF-statistic: Le reste est la sortie standard pour un modèle ajusté à des données, avec le test paramétrique. Ici, la p-valeur, présumant que toutes les conditions d’application sont remplies, est 0.034.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-appariés",
    "href": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-appariés",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "\n10.5 Comparer les moyennes de deux échantillons appariés",
    "text": "10.5 Comparer les moyennes de deux échantillons appariés\n\n\n\n\n\n\nAvertissement\n\n\n\nPour la section suivante veuillez télécharger le jeu de données skulldat_2020.csv qui a été récemment ajouté sur Brightspace.\n\n\nDans certaines expériences les mêmes individus sont mesurés deux fois, par exemple avant et après un traitement ou encore à deux moments au cours de leur développement. Les mesures obtenues lors de ces deux évènements ne sont donc pas indépendantes, et des comparaisons de ces mesures appariées doivent être faites.\nLe jeu de données skulldat_2020.csv contient des mesures de la partie inférieure du visage de jeunes filles d’Amérique du Nord prises à 5 ans, puis à 6 ans (données de Newman and Meredith, 1956).\n\nPour débuter, éprouvons l’hypothèse que la largeur de la figure est la même à 5 ans et à 6 ans en assumant que les mesures viennent d’échantillons indépendants.\n\n\ncrane &lt;- read.csv(\"data/skulldat_2020.csv\")\nt.test(width ~ age,\n       data = crane,\n       alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  width by age\nt = -1.7812, df = 27.93, p-value = 0.08576\nalternative hypothesis: true difference in means between group 5 and group 6 is not equal to 0\n95 percent confidence interval:\n -0.43002624  0.03002624\nsample estimates:\nmean in group 5 mean in group 6 \n       7.461333        7.661333 \n\n\nJusqu’à maintenant, nous avons spécifié le test de t en utilisant une notation de type formule avec y ~ x où y est la variable pour laquelle on souhaite comparer les moyennes et x correspond à une variable définissant les groupes. Cela marche bien lorsque les données ne sont pas appariées et sont présentées dans un format de type long où les données prise dans une même catégorie ou sur une même personne sont simplement les unes en-dessous des autres avec des colonnes indiquant l’appartenance des mesures aux différentes catégories (voir la structure de crane par exemple) Dans le jeu de données crane, il y a 3 colonnes:\n\n\nwidth: largeur de la tête\n\nage: age lors de la mesure\n\nid: identité de la personne\n\n\nhead(crane)\n\n  width age id\n1  7.33   5  1\n2  7.53   6  1\n3  7.49   5  2\n4  7.70   6  2\n5  7.27   5  3\n6  7.46   6  3\n\n\nQuand les données sont appariées, il faut indiquer comment elle doivent être associées. Dans notre exemple, elles sont appariées par individu. Le format de données de type long indique cet appariement via la colonne id. Cependant, la fonction t.test ne permet pas de le prendre en compte. Il faut donc transformer les données en format de type large ou horizontale ou il y a une colonne différente pour chaque catégorie. Dans notre exemple, on veut un jeu de données avec une colonne de mesure par age et où chaque ligne correspond à une personne différente. On peut modifier le format des données avec le code suivant.\n\ncrane_h &lt;- data.frame(id = unique(crane$id))\ncrane_h$width5 &lt;- crane$width[match(crane_h$id, crane$id) & crane$age == 5]\ncrane_h$width6 &lt;- crane$width[match(crane_h$id, crane$id) & crane$age == 6]\nhead(crane_h)\n\n  id width5 width6\n1  1   7.33   7.53\n2  2   7.49   7.70\n3  3   7.27   7.46\n4  4   7.93   8.21\n5  5   7.56   7.81\n6  6   7.81   8.01\n\n\nMaintenant, effectuons le test apparié qui est approprié: Que conclure? Comment les résultats diffèrent-ils de la première analyse? Pourquoi?\n\nt.test(crane_h$width5, crane_h$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  crane_h$width5 and crane_h$width6\nt = -19.72, df = 14, p-value = 1.301e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2217521 -0.1782479\nsample estimates:\nmean difference \n           -0.2 \n\n\nLa première analyse a comme supposition que les deux échantillons de filles de 5 et 6 ans sont indépendants, alors que la deuxième analyse a comme supposition que la même fille a été mesurée deux fois, une fois à 5 ans, et la deuxième fois à 6 ans.\nNotez que, dans le premier cas, on accepte l’hypothèse nulle, mais que le test apparié rejette l’hypothèse nulle. Donc, le test qui est approprié (le test apparié) indique un effet très significatif de l’âge, mais le test inapproprié suggère que l’âge n’importe pas. C’est parce qu’il y a une très forte corrélation entre la largeur du visage à 5 et 6 ans:\n\ngraphcrane &lt;- ggplot(data = crane_h, aes(x = width5, y = width6)) +\n  geom_point() +\n  labs(x = \"Largeur du visage à 5 ans\", y = \"Largeur du visage à 6 ans\") +\n  geom_smooth() +\n  scale_fill_continuous(low = \"lavenderblush\", high = \"red\")\n\ngraphcrane\n\n\n\n\n\n\nFigure 10.5: Relation entre la largeur du visage à 5 et 6 ans.\n\n\n\n\nAvec r = 0.9930841. En présence d’une si forte corrélation, l’erreur-type de la différence appariée de largeur du visage entre 5 et 6 ans est beaucoup plus petite que l’erreur-type de la différence entre la largeur moyenne à 5 ans et la largeur moyenne à 6 ans. Par conséquent, la statistique t associée est beaucoup plus élevée pour le test apparié, la puissance du test est plus grande, et la valeur de p plus petite.\n\nRépétez l’analyse en utilisant l’alternative non paramétrique, le test Wilcoxon rang-signés (signed-rank). (Que concluez-vous?\n\n\nwilcox.test(crane_h$width5, crane_h$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(crane_h$width5, crane_h$width6, alternative =\n\"two.sided\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  crane_h$width5 and crane_h$width6\nV = 0, p-value = 0.0007193\nalternative hypothesis: true location shift is not equal to 0\n\n\nDonc on tire la même conclusion qu’avec le test de t apparié et on conclue qu’il y a des différences significatives entre la taille des crânes de filles âgées de 5 et 6 ans (quelle surprise !).\nMais, attendez une minute ! On a utilisé des tests bilatéraux ici mais, compte tenu des connaissances sur la croissance des enfants, une hypothèse unilatérale serait préférable. Ceci peut être accommodé en modifiant l’option “alternative”. On utilise l’hypothèse alternative pour décider entre “less” ou “greater”. Ici, si il y a une différence, on s’attend à ce que width5 sera inférieur à width6, donc on utiliserait “less”.\n\nt.test(crane_h$width5, crane_h$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  crane_h$width5 and crane_h$width6\nt = -19.72, df = 14, p-value = 6.507e-12\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n       -Inf -0.1821371\nsample estimates:\nmean difference \n           -0.2 \n\nwilcox.test(crane_h$width5, crane_h$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(crane_h$width5, crane_h$width6, alternative =\n\"less\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  crane_h$width5 and crane_h$width6\nV = 0, p-value = 0.0003597\nalternative hypothesis: true location shift is less than 0\n\n\nPour estimer la puissance d’un test de t avec R, il faut utiliser la fonction power.t.test(). Il faut spécifier l’argument type = \"paired\", utiliser la moyenne et l’écart-type de la différence au sein des paires dans les arguments delta et sd.\n\ncrane_h$diff &lt;- crane_h$width6 - crane_h$width5\npower.t.test(n = 15,\n             delta = mean(crane_h$diff),\n             sd = sd(crane_h$diff),\n             type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 15\n          delta = 0.2\n             sd = 0.03927922\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#références",
    "href": "32-t_test.html#références",
    "title": "\n10  Comparaison de deux échantillons\n",
    "section": "\n10.6 Références",
    "text": "10.6 Références\nBumpus, H.C. (1898) The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. Biological Lectures, Woods Hole Biology Laboratory, Woods Hole, 11 th Lecture: 209 - 226.\nNewman, K.J. and H.V. Meredith. (1956) Individual growth in skele- tal bigonial diameter during the childhood period from 5 to 11 years of age. Amer. J. Anat. 99: 157 - 187.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "33-anova.html",
    "href": "33-anova.html",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "",
    "text": "11.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:\nlibrary(ggplot2)\nlibrary(car)\nlibrary(multcomp)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#set-ano",
    "href": "33-anova.html#set-ano",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\nmultcomp\ncar\n\n\nles fichiers de données\n\nDam10dat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#anova-à-un-critère-de-classification-et-comparaisons-multiples",
    "href": "33-anova.html#anova-à-un-critère-de-classification-et-comparaisons-multiples",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "\n11.2 ANOVA à un critère de classification et comparaisons multiples",
    "text": "11.2 ANOVA à un critère de classification et comparaisons multiples\nL’ANOVA à un critère de classification est l’analogue du test de t pour des comparaisons de moyennes de plus de deux échantillons. Les conditions d’application du test sont essentiellement les mêmes, et lorsque appliqué à deux échantillons ce test est mathématiquement équivalent au test de t.\nEn 1961-1962, le barrage Grand Rapids était construit sur la rivière Saskatchewan en amont de Cumberland House. On croit que durant la construction plusieurs gros esturgeons restèrent prisonniers dans des sections peu profondes et moururent. Des inventaires de la population d’esturgeons furent faits en 1954, 1958, 1965 et 1966. Au cours de ces inventaires, la longueur à la fourche (frklngth) furent mesurées (pas nécessairement sur chaque poisson cependant). Ces données sont dans le fichier Dam10dat.csv.\n\n11.2.1 Visualiser les données\n\nÀ partir des données, vous devez d’abord changer le type de donnée de la variable year, pour que R traite year comme une variable discontinue (factor) plutôt que continue.\n\n\nDam10dat &lt;- read.csv(\"data/Dam10dat.csv\")\nDam10dat$year &lt;- as.factor(Dam10dat$year)\nstr(Dam10dat)\n\n'data.frame':   118 obs. of  21 variables:\n $ year    : Factor w/ 4 levels \"1954\",\"1958\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ fklngth : num  45 50 39 46 54.5 49 42.5 49 56 54 ...\n $ totlngth: num  49 NA 43 50.5 NA 51.7 45.5 52 60.2 58.5 ...\n $ drlngth : logi  NA NA NA NA NA NA ...\n $ drwght  : num  16 20.5 10 17.5 19.7 21.3 9.5 23.7 31 27.3 ...\n $ rdwght  : num  24.5 33 15.5 28.5 32.5 35.5 15.3 40.5 51.5 43 ...\n $ sex     : int  1 1 1 2 1 2 1 1 1 1 ...\n $ age     : int  24 33 17 31 37 44 23 34 33 47 ...\n $ lfkl    : num  1.65 1.7 1.59 1.66 1.74 ...\n $ ltotl   : num  1.69 NA 1.63 1.7 NA ...\n $ ldrl    : logi  NA NA NA NA NA NA ...\n $ ldrwght : num  1.2 1.31 1 1.24 1.29 ...\n $ lrdwght : num  1.39 1.52 1.19 1.45 1.51 ...\n $ lage    : num  1.38 1.52 1.23 1.49 1.57 ...\n $ rage    : int  4 6 3 6 7 7 4 6 6 7 ...\n $ ryear   : int  1954 1954 1954 1954 1954 1954 1954 1954 1954 1954 ...\n $ ryear2  : int  1958 1958 1958 1958 1958 1958 1958 1958 1958 1958 ...\n $ ryear3  : int  1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ...\n $ location: int  1 1 1 1 1 1 1 1 1 1 ...\n $ girth   : logi  NA NA NA NA NA NA ...\n $ lgirth  : logi  NA NA NA NA NA NA ...\n\n\n\nEnsuite, visualisez les données comme dans le labo pour les tests de t. Créez un histogramme avec ligne de densité et un Box plot par année. Que vous révèlent ces données?\n\n\nmygraph &lt;- ggplot(Dam10dat, aes(x = fklngth)) +\n  labs(x = \"Fork length (cm)\") +\n  geom_density() +\n  geom_rug() +\n  geom_histogram(aes(y = ..density..),\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(Dam10dat$fklngth),\n      sd = sd(Dam10dat$fklngth)\n    ),\n    color = \"red\"\n  )\n\n# display graph, by year\nmygraph + facet_wrap(~year, ncol = 2)\n\n\n\n\n\n\nFigure 11.1: Distribution de la longueur des esturgeons par année\n\n\n\n\n\nboxplot(fklngth ~ year, data = Dam10dat)\n\n\n\n\n\n\nFigure 11.2: Boxplot de la longueur pas annéee\n\n\n\n\nIl semble que la taille des esturgeons est un peu plus petite après la construction du barrage, mais les données sont très variables et les effets ne sont pas parfaitement clairs. Il y a peut-être des problèmes de normalité avec les échantillons de 1954 et 1966, et il y a probablement des valeurs extrêmes dans les échantillons de 1958 et 1966. On va continuer en testant les conditions d’application de l’ANOVA. Il faut d’abord faire l’analyse et examiner les résidus.\n\n11.2.2 Vérifier les conditions d’application de l’ANOVA paramétrique\nL’ANOVA paramétrique a trois conditions principales d’application :\n\nles résidus sont normalement distribués,\nla variance des résidus est égale dans tous les traitements (homoscédasticité) et\nles résidus sont indépendants les uns des autres.\n\nCes conditions doivent être remplies avant qu’on puisse se fier aux résultats de l’ANOVA paramétrique.\n\nFaites une ANOVA à un critère de classification sur fklngth par année et produisez les graphiques diagnostiques\n\n\n# Fit anova model and plot residual diagnostics\nanova.model1 &lt;- lm(fklngth ~ year, data = Dam10dat)\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\n\n\n\nFigure 11.3: Conditions d’applications de l’ANOVA\n\n\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nFaire attention dans le cadre d’une ANOVA à ce que la variable indépendante soit bien un facteur factor. Si la variable indépendante est reconnu comme du texte character alors vous n’obtiendrez que 3 graphiques et un message d’erreur du type:\n`hat values (leverages) are all = 0.1\nand there are no factor predictors; no plot no. 5`\n\n\nD’après les graphiques, on peut douter de la normalité et de l’homogénéité des variances. Notez qu’il y a un point qui ressort vraiment avec une forte valeur résiduelle (cas numéro 59) et qu’il ne s’aligne pas bien avec les autres valeurs: c’est la valeur extrême qui avait été détectée plus tôt. Ce point fera sans doute gonfler la variance résiduelle du groupe auquel il appartient.\nDes tests formels nous confirmeront ou infirmeront nos conclusions faites à partir de ces graphiques.\n\nFaites un test de normalité sur les résidus de l’ANOVA.\n\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.91571, p-value = 1.63e-06\n\n\nCe test confirme nos soupçons: les résidus ne sont pas distribués normalement. Il faut cependant garder à l’esprit que la puissance est grande et que même de petites déviations de la normalité sont suffisantes pour rejeter l’hypothèse nulle.\n\nEnsuite, éprouvez l’hypothèse d’égalité des variances (homoscedasticité):\n\n\nleveneTest(fklngth ~ year, data = Dam10dat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  2.8159 0.04234 *\n      114                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa valeur de p vous dit que vous pouvez rejeter l’hypothèse nulle qu’il n’y a aucune différence dans les variances entre les années. Alors, nous concluons que les variances ne sont pas homogènes.\n\n11.2.3 Faire l’ANOVA\n\nFaites une ANOVA de fklnght en choisissant / en présumant pour l’instant que les conditions d’application sont suffisamment remplies. Que concluez-vous?\n\n\nsummary(anova.model1)\n\n\nCall:\nlm(formula = fklngth ~ year, data = Dam10dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2116  -2.6866  -0.7116   2.2103  26.7885 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.0243     0.8566  56.061  &lt; 2e-16 ***\nyear1958      0.1872     1.3335   0.140  0.88859    \nyear1965     -5.5077     1.7310  -3.182  0.00189 ** \nyear1966     -3.3127     1.1684  -2.835  0.00542 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.211 on 114 degrees of freedom\nMultiple R-squared:  0.1355,    Adjusted R-squared:  0.1128 \nF-statistic: 5.957 on 3 and 114 DF,  p-value: 0.0008246\n\n\n\n\nCoefficients: Estimates Les 4 coefficients peuvent être utilisés pour obtenir les valeurs prédites par le modèle (i.e. les moyennes de chaque groupe). La fklngth moyenne de la première année (1954) est 48.0243. Les coefficients pour les 3 autres années sont la différence entre la moyenne de l’année en question et la moyenne de 1954. La moyenne pour 1965 est 48.0243-5.5077=42.5166. Pour chaque coefficient, on a également accès à l’erreur-type, une valeur de t et la probabilité qui lui est associée (H0 que le coefficient est 0). Les poissons étaient plus petits après la construction du barrage qu’en 1954. Vous devez prendre ces p-valeurs avec un grain de sel, car elles ne sont pas corrigées pour les comparaisons multiples et. En général, je porte peu d’attention à cette partie des résultats imprimés et me concentre sur ce qui suit.\n\nResidual standard error: La racine carrée de la variance des résidus (valeurs observées moins valeurs prédites) qui correspond à la variabilité inexpliquée par le modèle (variation de la taille des poissons capturés la même année).\n\nMutiple R-squared Le R-carré est la proportion de la variabilité de la variable dépendante qui peut être expliquée par le modèle. Ici, le modèle explique 13.5% de la variabilité. Les différences de taille d’une année à l’autre sont relativement petites lorsqu’on les compare à la variation de taille entre les poissons capturés la même année.\n\n\n\nF-Statistic La p-valeur associée au test “omnibus” que toutes les moyennes sont égales. Ici, p est beaucoup plus petit que 0.05 et on rejetterait H0 pour conclure que fklngth varie selon les années.\n\nLa commande anova() produit le tableau d’ANOVA standard qui contient la plupart de cette information:\n\nanova(anova.model1)\n\nAnalysis of Variance Table\n\nResponse: fklngth\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nyear        3  485.26 161.755  5.9574 0.0008246 ***\nResiduals 114 3095.30  27.152                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa variabilité totale de fklngth, mesurée par la somme des carrés des écarts (Sum sq) est partitionnée en ce qui peut être expliqué par l’année (485.26) et la variabilité résiduelle inexpliquée (3095.30). L’année explique bien (485.26/(3095.30+485.26)=.1355 or 13.55% de la variabilité). Le carré moyen des résidus (Residual Mean Sq) est leur variance.\n\n11.2.4 Les comparaisons multiples\n\nLa fonction pairwise.t.test() peut être utilisée pour comparer des moyennes et ajuster (ou non, si désiré) les probabilités pour le nombre de comparaisons en utilisant l’une des options pour p.adj:\n\nCompare toutes les moyennes sans ajuster les probabilités\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"none\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0019 0.0022 -     \n1966 0.0054 0.0079 0.1996\n\nP value adjustment method: none \n\n\nOption bonf ajuste les p-valeurs avec la correction de Bonferroni. Ici, il y a 6 valeurs de p calculées, et la correction de Bonferroni revient à simplement multiplier la p-valeur par 6 (sauf si le résultat est supérieur à 1. Si tel est le cas, la p-value ajustée est 1).\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"bonf\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954  1958  1965 \n1958 1.000 -     -    \n1965 0.011 0.013 -    \n1966 0.033 0.047 1.000\n\nP value adjustment method: bonferroni \n\n\nOption “holm” is est la correction séquentielle de Bonferroni dans laquelle les p-valeurs sont ordonnées de (i=1) la plus faible à (N) la plus grande. La correction pour les p-valeurs est (N-i+1). Ici, il y a N=6 paires de moyennes qui sont comparées. La plus petite valeur de p non corrigée est 0.0019 pour 1954 vs 1965. La p-valeur corrigée est donc \\(0.0019*(6-1+1)=0.011\\). La seconde plus petite p-valeur est 0.0022. Sa p-valeur corrigée est 0.0022*(6-2+1)=0.011. Pour la p-valeur la plus élevée, la correction est (N-N+1)=1, donc la p-valeur corrigée est égale à la p-valeur brute.\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"holm\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954  1958  1965 \n1958 0.889 -     -    \n1965 0.011 0.011 -    \n1966 0.022 0.024 0.399\n\nP value adjustment method: holm \n\n\nL’option “fdr” sert à contrôler le “false discovery rate”.\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"fdr\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0066 0.0066 -     \n1966 0.0108 0.0119 0.2395\n\nP value adjustment method: fdr \n\n\nLes quatre méthodes mènent ici à la même conclusion: les poissons sont plus gros après la construction du barrage et toutes les comparaisons entre les années 50 et 60 sont significatives alors que les différences entre 54 et 58 ou 65 et 66 ne le sont pas. La conclusion ne dépend pas du choix de méthode.\nDans d’autres situations, vous pourriez obtenir des résultats contradictoires. Alors, quelle méthode choisir? Les p-valeurs qui ne sont pas corrigées sont certainement suspectes lorsqu’il y a plusieurs comparaisons. D’un autre coté, la correction de Bonferroni est conservatrice et le devient encore plus lorsqu’il y a de très nombreuses comparaisons. Des travaux récents suggèrent que la correction fdr est un bon compromis lorsqu’il y a beaucoup de comparaisons.\nLa méthode de Tukey est l’une des plus populaires et est facile à utiliser en R (notez cependant qu’il y a un petit bug qui se manifeste quand la variable indépendante peut ressembler à un nombre plutôt qu’un facteur, ce qui explique la petite pirouette avec paste0 dans le code):\n\nDam10dat$myyear &lt;- as.factor(paste0(\"m\", Dam10dat$year))\nTukeyHSD(aov(fklngth ~ myyear, data = Dam10dat))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fklngth ~ myyear, data = Dam10dat)\n\n$myyear\n                  diff        lwr        upr     p adj\nm1958-m1954  0.1872141  -3.289570  3.6639986 0.9990071\nm1965-m1954 -5.5076577 -10.021034 -0.9942809 0.0100528\nm1966-m1954 -3.3126964  -6.359223 -0.2661701 0.0274077\nm1965-m1958 -5.6948718 -10.436304 -0.9534397 0.0116943\nm1966-m1958 -3.4999106  -6.875104 -0.1247171 0.0390011\nm1966-m1965  2.1949612  -2.240630  6.6305526 0.5710111\n\n\n\nplot(TukeyHSD(aov(fklngth ~ myyear, data = Dam10dat)))\n\n\n\n\n\n\nFigure 11.4: Différence anuelles dans la longueur des esturgeons\n\n\n\n\nLes intervalles de confiance, corrigés pour les comparaisons multiples par la méthode de Tukey, sont illustrés pour les différences entre années. Malheureusement les légendes ne sont pas complètes, mais l’ordre est le même que dans le tableau précédent.\nLe package multcomp peut produire de meilleurs graphiques, mais requiert un peu plus de code:\n\n# Alternative way to compute Tukey multiple comparisons\n# set up a one-way ANOVA\nanova.fkl.vs.year &lt;- aov(aov(fklngth ~ myyear, data = Dam10dat))\n# set up all-pairs comparisons for factor `year'\n\nmeandiff &lt;- glht(anova.fkl.vs.year, linfct = mcp(\n  myyear =\n    \"Tukey\"\n))\nconfint(meandiff)\n\n\n     Simultaneous Confidence Intervals\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = aov(fklngth ~ myyear, data = Dam10dat))\n\nQuantile = 2.589\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n                   Estimate lwr      upr     \nm1958 - m1954 == 0   0.1872  -3.2652   3.6396\nm1965 - m1954 == 0  -5.5077  -9.9894  -1.0259\nm1966 - m1954 == 0  -3.3127  -6.3378  -0.2875\nm1965 - m1958 == 0  -5.6949 -10.4030  -0.9867\nm1966 - m1958 == 0  -3.4999  -6.8514  -0.1484\nm1966 - m1965 == 0   2.1950  -2.2095   6.5994\n\nplot(meandiff)\n\n\n\n\n\n\nFigure 11.5: Différence anuelles dans la longueur des esturgeons\n\n\n\n\nC’est un peu mieux, mais ce qui le serait encore plus c’est un graphique des moyennes, avec leurs intervalles de confiance ajustés pour les comparaisons multiples:\n\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(\n  anova.fkl.vs.year,\n  linfct = mcp(myyear = \"Tukey\")\n)\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\n# plot\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\nplot(cimeans)\n\n\n\n\n\n\nFigure 11.6: Différence anuelles dans la longueur des esturgeons\n\n\n\n\nNotez les lettres au dessus du graphique: les années étiquetées avec la même lettre ne diffèrent pas significativement l’une de l’autre.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#transformations-de-données-et-anova-non-paramétrique",
    "href": "33-anova.html#transformations-de-données-et-anova-non-paramétrique",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "\n11.3 Transformations de données et ANOVA non-paramétrique",
    "text": "11.3 Transformations de données et ANOVA non-paramétrique\nDans l’exemple précédent sur les différences annuelles de la variable fklgnth, on a noté que les conditions d’application de l’ANOVA n’étaient pas remplies. Si les données ne remplissent pas les conditions de l’ANOVA paramétrique, il y a 3 options :\n\nNe rien faire. Si les effectifs dans chaque groupe sont grands, on peut relaxer les conditions d’application car l’ANOVA est alors assez robuste aux violations de normalité (mais moins aux violations d’homoscedasticité),\non peut transformer les données\non peut faire une analyse non-paramétrique.\n\n\nRefaites l’ANOVA de la section précédente après avoir transformé en faisant le logarithme à la base de 10. Avec les données transformées, est-ce que les problèmes qui avaient été identifiés disparaissent ?\n\n\n# Fit anova model on log10 of fklngth and plot residual diagnostics\npar(mfrow = c(2, 2))\nanova.model2 &lt;- lm(log10(fklngth) ~ year, data = Dam10dat)\nplot(anova.model2)\n\n\n\n\n\n\nFigure 11.7: Conditions d’application de l’ANOVA\n\n\n\n\nLes graphiques diagnostiques des résidus donnent:\nLes graphiques sont à peine mieux ici. Si on fait le test Wilk-Shapiro sur les résidus, on obtient:\n\nshapiro.test(residuals(anova.model2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model2)\nW = 0.96199, p-value = 0.002048\n\n\nAlors, on a toujours des problèmes avec la normalité et on est juste sur le seuil de décision pour l’égalité des variances. Vous avez le choix à ce point:\n\nessayer de trouver une autre transformation pour mieux rencontrer les conditions d’application\nassumer que les données sont rencontrent suffisamment les conditions d’application\nfaire une ANOVA non-paramétrique.\n\n\nL’analogue non-paramétrique de l’ANOVA à un critère de classification le plus employé est le test de Kruskall-Wallis. Faites ce test sur fklngth et comparez les résultats à ceux de l’analyse paramétrique. Que concluez-vous?\n\n\nkruskal.test(fklngth ~ year, data = Dam10dat)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fklngth by year\nKruskal-Wallis chi-squared = 15.731, df = 3, p-value = 0.001288\n\n\nLa conclusion est donc la même qu’avec l’ANOVA paramétrique: on rejette l’hypothèse nulle que le rang moyen est le même pour chaque année. Donc, même si les conditions d’application de l’analyse paramétrique n’étaient pas parfaitement rencontrées, les conclusions sont les mêmes, ce qui illustre la robustesse de l’ANOVA paramétrique.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#examen-des-valeurs-extrêmes",
    "href": "33-anova.html#examen-des-valeurs-extrêmes",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "\n11.4 Examen des valeurs extrêmes",
    "text": "11.4 Examen des valeurs extrêmes\nVous devriez avoir remarqué au cours des analyses précédentes qu’il y avait peut-être des valeurs extrêmes dans les données. Ces points étaient évidents dans le Box Plot de fklngth by year et ont été notés comme les points 59, 23, et 87 dans les diagrammes de probabilité des résidus et dans le diagramme de dispersion des résidus et des valeurs estimées. En général, vous devez avoir de très bonnes raisons pour enlever des valeurs extrêmes de la base de données (i.e. vous savez qu’il y a eu une erreur avec un cas). Cependant, il est quand même toujours valable de voir comment l’analyse change en enlevant des valeurs extrêmes de la base de données.\n\nRépétez l’ANOVA originale sur fklngth et year mais faites le avec un sous-ensemble de données sans les valeurs extrêmes. Est-ce que les conclusions ont changé?\n\n\nDamsubset &lt;- Dam10dat[-c(23, 59, 87), ] # removes obs 23, 59 and 87\naov.Damsubset &lt;- aov(fklngth ~ as.factor(year), Damsubset)\nsummary(aov.Damsubset)\n\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(year)   3  367.5  122.50   6.894 0.000267 ***\nResiduals       111 1972.4   17.77                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nshapiro.test(residuals(aov.Damsubset))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(aov.Damsubset)\nW = 0.98533, p-value = 0.2448\n\n\n\nleveneTest(fklngth ~ year, Damsubset)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   3  4.6237 0.004367 **\n      111                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’élimination de trois valeurs extrêmes améliore un peu les choses, mais ce n’est pas parfait. On a toujours une problème avec les variances, mais les résidus sont maintenant normaux. Cependant, le fait que la conclusion qu’on tire de l’ANOVA originale ne change pas en enlevant les points renforce le fait qu’on n’a pas une bonne raison pour enlever les points.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#test-de-permutation",
    "href": "33-anova.html#test-de-permutation",
    "title": "\n11  ANOVA à un critère de classification\n",
    "section": "\n11.5 Test de permutation",
    "text": "11.5 Test de permutation\nCommande R pour un test de permutation d’une ANOVA à un critère de classification.\n\n#############################################################\n# Permutation Test for one-way ANOVA\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html\n# set desired number of permutations\nnreps &lt;- 500\n# to simplify reuse of this code, copy desired dataframe to mydata\nmydata &lt;- Dam10dat\n# copy model formula to myformula\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# copy dependent variable vector to mydep\nmydep &lt;- mydata$fklngth\n# copy independent variable vector to myindep\nmyindep &lt;- as.factor(mydata$year)\n################################################\n# You should not need to modify code chunk below\n################################################\n# Compute observed F value for original sample\nmod1 &lt;- lm(myformula, data = mydata) # Standard Anova\nANOVA &lt;- summary(aov(mod1)) # Save summary to variable\nobservedF &lt;- ANOVA[[1]]$\"F value\"[1] # Save observed F value\n# Print standard ANOVA results\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\n\nprint(ANOVA, \"\\n\")\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in Manly with unrestricted sampling of observations. \")\n\n# Now start resampling\nFboot &lt;- numeric(nreps) # initalize vector to receive permuted\nvalues\nFboot[1] &lt;- observedF\nfor (i in 2:nreps) {\n  newdependent &lt;- sample(mydep, length(mydep)) # randomize dep\n  var\n  mod2 &lt;- lm(newdependent ~ myindep) # refit model\n  b &lt;- summary(aov(mod2))\n  Fboot[i] &lt;- b[[1]]$\"F value\"[1] # store F stats\n}\npermprob &lt;- length(Fboot[Fboot &gt;= observedF]) / nreps\ncat(\n  \" The permutation probability value is: \", permprob,\n  \"\\n\"\n)\n# end of code chunk for permutation\n\nVersion lmPerm du test de permutation.\n\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- Dam10dat\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html",
    "href": "34-anova_mult.html",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "",
    "text": "12.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:\nlibrary(multcomp)\nlibrary(car)\nlibrary(tidyverse)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#set-anomul",
    "href": "34-anova_mult.html#set-anomul",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "",
    "text": "les paquets R:\n\nmulticomp\ncar\ntidyverse\n\n\nles fichiers de données\n\nStu2wdat.csv\nStu2mdat.csv\nnr2wdat.csv\nnestdat.csv\nwmcdat2.csv\nwmc2dat2.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-et-réplication",
    "href": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-et-réplication",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.2 Plan factoriel à deux facteurs de classification et réplication",
    "text": "12.2 Plan factoriel à deux facteurs de classification et réplication\nIl est fréquent de vouloir analyser l’effet de plusieurs facteurs simultanément. L’ANOVA factorielle à deux critères de classification permet d’examiner deux facteurs à la fois, mais la même approche peut être utilisée pour 3, 4 ou même 5 facteurs quoique l’interprétation des résultats devienne beaucoup plus complexe.\nSupposons que vous êtes intéressés par l’effet de deux facteurs : site (location, Cumberland House ou The Pas) et sexe (sex, mâle ou femelle) sur la taille des esturgeons. Comme l’effectif n’est pas le même pour tous les groupes, c’est un plan qui n’est pas balancé. Notez aussi qu’il y a des valeurs manquantes pour certaines variables, ce qui veut dire que chaque mesure n’a pas été effectuée sur chaque poisson.\n\n12.2.1 ANOVA à effets fixes\n\nExaminez d’abord les données en faisant des box plots de rdwght pour sex et location des données du fichier Stu2wdat.csv .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nStu2wdat &lt;- read.csv(\"data/Stu2wdat.csv\")\n\nggplot(Stu2wdat, aes(x = sex, y = rdwght)) +\ngeom_boxplot(notch = TRUE) +\nfacet_grid(~location)\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\nFigure 12.1\n\n\n\n\n\n\n\nLes graphiques montrent qu’aux deux sites les femelles sont probablement plus grandes que les mâles, mais que les tailles ne varient pas beaucoup d’un site à l’autre. La présence de valeurs extrêmes sur ces graphiques suggère qu’il y aura peut-être des problèmes avec la condition de normalité des résidus.\n\nGénérez les statistiques sommaires pour RDWGHT par sex et Location.\n\n\nStu2wdat %&gt;%\n  group_by(sex, location) %&gt;%\n  summarise(\n    mean = mean(rdwght, na.rm = TRUE), sd = sd(rdwght, na.rm = TRUE), n = n()\n  )\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 5\n# Groups:   sex [2]\n  sex            location        mean    sd     n\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 \"FEMALE      \" \"CUMBERLAND  \"  27.4  9.33    51\n2 \"FEMALE      \" \"THE_PAS     \"  28.0 12.5     55\n3 \"MALE        \" \"CUMBERLAND  \"  22.1  4.79    34\n4 \"MALE        \" \"THE_PAS     \"  20.6  9.92    46\n\n\nCes résultats supportent l’interprétation des box plots: Les femelles sont plus grosses que les mâles, et la différence de taille entre les deuxsites sont petites.\n\nÀ l’aide du fichier Stu2wdat.csv faites une ANOVA factorielle à deux critères de classification:\n\n\n# Fit anova model and plot residual diagnostics\n# but first, save current graphic parameters\nopar &lt;- par()\nanova.model1 &lt;- lm(rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2wdat\n)\nanova(anova.model1)\n\nAnalysis of Variance Table\n\nResponse: rdwght\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1  1839.6 1839.55 18.6785 2.569e-05 ***\nlocation       1     4.3    4.26  0.0433    0.8355    \nsex:location   1    48.7   48.69  0.4944    0.4829    \nResiduals    178 17530.4   98.49                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention, R imprime les sommes des carrés séquentielles (Type I) les carrés moyens et probabilités associés. Vous ne pouvez pas vous y fier si votre plan d’expérience n’est pas parfaitement balancé. Dans cet exemple, le nombre de poissons capturés change selon le site et le sexe et le plan d’expérience n’est donc pas balancé.\n\n\nVous devez extraire les sommes de carrés partielles (Type III). Le moyen le plus simple que j’ai trouvé est d’utiliser la fonction Anova()du package car (notez la différence subtile, Anova() n’est pas la même chose que anova(), R est impitoyable et distingue les majuscules des minuscules). Malheureusement, Anova() ne suffit pas; il faut également spécifier le type de contraste dans le modèle avecl’argument contrasts = list(sex = contr.sum,location = contr.sum)\n\nlibrary(car)\nAnova(anova.model1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSuite à l’ANOVA, on accepte deux hypothèses nulles: (1) que l’effet du sexe ne varie pas entre les sites (pas d’interaction significative) et (2) qu’il n’y a pas de différence de taille des esturgeons (peu importe le sexe) entre les deux sites. D’un autre coté, on rejette l’hypothèse nulle qu’il n’y a pas de différence de taille entre les esturgeons mâles et les femelles, tel que suggéré par les graphiques.\n\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\n\n\n\nFigure 12.2: Conditions d’application ANOVA model1\n\n\n\n\nCependant, on ne peut se fier à ces résultats sans vérifier si les conditions d’application de l’ANOVA étaient remplies. Un examen des graphiques des résidus, en haut, montre que les résidus semblent être distribués plus ou moins normalement, si ce n’est des 3 valeurs extrêmes qui sont notées sur le diagramme de probabilité (cas 101, 24,& 71). D’après le graphique des résidus vs les valeurs prédites, on voit que l’étendue des résidus est plus ou moins égale pour les valeurs estimées, sauf encore pour 2 ou 3 cas. Si on éprouve la normalité, on obtient:\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.87213, p-value = 2.619e-11\n\n\nAlors, il y a évidence que les résidus ne sont pas distribués normalement.\nNous allons utiliser le test de Levene pour examiner l’homoscédasticité des résidus, de la même façon qu’on a fait pour l’ANOVA à un critère de classification.\n\nlibrary(car)\nleveneTest(rdwght ~ sex * location, data = Stu2wdat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  3.8526 0.01055 *\n      178                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi les résidus étaient homoscédastiques, on accepterait l’hypothèse nulle que le absres moyen ne varie pas entre les niveaux de sexe et location (i.e., sexloc). Le tableau d’ANOVA ci-dessus montre que l’hypothèse est rejetée. Il y a donc évidence d’hétéroscédasticité. En bref, nous avons donc plusieurs conditions d’application qui ne sont pas respectées. La question qui reste est: ces violations sont-elles suffisantes pour invalider nos conclusions ?\n\n\n\n\n\n\nExercice\n\n\n\nRépétez la même analyse avec les données du fichier stu2mdat.csv . Que concluez-vous? Supposons que vous vouliez comparer la taille des mâles et des femelles. Comment cette comparaison diffère entre les deux ensembles de données ?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nStu2mdat &lt;- read.csv(\"data/Stu2mdat.csv\")\nanova.model2 &lt;- lm(\n  formula = rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2mdat\n)\nsummary(anova.model2)\nAnova(anova.model2, type = 3)\n\n\n\n\n\n\n\nCall:\nlm(formula = rdwght ~ sex + location + sex:location, data = Stu2mdat, \n    contrasts = list(sex = contr.sum, location = contr.sum))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.917  -6.017  -0.580   4.445  65.743 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     24.5346     0.7461  32.885  &lt; 2e-16 ***\nsex1            -0.5246     0.7461  -0.703    0.483    \nlocation1        0.2227     0.7461   0.299    0.766    \nsex1:location1   3.1407     0.7461   4.210 4.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.924 on 178 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.09744,   Adjusted R-squared:  0.08223 \nF-statistic: 6.405 on 3 and 178 DF,  p-value: 0.0003817\n\n\nNotez que cette fois les femelles sont plus grandes que les mâles à Cumberland House, mais que c’est le contraire à The Pas. Quel est le résultat de l’ANOVA (n’oubliez pas qu’il faut des Type III sums of squares pour les résultats)?\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex              49   1    0.4944    0.4829    \nlocation          9   1    0.0891    0.7656    \nsex:location   1745   1   17.7220 4.051e-05 ***\nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDans ce cas, le terme de l’interaction (sex:location) est maintenant significatif mais les effets principaux ne le sont pas.\n\nVous trouverez utile ici de créer des graphiques pour les deux fichiers de données pour comparer les interactions entre sex et location. Le graphique d’interaction montre les relations entre les moyennes de chaque combinaison de facteurs (appelées aussi les moyennes des ce$lules).Générez un graphique illustrant les intéractions en utilisant la fonction allEffects du package effects :\n\n\nlibrary(effects)\nallEffects(anova.model1)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     27.97717\n  MALE             22.14118     20.64652\n\nplot(allEffects(anova.model1), \"sex:location\")\n\n\n\n\n\n\nFigure 12.3: Effet du sexe et du lieu sur le poids des esturgeons\n\n\n\n\n\nallEffects(anova.model2)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     20.64652\n  MALE             22.14118     27.97717\n\nplot(allEffects(anova.model2), \"sex:location\")\n\n\n\n\n\n\nFigure 12.4: Effet du sexe et du lieu sur le poids des esturgeons\n\n\n\n\nIl y a une différence importante entre les résultats obtenus avec stu2wdat et stu2mdat. Dans le premier cas, puisqu’il n’y a pas d’interaction, on peut regrouper les données des deux niveaux d’un facteur (le site, par exemple) pour éprouver l’hypothèse d’un effet de l’autre facteur (le sexe). En fait, si on fait cela et que l’on calcule une ANOVA à un critère de classification (sex), on obtient:\n\nAnova(aov(rdwght ~ sex, data = Stu2wdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)  78191   1 800.440 &lt; 2.2e-16 ***\nsex           1840   1  18.831 2.377e-05 ***\nResiduals    17583 180                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotez que la somme des carrés des résidus (17583) est presque égale à celle du modèle complet (17530) de l’ANOVA factorielle à deux facteurs. C’est parce que dans cette anova factorielle, le terme d’interaction et le terme représentant l’effet du site n’expliquent qu’une partie infime de la variabilité. D’un autre coté, si on essaie le même truc avec stu2mdat, on obtient:\n\nAnova(aov(rdwght ~ sex, data = Stu2mdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df  F value Pr(&gt;F)    \n(Intercept)  55251   1 515.0435 &lt;2e-16 ***\nsex            113   1   1.0571 0.3053    \nResiduals    19309 180                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIci la somme des carrées des résidus (19309) est beaucoup plus grande que celle de l’ANOVA factorielle (175306) parce qu’une partie importante de la variabilité expliquée par le modèle est associée à l’interaction. Notez que si on n’avait fait que cette analyse, on conclurait que les esturgeons mâles et femelles ont la même taille. Mais en fait leur taille diffère; seulement la différence est à l’avantage des mâles à un site et à l’avantage des femelles à l’autre. Il est donc délicat d’interpréter l’effet principal (sexe) en présence d’une interaction significative…\n\n12.2.2 ANOVA à effets mixtes\nLes analyses qui précèdent négligent un point important: location pourrait être traité comme un facteur aléatoire et sex est fixe. Par conséquent le modèle approprié d’ANOVA est de type mixte.\nNotez que dans toutes les analyses qui précèdent, R a traité cette ANOVA comme si elle etait de type effet fixe seulement, et les termes principaux et celui d’interaction ont été testés en utilisant le carré moyen des résidus comme dénominateur des tests de F. Cependant, pour une ANOVA de type mixte, ces effets devraient être testés en utilisant le carré moyen du terme d’interaction, ou en combinant la somme des carrés de l’erreur et de l’interaction (selon le statisticien consulté!).\nEn utilisant Stu2wdat, refaites un tableau d’ANOVA pour RDWGHT en considérant location comme facteur aléatoire et sex comme un facteur fixe. Pour ce faire, vous devrez recalculer les valeurs de F pour sex et location en utilisant le carré moyen de l’interaction sex:location au lieu du carré moyen des résidus comme dénominateur. Le mieux c’est de le faire à la mitaine ent travaillant avec les Type III Sums of squares du tableau d’ANOVA.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAnova(anova.model1, type = 3)\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPour sex, la nouvelle valeur de F (le rapport des carrés moyens) est de\n\\[F = \\frac{(1745/1)}{(49/1)} = 35.6\\]\nPour obtenir la valeur de p correspondant à cette statistique F, il faut utilisez la fonction de probabilité de la distribtuion de F pf(F, df1, df2, lower.tail = FALSE), où F est la valeur de F calculée, et df1 et df2 sont les degrés de liberté du numérateur (sex) et dénominateur(SEX:location).\n\npf(35.6, 1, 1, lower.tail = FALSE)\n\n[1] 0.1057152\n\n\nNotez que maintenant la valeur de p pour sex n’est plus significative. C’est parce que le carré moyen de l’erreur dans l’ANOVA initiale est plus petit que celui associé à l’interaction, mais surtout parce que le nombre de degrés de liberté pour le dénominateur du test de F est passé de 178 à 1 seulement. En général, c’est beaucoup plus difficile d’obtenir des résultats significatifs quand les degrés de liberté pour le dénominateur sont petits.\n\n\n\n\n\n\nNote\n\n\n\nLes modèles mixtes qui sont une généralisation de l’ANOVA à effets mixtes sont maintenant extrêmement bien développé et sont à favoriser lors d’analyse incluant des effets dit aléatoires.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-sans-réplication",
    "href": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-sans-réplication",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.3 Plan factoriel à deux facteurs de classification sans réplication",
    "text": "12.3 Plan factoriel à deux facteurs de classification sans réplication\nDans certains plans d’expérience il n’y a pas de réplicats pour chaque combinaison de facteurs, par exemple parce qu’il serait trop coûteux de faire plus d’une observation. L’ANOVA à deux critères de classification est quand même possible dans ces circonstances, mais il y a une limitation importante.\n\n\n\n\n\n\nAvertissement\n\n\n\nComme il n’y a pas de réplicats, on ne peut estimer la variance du terme d’erreur. En effet on ne peut qu’estimer la somme des carrés associés à chacun des facteurs principaux, et la quantité de variabilité qui reste (Remainder Mean Square) représente la somme de la variabilité attribuable à l’interaction et au terme d’erreur. Cela a une implication importante. Dans le cas d’un modèle avec uniquement des effets fixes ou pour l’effet aléatoire d’un modèle d’ANOVA mixtes on ne peut tester les effets principaux que si on est sur qu’il n’y a pas d’interaction.\n\n\nUn limnologiste qui étudie Round Lake dans le Parc Algonquin prend une seule mesure de température (temp,en degrés C) à 10 profondeurs différentes (depth, en m) à quatre dates (date) au cours de l’été. Ses données sont au fichier nr2wdat.csv.\n\nEffectuez une ANOVA à deux critères de classification en utilisant temp comme variable dépendante et date et depth comme variables indépendantes (vous devez changer le type de données pour DEPTH pour que R traite cette variable comme un facteur et non pas une var$able continue).\n\n\nnr2wdat &lt;- read.csv(\"data/nr2wdat.csv\")\nnr2wdat$depth &lt;- as.factor(nr2wdat$depth)\nanova.model4 &lt;- lm(temp ~ date + depth, data = nr2wdat)\nAnova(anova.model4, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: temp\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 1511.99  1 125.5652 1.170e-11 ***\ndate         591.15  3  16.3641 2.935e-06 ***\ndepth       1082.82  9   9.9916 1.450e-06 ***\nResiduals    325.12 27                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi on suppose que c’est un modèle d’ANOVA mixte (date aléatoire, Depth fixe), que concluez vous? (Indice: faites un graphique d’interaction température en fonction de la profondeur et la date, pour voir ce qui se passe).\n\ninteraction.plot(nr2wdat$depth, nr2wdat$date, nr2wdat$temp)\n\n\n\n\n\n\nFigure 12.5: Effet du mois et de la profondeur sur la température\n\n\n\n\nLa température diminue significativement en profondeur. Pour tester l’effet du mois (le facteur aléatoire), on doit présumer qu’il n’y a pas d’interaction entre la profondeur et le mois (donc que l’effet de la profondeur sur la température est le même à chaque mois). C’est peu probable: si vous faites un graphique de la température en fonction de la profondeur pour chaque mois, vous observerez que le profil de température change au fur et à mesure du développement de la thermocline. Bref, comme le profil change au cours de l’été, ce modèle ne fait pas de très bonnes prédictions.\nJetez un coup d’oeil sur les graphiques des résidus:\n\npar(mfrow = c(2, 2))\nplot(anova.model4)\n\n\n\n\n\n\nFigure 12.6: Conditions d’applications du modèle anova.model4\n\n\n\n\n\nshapiro.test(residuals(anova.model4))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model4)\nW = 0.95968, p-value = 0.1634\n\n\nLe test de normalité sur les résidus donne p = 0.16, donc l’hypothèse de normalité ne semble pas être sérieusement en doute. Pour l’égalité des variances, on peut seulement comparer entre les mois en utilisant les profondeurs comme réplicats (ou l’inverse). En utilisant les profondeurs comme réplicats, on obtient:\n\nleveneTest(temp ~ date, data = nr2wdat)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  3  17.979 2.679e-07 ***\n      36                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl y a donc un problème d’hétéroscédasticité, comme on peut très bien voir dans le graphique des résidus vs les valeurs estimées. Cette analyse n’est donc pas très satisfaisante: il y a des violations des conditions d’application et il semble y avoir une interaction entre depth et date qui pourrait invalider l’analyse.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plans-hiérarchiques",
    "href": "34-anova_mult.html#plans-hiérarchiques",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.4 Plans hiérarchiques",
    "text": "12.4 Plans hiérarchiques\nUn design expérimental fréquent implique la division de chaque groupe du facteur majeur en sous-groupes aléatoires. Par exemple, une généticienne intéressée par l’effet du génotype sur la résistance à la dessiccation chez la drosophile effectue une expérience. Pour chaque génotype (facteur principal) elle prépare trois chambres de croissance (sous-groupes) avec une température et humidité contrôlées. Dans chaque chambre de croissance, elle place cinq larves, puis mesure le nombre d’heures pendant lesquelles chaque larve survit. Les données ont donc un structure hiérarchique. Il ya des observations répétées dans chaque chambre au sein de chaque génotype.\n\nLe fichier nestdat.csv contient les résultats d’une expérience semblable. Il contient trois variables : genotype, chamber et survival. Effectuez une ANOVA hiérarchique avec survival comme variable dépendante et genotype et chamber/genotype comme variables indépendantes.\n\n\nnestdat &lt;- read.csv(\"data/nestdat.csv\")\nnestdat$chamber &lt;- as.factor(nestdat$chamber)\nnestdat$genotype &lt;- as.factor(nestdat$genotype)\nanova.nested &lt;- lm(survival ~ genotype / chamber, data = nestdat)\n\nQue concluez-vous de cette analyse ? Que devrait être la prochaine étape ? (Indice: si l’effet de Chamber / genotype n’est pas significatif, vous pouvez augmenter la puissance des comparaisons entre génotypes en regroupant les chambres de chaque génotype.). Faites-le ! N’oubliez pas de vérifier les conditions d’applications de l’ANOVA!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanova(anova.nested)\n\nAnalysis of Variance Table\n\nResponse: survival\n                 Df  Sum Sq Mean Sq  F value Pr(&gt;F)    \ngenotype          2 2952.22 1476.11 292.6081 &lt;2e-16 ***\ngenotype:chamber  6   40.65    6.78   1.3432 0.2639    \nResiduals        36  181.61    5.04                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow = c(2, 2))\nplot(anova.nested)\n\n\n\n\n\n\nFigure 12.7: Conditions d’applications du modèle anova.nested\n\n\n\n\n\n\n\nOn conclue de cette analyse que la variation entre les chambres de croissance n’est pas significative, mais qu’on doit rejeter l’hypothèse nulle que tous les génotypes ont la même résistance à la dessiccation.\nComme l’effet hiérarchique chamber / genotype n’est pas significatif, on peut regrouper les observations pour augmenter le nombre de degrés de liberté:\n\nanova.simple &lt;- lm(survival ~ genotype, data = nestdat)\nanova(anova.simple)\n\nAnalysis of Variance Table\n\nResponse: survival\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ngenotype   2 2952.22 1476.11  278.93 &lt; 2.2e-16 ***\nResiduals 42  222.26    5.29                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDonc on conclue qu’il y a une variation significative de résistance à la dessiccation entre les trois génotypes.\nLe graphique de survival en fonction du génotype suggère que la résistance à la dessiccation varie entre chaque génotype. On peut combiner cela avec un test de Tukey.\n\npar(mfrow = c(1, 1))\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(anova.simple, linfct = mcp(\n  genotype =\n    \"Tukey\"\n))\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(cimeans, las = 1) # las option to put y-axis labels as God intended them\n\n\n\n\n\n\nFigure 12.8: Effet du genotype sur la résistance à la dessication avec un test de Tukey\n\n\n\n\nOn conclue donc que la résistance à la dessiccation (R), telle que mesurée par la survie dans des conditions chaudes et sèches, varie significativement entre les trois génotypes avec R(AA) &gt; R(Aa) &gt; R(aa).\nCependant, avant d’accepter cette conclusion, il faut éprouver les conditions d’application du test. Voici les diagnostics des résidus pour l’ANOVA à un critère de classification (non hiérarchique):\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.simple)\n\n\n\n\n\n\nFigure 12.9: Conditions d’applications du modèle anova.simple\n\n\n\n\n\n\n\nDonc, toutes les conditions d’application semblent être remplies, et on peut donc accepter les conclusions. Notez que si l’on compare le carré moyen des résidus de l’ANOVA hiérarchique et de l’ANOVA à un critère de classification (5.045 vs 5.292), ils sont presque identiques. Cela n’est pas surprenant compte tenu de la faible variabilité associée aux chambres de croissance pour chaque génotype.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#anova-non-paramétrique-avec-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#anova-non-paramétrique-avec-deux-facteurs-de-classification",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.5 ANOVA non paramétrique avec deux facteurs de classification",
    "text": "12.5 ANOVA non paramétrique avec deux facteurs de classification\nL’ANOVA non paramétrique à deux critères de classification est une extension de celle à un critère de classification vue précédemment. Elle débute par une ANOVA faite sur les données transformées en rangs. Elle peut se faire sur des données avec ou sans réplicats.\nÀ partir du fichier stu2wdat.csv, effectuez une ANOVA non paramétrique à deux facteurs de classification pour examiner l’effet de sex et location sur rank(rdwght).\n\naov.rank &lt;- aov(\n  rank(rdwght) ~ sex * location,\n  contrasts = list(\n    sex = contr.sum, location = contr.sum\n  ),\n  data = Stu2wdat\n)\n\nL’extension de Schreirer-Ray-Hare au test de Kruskall-Wallis se fait ensuite à la main. Il faut d’abord calculer la statistique H égale au rapport de la somme des carrées de l’effet testé, divisée par le carré moyen total. On calcule la statistique H pour chacun des termes. Les statistiques H sont ensuite comparées à une distribution théorique de \\(\\chi^2\\) (chi-carré) en utilisant la commande pchisq(H, df, lower.tail = FALSE), où H et df sont les statistiques H calculées et les degrés de libertés, respectivement.\nTestez l’effet de sex et location sur rdwght. Que concluez-vous ? Comment ce résultat se compare-t-il à celui obtenu en faisant l’ANOVA paramétrique faite précédemment ?\n\nAnova(aov.rank, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rank(rdwght)\n              Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept)  1499862   1 577.8673 &lt; 2.2e-16 ***\nsex            58394   1  22.4979 4.237e-06 ***\nlocation        1128   1   0.4347    0.5105    \nsex:location    1230   1   0.4738    0.4921    \nResiduals     472383 182                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPour calculer l’extension Schreirer-Ray-Hare au test de Kruskall-Wallis, on doit d’abord calculer le carré moyen total (MS), i.e. la variance des données transformées en rang. Ici, on a 186 observations, donc des rangs; 1, 2, 3, … 186. La variance de cette série de 186 valeurs peut être calculée simplement par var(1:186).\nDonc on peut calculer la statistique H pour chaque terme:\n\nHsex &lt;- 58394 / var(1:186)\nHlocation &lt;- 1128 / var(1:186)\nHsexloc &lt;- 1230 / var(1:186)\n\nEt convertir ces statistiques en valeur de ps:\n\n# sex\nHsex\n\n[1] 20.14628\n\npchisq(Hsex, 1, lower.tail = FALSE)\n\n[1] 7.173954e-06\n\n# location\nHlocation\n\n[1] 0.3891668\n\npchisq(Hlocation, 1, lower.tail = FALSE)\n\n[1] 0.5327377\n\n# sex:location\nHsexloc\n\n[1] 0.4243574\n\npchisq(Hsexloc, 1, lower.tail = FALSE)\n\n[1] 0.5147707\n\n\nCes résultats sont semblables aux résultats de l’ANOVA non-paramétrique à deux critères de classification. Malgré la puissance réduite, il y a encore un effet significatif du sexe, mais ni interaction ni effet du site.\nIl y a toutefois une différence importante. Rappelez-vous que dans l’ANOVA paramétrique il y avait un effet significatif de sex en considérant le problème comme un modèle ANOVA à effet fixe. Cependant, si on traite le problème comme un modèle d’ANOVA à effet mixte l’effet significatif de sex peut en principe disparaître parce que le nombre de degré de liberté (dl) associés au carré moyen (CM) de l’interaction est plus faible que le nombre de dl du CM de l’erreur du modèle à effet fixes. Dans ce cas ci, cependant, le CM de l’interaction est environ la moitié du CM de l’erreur. Par conséquent, l’effet significatif de sex pourrait devenir encore plus significatif si le problème est analysé (comme il se doit) comme une ANOVA mixte. Encore une fois on peut voir l’importance de spécifier le modèle adéquat en ANOVA.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#comparaisons-multiples",
    "href": "34-anova_mult.html#comparaisons-multiples",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.6 Comparaisons multiples",
    "text": "12.6 Comparaisons multiples\nLes épreuves d’hypothèses subséquentes en ANOVA à plus d’un critère de classification dépendent des résultats initiaux de l’ANOVA. Si vous êtes intéressés à comparer des effets moyens d’un facteur pour tous les niveaux d’un autre facteur (par exemple l’effet du sexe sur la taille des esturgeons peu importe d’où ils viennent), alors vous pouvez procéder exactement tel que décrit dans la section sur les comparaisons multiples suivant l’ANOVA à un critère de classification. Pour comparer les moyennes des cellules entre elles, il faut spécifier l’interaction comme variable qui représente le groupe.\nLe fichier wmcdat2.csv contient des mesures de consommation d’oxygène, o2cons, de deux espèces, species, d’un mollusque (une patelle) à trois concentrations différentes d’eau de mer, conc. Ces données sont présentées à la p. 332 de Sokal et Rohlf 1995.\n\nEffectuez une ANOVA factorielle à deux critères de classification sur ces données en utilisant o2cons comme variable dépendante et species et conc comme les facteurs (il va probablement falloir changer le type de données de variable conc à facteur). Que concluez-vous ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwmcdat2 &lt;- read.csv(\"data/wmcdat2.csv\")\nwmcdat2$species &lt;- as.factor(wmcdat2$species)\nwmcdat2$conc &lt;- as.factor(wmcdat2$conc)\nanova.model5 &lt;- lm(o2cons ~ species * conc, data = wmcdat2)\nAnova(anova.model5, type = 3)\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n              Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept)  1185.60  1 124.0165 4.101e-14 ***\nspecies         0.09  1   0.0097   0.92189    \nconc           74.90  2   3.9172   0.02755 *  \nspecies:conc   23.93  2   1.2514   0.29656    \nResiduals     401.52 42                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComme l’effectif dans chaque cellule est relativement petit, il faudrait idéalement refaire cette analyse avec une ANOVA non-paramétrique. Pour le moment, contentons nous de la version paramétrique.\nExaminons les graphiques diagnostiques:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.model5)\n\n\n\n\n\n\nFigure 12.10\n\n\n\n\n\n\n\nLes variances semblent donc égales. Le test de normalité donne:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nshapiro.test(residuals(anova.model5))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model5)\nW = 0.93692, p-value = 0.01238\n\n\n\n\n\nIl y a donc évidence de non-normalité, mais à part ça tout semble aller. Comme l’ANOVA est relativement robuste à la non-normalité, on va regarder de l’autre coté. (Si vous voulez être plus confiants, vous pouvez tourner une ANOVA non paramétrique. Vous arriverez aux mêmes conclusions.)\n\nÀ la suite des résultats que vous venez d’obtenir, quelles moyennes voudriez-vous comparer ? Pourquoi?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\najouter une explication ici\n\n\n\nOn conclue donc qu’il n’y a pas de différence entre les espèces et que l’effet de la concentration ne dépends pas de l’espèce (il n’y a pas d’interaction). Par conséquent, les seules comparaisons justifiables sont entre les concentrations:\n\n# fit simplified model\nanova.model6 &lt;- aov(o2cons ~ conc, data = wmcdat2)\n# Make Tukey multiple comparisons\nTukeyHSD(anova.model6)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = o2cons ~ conc, data = wmcdat2)\n\n$conc\n           diff       lwr        upr     p adj\n75-50  -4.63625 -7.321998 -1.9505018 0.0003793\n100-50 -3.25500 -5.940748 -0.5692518 0.0141313\n100-75  1.38125 -1.304498  4.0669982 0.4325855\n\npar(mfrow = c(1, 1))\n# Graph of all comparisons for conc\ntuk &lt;- glht(anova.model6, linfct = mcp(conc = \"Tukey\"))\n# extract information\ntuk.cld &lt;- cld(tuk)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk.cld)\npar(old.par)\n\n\n\n\n\n\nFigure 12.11: Comparaison de Tukey des moyennes de consommation d’oxygèn en fonction del la concentration\n\n\n\n\nIl y a donc une différence de consommation d’oxygène significative lorsque la salinité est réduite de 50%, mais pas à 25% de réduction.\n\nRépétez les deux analyses précédentes sur les données du fichier wmc2dat2.csv. Comment les résultats se comparent-ils à ceux obt$nus précédemment ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwmc2dat2 &lt;- read.csv(\"data/wmc2dat2.csv\")\nwmc2dat2$species &lt;- as.factor(wmc2dat2$species)\nwmc2dat2$conc &lt;- as.factor(wmc2dat2$conc)\nanova.model7 &lt;- lm(o2cons ~ species * conc, data = wmc2dat2)\n\n\n\n\nEn utilisant wmc2dat2.csv, on obtient:\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n             Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  343.09  1 36.2132 3.745e-07 ***\nspecies      133.52  1 14.0929 0.0005286 ***\nconc          66.76  2  3.5232 0.0385011 *  \nspecies:conc 168.15  2  8.8742 0.0006101 ***\nResiduals    397.91 42                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDans ce cas ci, il y a une interaction significative, et il n’est par conséquent pas approprié de comparer les moyennes regroupées par espèce ou concentration. Ceci est clairement visualisé par un graphique d’interaction:\n\nwith(wmc2dat2, interaction.plot(conc, species, o2cons))\n\n\n\n\n\n\n\n\nToujours en utilisant les données de wmc2dat2.csv, comparez les 6 moyennes avec l’ajustement Bonferonni. Pour ce faire, il sera utile de créer une nouvelle variable qui combine species et conc:\n\n\nwmc2dat2$species.conc &lt;- as.factor(paste0(wmc2dat2$species, wmc2dat2$conc))\n\nensuite on peut faire les comparaisons de Bonferroni:\n\nwith(wmc2dat2, pairwise.t.test(o2cons, species.conc, p.adj = \"bonf\"))\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  o2cons and species.conc \n\n     A100   A50    A75    B100   B50   \nA50  0.1887 -      -      -      -     \nA75  1.0000 1.0000 -      -      -     \nB100 0.7223 1.0000 1.0000 -      -     \nB50  1.0000 0.0079 0.0929 0.0412 -     \nB75  0.6340 1.0000 1.0000 1.0000 0.0350\n\nP value adjustment method: bonferroni \n\n\nCes comparaisons sont un peu plus difficiles à interpréter, mais l’analyse examine essentiellement les différences entre les concentrations de l’eau dans l’espèce A (nommé adj1) et pour les différences entre les concentrations dans l’espèce B (nommé adj2). Cette analyse indique que la différence principale est entre la concentration de 50% pour l’espèce B et les concentrations de 75 et 100% de l’espèce B, tandis qu’il n’y a aucunes différences significatives pour l’espèce A.\nJe trouve ces tableaux de résultats peu satisfaisants parce qu’ils indiquent seulement les valeur de ps sans indices de la taille de l’effet. On peut obtenir à la fois le résultat des tests de comparaison multiple et un indice de la taille de l’effet à l’aide du code suivant:\n\n# fit one-way anova comparing all combinations of species.conc combinations\nanova.modelx &lt;- aov(o2cons ~ species.conc, data = wmc2dat2)\ntuk2 &lt;- glht(anova.modelx, linfct = mcp(species.conc = \"Tukey\"))\n# extract information\ntuk2.cld &lt;- cld(tuk2)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk2.cld)\npar(old.par)\n\n\n\n\n\n\nFigure 12.12\n\n\n\n\nDans cette analyse on a utilisé le CM = 9.474 du modèle d’ANOVA pour comparer les moyennes. En ce faisant, on présume qu’il s’agit d’une situation d’ANOVA à effet fixes, ce qui n’est peut-être pas le cas (conc est certainement fixe, mais species peut être fixe ou aléatoire).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#test-de-permutation-pour-lanova-à-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#test-de-permutation-pour-lanova-à-deux-facteurs-de-classification",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.7 Test de permutation pour l’ANOVA à deux facteurs de classification",
    "text": "12.7 Test de permutation pour l’ANOVA à deux facteurs de classification\nQuand les données ne rencontrent pas les conditions d’application des tests paramétriques d’ANOVA à un ou plusieurs facteurs de classification, il est possible d’utiliser les tests de permutation comme une alternative aux tests non-paramétriques pour calculer des p-valeurs. Le code suivant est pour un modèle I d’une ANOVA à deux facteurs de classification. Je vous laisse le soin d’adapter ce code pour d’autres modèles. (J’offre même des points boni pour une solution élégante pour des modèles à plusieurs facteurs de classification).\n\n###########################################################\n# Permutation test for two way ANOVA\n# Ter Braak creates residuals from cell means and then permutes across\n# all cells\n# This can be accomplished by taking residuals from the full model\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html\nnreps &lt;- 500\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nmod &lt;- lm(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\nres &lt;- mod$residuals\nTBint &lt;- numeric(nreps)\nTB1 &lt;- numeric(nreps)\nTB2 &lt;- numeric(nreps)\nANOVA &lt;- summary(aov(mod))\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\nF1 &lt;- ANOVA[[1]]$\"F value\"[1]\nF2 &lt;- ANOVA[[1]]$\"F value\"[2]\nFinteract &lt;- ANOVA[[1]]$\"F value\"[3]\nprint(ANOVA)\ncat(\"\\n\")\ncat(\"\\n\")\nTBint[1] &lt;- Finteract\nfor (i in 2:nreps) {\n  newdat &lt;- sample(res, length(res), replace = FALSE)\n  modb &lt;- summary(aov(newdat ~ factor1 + factor2 +\n    factor1:factor2,\n  data = my.dataframe.noNA\n  ))\n  TBint[i] &lt;- modb[[1]]$\"F value\"[3]\n  TB1[i] &lt;- modb[[1]]$\"F value\"[1]\n  TB2[i] &lt;- modb[[1]]$\"F value\"[2]\n}\nprobInt &lt;- length(TBint[TBint &gt;= Finteract]) / nreps\nprob1 &lt;- length(TB1[TB1 &gt;= F1]) / nreps\nprob2 &lt;- length(TB2[TB1 &gt;= F2]) / nreps\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in ter Braak with unrestricted sampling\nof cell residuals. \")\ncat(\n  \"The probability for the effect of Interaction is \",\n  probInt, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 1 is \",\n  prob1, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 2 is \",\n  prob2, \"\\n\"\n)\n\nSi vous avez la chance d’avoir accès au package lmPerm, vous pouvez effectuer le test de permutation beaucoup plus rapidement et facilement:\n\n#######################################################################\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- Stu2wdat\nmyformula &lt;- as.formula(\"rdwght ~ sex+location+sex:location\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#bootstrap-pour-lanova-à-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#bootstrap-pour-lanova-à-deux-facteurs-de-classification",
    "title": "\n12  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n12.8 Bootstrap pour l’ANOVA à deux facteurs de classification",
    "text": "12.8 Bootstrap pour l’ANOVA à deux facteurs de classification\nDans la plupart des cas, les tests de permutation seront plus appropriés que le bootstrap pour les designs d’ANOVA. J’ai quand même un bout de code qui pourra servir si vous en avez besoin:\n\n############################################################\n###########\n# Bootstrap for two-way ANOVA\n# You possibly want to edit bootfunction.mod1 to return other values\n# Here it returns the standard coefficients of the fitted model\n# Requires boot library\n#\nnreps &lt;- 5000\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nlibrary(boot)\n# Fit model on observed data\nmod1 &lt;- aov(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\n\n\n# Bootstrap 1000 time using the residuals bootstraping methods to\n# keep the same unequal number of observations for each level of the indep. var.\nfit &lt;- fitted(mod1)\ne &lt;- residuals(mod1)\nX &lt;- model.matrix(mod1)\nbootfunction.mod1 &lt;- function(data, indices) {\n  y &lt;- fit + e[indices]\n  bootmod &lt;- lm(y ~ X)\n  coefficients(bootmod)\n}\nbootresults &lt;- boot(my.dataframe.noNA, bootfunction.mod1,\n  R = 1000\n)\nbootresults\n## Calculate 90% CI and plot bootstrap estimates separately for each model parameter\nboot.ci(bootresults, conf = 0.9, index = 1)\nplot(bootresults, index = 1)\nboot.ci(bootresults, conf = 0.9, index = 3)\nplot(bootresults, index = 3)\nboot.ci(bootresults, conf = 0.9, index = 4)\nplot(bootresults, index = 4)\nboot.ci(bootresults, conf = 0.9, index = 5)\nplot(bootresults, index = 5)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html",
    "href": "35-reg_mult.html",
    "title": "\n13  Régression multiple\n",
    "section": "",
    "text": "13.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#set-reg-mul",
    "href": "35-reg_mult.html#set-reg-mul",
    "title": "\n13  Régression multiple\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\ncar\nlmtest\nsimpleboot\nboot\nMuMIn\n\n\nles fichiers de données\n\nMregdat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#conseils-généraux",
    "href": "35-reg_mult.html#conseils-généraux",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.2 Conseils généraux",
    "text": "13.2 Conseils généraux\nLes variables qui intéressent les biologistes sont généralement influencées par plusieurs facteurs, et une description exacte ou une prédiction de la variable dépendante requiert que plus d’une variable soit incluse dans le modèle. La régression multiple permet de quantifier l’effet de plusieurs variables continues sur la variable dépendante.\nIl est important de réaliser que la maîtrise de la régression multiple ne s’acquiert pas instantanément. Les débutants doivent garder à l’esprit plusieurs points importants :\n\nUn modèle de régression multiple peut être hautement significatif même si aucun des termes pris isolément ne l’est (ceci est causé par la multicolinéarité),\nUn modèle peut ne pas être significatif alors que l’un ou plusieurs des termes le sont (ceci est un signe d’un modèle trop complexe (“overfitting”)) et,\nÀ moins que les variables indépendantes soient parfaitement orthogonales (c’est-à-dire qu’il n’y ait aucune corrélation entre elles et donc pas de multicolinéarité) les diverses approches de sélection des variables indépendantes peuvent mener à des modèles différents.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#premières-régressions-multiples",
    "href": "35-reg_mult.html#premières-régressions-multiples",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.3 Premières régressions multiples",
    "text": "13.3 Premières régressions multiples\nLe fichier Mregdat.csv contient des données de richesse spécifique de quatre groupes d’organismes dans 30 marais de la région Ottawa-Cornwall-Kingston. Les variables sont:\n\nla richesse spécifique:\n\ndes oiseaux (bird, et son logarithme base 10 logbird)\ndes mammifères (mammal, logmam)\ndes amphibiens et reptiles (herptile, logherp)\ndes vertébrés (totsp, logtot)\n\n\nles coordonnées des sites (lat, long)\nla superficie du marais (logarea)\nle pourcentage du marais inondé toute l’année (swamp)\nle pourcentage des terres couvertes par des forêts dans un rayon de 1km du marais (cpfor2)\nla densité des routes pavées (en m/ha) dans un rayon de 1km du marais (thtden).\n\nNous allons nous concentrer sur les amphibiens et les reptiles (herptile) pour cet exemple, il est donc avisé d’examiner la distribution de cette variable et les corrélations avec les variables indépendantes potentielles:\n\nmydata &lt;- read.csv(\"data/Mregdat.csv\")\nscatterplotMatrix(\n  ~ logherp + logarea + cpfor2 + thtden + swamp,\n  regLine = TRUE, smooth = TRUE, diagonal = TRUE,\n  data = mydata\n)\n\n\n\n\n\n\nFigure 13.1: Matrice de rélation et densité pour la richesse spécifique des amphibiens et reptiles\n\n\n\n\n\nEn utilisant les données de ce fichier, faites la régression simple de logherp en fonction de logarea . Que concluez-vous à partir de cette analyse?\n\n\nmodel.loga &lt;- lm(logherp ~ logarea, data = mydata)\nsummary(model.loga)\n\n\nCall:\nlm(formula = logherp ~ logarea, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38082 -0.09265  0.00763  0.10409  0.46977 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.18503    0.15725   1.177 0.249996    \nlogarea      0.24736    0.06536   3.784 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1856 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3552,    Adjusted R-squared:  0.3304 \nF-statistic: 14.32 on 1 and 26 DF,  p-value: 0.0008185\n\npar(mfrow = c(2, 2))\nplot(model.loga)\n\n\n\n\n\n\nFigure 13.2: Conditions d’applications de la régression de logherp sur logarea\n\n\n\n\nIl semble donc y avoir une relation positive entre la richesse spécifique des reptiles et des amphibiens et la surface des marais. La régression n’explique cependant qu’environ le tiers de la variabilité (R 2 =0.355). L’analyse des résidus indique qu’il n’y a pas de problème avec la normalité, l’homoscédasticité, ni l’indépendance.\n\nFaites ensuite la régression de logherp en fonction de cpfor2 . Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.logcp &lt;- lm(logherp ~ cpfor2, data = mydata)\nsummary(model.logcp)\n\n\nCall:\nlm(formula = logherp ~ cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49095 -0.10266  0.05881  0.16027  0.25159 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.609197   0.104233   5.845 3.68e-06 ***\ncpfor2      0.002706   0.001658   1.632    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2202 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.09289,   Adjusted R-squared:  0.058 \nF-statistic: 2.662 on 1 and 26 DF,  p-value: 0.1148\n\n\n\n\n\nIci, on doit accepter l’hypothèse nulle et conclure qu’il n’y a pas de relation entre la richesse spécifique dans les marais (logherp) et la proportion de forêts sur les terres adjacentes (cpfor2). Qu’est-ce qui arrive quand on fait une régression avec les 2 variables indépendantes?\n\nRefaites la régression de logherp enfonction de logarea et cpfor2 à la fois. Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mcp &lt;- lm(logherp ~ logarea + cpfor2, data = mydata)\nsummary(model.mcp)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40438 -0.11512  0.01774  0.08187  0.36179 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.027058   0.166749   0.162 0.872398    \nlogarea     0.247789   0.061603   4.022 0.000468 ***\ncpfor2      0.002724   0.001318   2.067 0.049232 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.175 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4052 \nF-statistic:  10.2 on 2 and 25 DF,  p-value: 0.0005774\n\n\n\n\n\nOn voit donc qu’on peut rejeter les 2 hypothèses nulles que la pente de la régression de logherp sur logarea est zéro et que la pente de la régression de logherp sur cpfor2 est zéro. Pourquoi cpfor2 devient-il un facteur significatif dans la régression multiple alors qu’il n’est pas significatif dans la régression simple? Parce qu’il est parfois nécessaire de contrôler pour l’effet d’une variable pour pouvoir détecter les effets plus subtils d’autres variables. Ici, il y a une relation significative entre logherp et logarea qui masque l’effet de cpfor2 sur logherp . Lorsque le modèle tient compte des deux variables explicatives, il devient possible de détecter l’effet de cpfor2 .\n\nAjustez un autre modèle, cette fois en remplaçant cpfor2 par thtden (logherp ~ logarea + thtden). Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mden &lt;- lm(logherp ~ logarea + thtden, data = mydata)\nsummary(model.mden)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31583 -0.12326  0.02095  0.13201  0.31674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.37634    0.14926   2.521 0.018437 *  \nlogarea      0.22504    0.05701   3.947 0.000567 ***\nthtden      -0.04196    0.01345  -3.118 0.004535 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1606 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5358,    Adjusted R-squared:  0.4986 \nF-statistic: 14.43 on 2 and 25 DF,  p-value: 6.829e-05\n\n\n\n\n\nOn rejette donc l’hypothèse nulle que la richesse spécifique n’est pas influencée par la taille des marais (logarea) ni par la densité des routes (thtden). Notez qu’ici il y a une relation négative significative entre la richesse spécifique des amphibiens et reptiles et la densité des routes sur les terres adjacentes, tandis que la relation est positive pour la taille des marais et pour la densité des forêts (cpfor2 ; résultat de la dernière régression).\nLe \\(R^2\\) de ce modèle est plus élevé que pour le précédent, reflétant une corrélation plus forte entre logherp et thtden qu’entre logherp et cpfor2 .\nLa richesse spécifique des reptiles et amphibiens semble donc reliée à la surface de marais (logarea), la densité des routes (thtden), et possiblement au couvert forestier sur les terres adjacentes aux marais (cpfor2). Cependant, les trois variables ne sont peut-être pas nécessaires dans un modèle prédictif. Si deux des trois variables (disons cpfor2 et thtden) sont parfaitement corrélées, alors l’effet de thtden ne serait rien de plus que celui de cpfor2 (et vice-versa) et un modèle incluant l’une des deux variables ferait des prédictions identiques à un modèle incluant ces deux variables (en plus de logarea).\n\nEstimez un modèle de régression avec logherp comme variable dépendante et logarea, cpfor2 et thtden comme variables indépendantes. Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mtri &lt;- lm(logherp ~ logarea + cpfor2 + thtden, data = mydata)\nsummary(model.mtri)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30729 -0.13779  0.02627  0.11441  0.29582 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.284765   0.191420   1.488 0.149867    \nlogarea      0.228490   0.057647   3.964 0.000578 ***\ncpfor2       0.001095   0.001414   0.774 0.446516    \nthtden      -0.035794   0.015726  -2.276 0.032055 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1619 on 24 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5471,    Adjusted R-squared:  0.4904 \nF-statistic: 9.662 on 3 and 24 DF,  p-value: 0.0002291\n\n\n\n\n\nPlusieurs choses sont à noter ici: 1. Tel que prédit, le coefficient de régression pour cpfor2 n’est plus significativement différent de 0. Une fois que la variabilité attribuable à logarea et thtden est enlevée, il ne reste qu’une fraction nonsignificative de la variabilité attribuable à cpfor2 . 2. Le \\(R^2\\) pour ce modèle(0.547) n’est que légèrement supérieur au \\(R^2\\) du modèle avec seulement logarea et thtden (.536), ce qui confirme que cpfor2 n’explique pas grand-chose de plus.\nNotez aussi que même si le coefficient de régression pour thtden n’a pas beaucoup changé par rapport à ce qui avait été estimé lorsque seul thtden et logarea étaient dans le modèle (0-.036 vs -0.042), l’erreur type pour l’estimé du coefficient est plus grand, et ce modèle plus complexe mène à un estimé moins précis. Si la corrélation entre thtden et cpfor2 était plus forte, la décroissance de la précision serait encore plus grande.\nOn peut comparer les deux derniers modèles (i.e., le modèle incluant les 3 variables et celui avec seulement logarea and thtden) pour décider lequel privilégier.\n\nanova(model.mtri, model.mden)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden\nModel 2: logherp ~ logarea + thtden\n  Res.Df     RSS Df Sum of Sq     F Pr(&gt;F)\n1     24 0.62937                          \n2     25 0.64508 -1 -0.015708 0.599 0.4465\n\n\nCette comparaison révèle que le modèle à 3 variables ne fait pas de prédictions significativement meilleures que le modèle avec seulement Logarea et thtden . Ce résultat n’est pas surprenant puisque le test de signification pour cpfor2 dans le modèle complet indique qu’il faut accepter l’hypothèse nulle.\nÀ la suite de cette analyse, on doit conclure que:\n\nLe meilleur modèle est celui incluant thtden et logarea .\nIl y a une relation négative entre la richesse spécifique des amphibiens et reptiles et la densité des routes sur les terres adjacentes.\nIl y a une relation positive entre la richesse spécifique et la taille des marais.\n\nNotez que le “meilleur” modèle n’est pas nécessairement le modèle parfait, seulement le meilleur n’utilisant que ces trois variables indépendantes. Il est évident qu’il y a d’autres facteurs qui contrôlent la richesse spécifique dans les marais puisque, même le “meilleur” modèle n’explique que la moitié de la variabilité.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#régression-multiple-pas-à-pas-stepwise",
    "href": "35-reg_mult.html#régression-multiple-pas-à-pas-stepwise",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.4 Régression multiple pas-à-pas (stepwise)",
    "text": "13.4 Régression multiple pas-à-pas (stepwise)\n\n\n\n\n\n\nAvertissement\n\n\n\nSection encore présente à titre d’information mais à ne jamais utiliser à cause de l’ensemble des problèmes d’inférences et de bias dans l’erreur de type 1 qu’elle génère.\n\n\nQuand le nombre de variables prédictives est restreint, comme dans l’exemple précédent, il est aisé de comparer manuellement les modèles pour sélectionner le plus adéquat. Cependant, lorsque le nombre de variables indépendantes augmente, cette approche n’est rapidement plus utilisable et il est alors utile d’utiliser une méthode automatisée.\nLa sélection pas à pas avec R utilise le Critère Informatif de Akaike (Akaike Information Criterion, \\(AIC = 2* ln(RSS) + 2K\\) où K le nombre de variables indépendantes, n est le nombre d’observations, et RSS est la somme des carrés des résidus) comme mesure de la qualité d’ajustement des modèles. Cette mesure favorise la précision des prédictions et pénalise la complexité. Lorsque l’on compare des modèles par AIC, le modèle avec le plus petit AIC est le modèle à préférer.\n\nUtiliser la fonction step pour activer la sélection pas à pas des variables indépendantes sur le modèles de régression incluant logarea, cpfor2 et thtden:\n\n\n# Stepwise Regression\nstep.mtri &lt;- step(model.mtri, direction = \"both\")\n\nStart:  AIC=-98.27\nlogherp ~ logarea + cpfor2 + thtden\n\n          Df Sum of Sq     RSS     AIC\n- cpfor2   1   0.01571 0.64508 -99.576\n&lt;none&gt;                 0.62937 -98.267\n- thtden   1   0.13585 0.76522 -94.794\n- logarea  1   0.41198 1.04135 -86.167\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n          Df Sum of Sq     RSS     AIC\n&lt;none&gt;                 0.64508 -99.576\n+ cpfor2   1   0.01571 0.62937 -98.267\n- thtden   1   0.25092 0.89600 -92.376\n- logarea  1   0.40204 1.04712 -88.013\n\nstep.mtri$anova # display results\n\n      Step Df   Deviance Resid. Df Resid. Dev       AIC\n1          NA         NA        24  0.6293717 -98.26666\n2 - cpfor2  1 0.01570813        25  0.6450798 -99.57640\n\n\nR nous donne:\n\nL’ajustement (mesuré par AIC) du modèle complet en premier lieu.\nL’AIC des modèles dans lesquels une variable a été enlevée du modèle complet. Notez que c’est seulement en enlevant cpfor2 du modèle qu’on peut réduire l’AIC\nLa valeur de AIC pour les modèles auxquels on enlève ou on ajoute une variable au modèle sélectionné à la première étape.(i.e. logherp ~ logarea + thtden). Notez qu’aucun des modèles n’a un AIC inférieur à ce modèle.\n\nAu lieu de débuter par le modèle complet (saturé) et enlever des termes, on peut commencer par le modèle nul et ajouter des termes:\n\n# Forward selection approach\nmodel.null &lt;- lm(logherp ~ 1, data = mydata)\nstep.f &lt;- step(\n  model.null,\n  scope = ~ . + logarea + cpfor2 + thtden, direction = \"forward\"\n)\n\nStart:  AIC=-82.09\nlogherp ~ 1\n\n          Df Sum of Sq    RSS     AIC\n+ logarea  1   0.49352 0.8960 -92.376\n+ thtden   1   0.34241 1.0471 -88.013\n+ cpfor2   1   0.12907 1.2605 -82.820\n&lt;none&gt;                 1.3895 -82.091\n\nStep:  AIC=-92.38\nlogherp ~ logarea\n\n         Df Sum of Sq     RSS     AIC\n+ thtden  1   0.25093 0.64508 -99.576\n+ cpfor2  1   0.13078 0.76522 -94.794\n&lt;none&gt;                0.89600 -92.376\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n         Df Sum of Sq     RSS     AIC\n&lt;none&gt;                0.64508 -99.576\n+ cpfor2  1  0.015708 0.62937 -98.267\n\nstep.f$anova # display results\n\n       Step Df  Deviance Resid. Df Resid. Dev       AIC\n1           NA        NA        27  1.3895281 -82.09073\n2 + logarea -1 0.4935233        26  0.8960048 -92.37639\n3  + thtden -1 0.2509250        25  0.6450798 -99.57640\n\n\nLe résultat final est le même, mais la trajectoire est différente. Dans ce cas, R débute avec le modèle le plus simple et ajoute une variable indépendante à chaque étape, sélectionnant la variable minimisant AIC à cette étape. Le modèle de départ a donc seulement une ordonnée à l’origine. Puis, logarea est ajouté, suivi de thtden. cpfor2 n’est pas ajouté au modèle, car son addition fait augmenter l’AIC.\nIl est recommandé de comparer le résultat final de plusieurs approches. Si le modèle retenu diffère selon l’approche utilisée, c’est un signe que le “meilleur” modèle est possiblement difficile à identifier et que vous devriez être circonspects dans vos inférences. Dans notre exemple, pas de problème: toutes les méthodes convergent sur le même modèle final.\nPour conclure cette section, quelques conseils concernant les méthodes automatisées de sélection des variables indépendantes:\n\nLes différentes méthodes de sélection des variables indépendantes peuvent mener à des modèles différents. Il est souvent utile d’essayer plus d’une méthode et de comparer les résultats. Si les résultats diffèrent, c’est presque toujours à cause de multicolinéarité entre les variables indépendantes.\nAttention à la régression pas-à-pas. Les auteurs de SYSTAT en disent:\n\n\nStepwise regression is probably the most abused computerized statistical technique ever devised. If you think you need automated stepwise regression to solve a particular problem, you probably don’t. Professional statisticians rarely use automated stepwise regression because it does not necessarily find the “best” fitting model, the “real” model, or alternative “plausible” models. Furthermore, the order in which variables enter or leave a stepwise program is usually of no theoretical significance. You are always better off thinking about why a model could generate your data and then testing that model.\n\nEn bref, on abuse trop souvent de cette technique.\n\nIl faut toujours garder à l’esprit que l’existence d’une régression significative n’est pas suffisante pour prouver une relation causale.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#détecter-la-multicolinéarité",
    "href": "35-reg_mult.html#détecter-la-multicolinéarité",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.5 Détecter la multicolinéarité",
    "text": "13.5 Détecter la multicolinéarité\nLa multicolinéarité est la présence de corrélations entre les variables indépendantes. Lorsqu’elle est extrême (multicolinéarité parfaite) elle empêche l’estimation des modèles statistiques. Lorsqu’elle est grande ou modérée, elle réduit la puissance de détection de l’effet des variables indépendantes individuellement, mais elle n’empêche pas le modèle de faire des prédictions.\nUn des indices les plus utilisés pour quantifier la multicolinéarité et le facteur d’inflation de la variance (VIF, variance inflation factor). Le fichier d’aide du package HH explique ainsi son calcul:\n\nA simple diagnostic of collinearity is the variance inflation factor, VIF one for each regression coefficient (other than the intercept). Since the condition of collinearity involves the predictors but not the response, this measure is a function of the X’s but not of Y. The VIF for predictor i is \\(1/(1-R_i^2)\\), where \\(R_i^2\\) is the \\(R^2\\) from a regression of predictor i against the remaining predictors. If \\(R_i^2\\) is close to 1, this means that predictor i is well explained by a linear function of the remaining predictors, and, therefore, the presence of predictor i in the model is redundant. Values of VIF exceeding 5 are considered evidence of collinearity: The information carried by a predictor having such a VIF is contained in a subset of the remaining predictors. If, however, all of a model’s regression coefficients differ significantly from 0 (p-value &lt; .05), a somewhat larger VIF may be tolerable.\n\nBref, les VIF indiquent de combien l’incertitude de chaque coefficient de régression est augmentée par la multicolinéarité.\nAttrappe. Il y a plusieurs fonctions vif() (j’en connais au moins trois dans les extensions car, HH et DAAG), et je ne sais pas en quoi elles diffèrent.\nOn peut calculer les VIF avec la fonction vif() de l’extension car:\n\nlibrary(car)\nvif(model.mtri)\n\n logarea   cpfor2   thtden \n1.022127 1.344455 1.365970 \n\n\nIci, il n’y a pas d’évidence de multicolinéarité car toutes les valeurs de VIF sont près de 1.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#régression-polynomiale",
    "href": "35-reg_mult.html#régression-polynomiale",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.6 Régression polynomiale",
    "text": "13.6 Régression polynomiale\nLa régression requiert la linéarité de la relation entre les variables dépendante et indépendante(s). Lorsque la relation n’est pas linéaire, il est parfois possible de linéariser la relation en effectuant une transformation sur une ou plusieurs variables. Cependant, dans bien des cas il est impossible de transformer les axes pour rendre la relation linéaire. On doit alors utiliser une forme ou l’autre de régression nonlinéaire. La forme la plus simple de régression non-linéaire est la régression polynomiale dans laquelle les variables indépendantes sont à une puissance plus grande que 1 (Ex : \\(X^2\\) ou \\(X^3\\)).\n\nFaites un diagramme de dispersion des résidus (residual) de la régression logherp ~ logarea en fonction de swamp .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# problème avec les données de manquantes dans logherp\nmysub &lt;- subset(mydata, !is.na(logherp))\n# ajouter les résidus dans les donnée\nmysub$resloga &lt;- residuals(model.loga)\nggplot(data = mysub, aes(y = resloga, x = swamp)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nFigure 13.3: Relation entre swamp et les résidus de la régression entre logherp et logarea\n\n\n\n\n\n\n\n\nL’examen de ce graphique suggère qu’il y a une forte relation entre les deux variables, mais qu’elle n’est pas linéaire. Essayez de faire une régression de residual sur swamp . Quelle est votre conclusion?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.resloga &lt;- lm(resloga ~ swamp, mysub)\nsummary(model.resloga)\n\n\nCall:\nlm(formula = resloga ~ swamp, data = mysub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35088 -0.13819  0.00313  0.10849  0.45802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.084571   0.109265   0.774    0.446\nswamp       -0.001145   0.001403  -0.816    0.422\n\nResidual standard error: 0.1833 on 26 degrees of freedom\nMultiple R-squared:  0.02498,   Adjusted R-squared:  -0.01252 \nF-statistic: 0.666 on 1 and 26 DF,  p-value: 0.4219\n\n\n\n\n\nEn deux mots, l’ajustement est épouvantable! Malgré le fait que le graphique suggère une relation très forte entre les deux variables. Cependant, cette relation n’est pas linéaire… (ce qui est également apparent si vous examinez les résidus du modèle linéaire).\n\nRefaites la régression d’en haut, mais cette fois incluez un terme pour représenter \\(swamp^2\\) . L’expression devrait apparaître comme: \\(Y ~ X + I(X^2)\\) . Que concluez-vous? Qu’est-ce que l’examen des résidus de cette régression multiple révèle?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.resloga2 &lt;- lm(resloga ~ swamp + I(swamp^2), mysub)\nsummary(model.resloga2)\n\n\nCall:\nlm(formula = resloga ~ swamp + I(swamp^2), data = mysub)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.181185 -0.085350  0.007377  0.067327  0.242455 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.804e-01  1.569e-01  -4.975 3.97e-05 ***\nswamp        3.398e-02  5.767e-03   5.892 3.79e-06 ***\nI(swamp^2)  -2.852e-04  4.624e-05  -6.166 1.90e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1177 on 25 degrees of freedom\nMultiple R-squared:  0.6132,    Adjusted R-squared:  0.5823 \nF-statistic: 19.82 on 2 and 25 DF,  p-value: 6.972e-06\n\n\n\n\n\nIl devient évident que si on corrige la richesse spécifique pour la taille des marais, une fraction importante de la variabilité résiduelle peut être associée à swamp, selon une relation quadratique. Si vous examinez les résidus, vous observerez que l’ajustement est nettement meilleur qu’avec le modèle linéaire.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.resloga2)\n\n\n\n\n\n\nFigure 13.4: Relation\n\n\n\n\n\n\n\n\nEn vous basant sur les résultats de la dernière analyse, comment suggérez-vous de modifier le modèle de régression multiple? Quel est, d’après vous, le meilleur modèle? Pourquoi? Ordonnez les différents facteurs en ordre croissant de leur effet sur la richesse spécifique des reptiles.\n\nSuite à ces analyses, il semble opportun d’essayer d’ajuster un modèle incluant logarea, thtden, cpfor2, swamp et swamp^2 :\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.poly1 &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model.poly1)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.201797 -0.056170 -0.002072  0.051814  0.205626 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.203e-01  1.813e-01  -1.766   0.0912 .  \nlogarea      2.202e-01  3.893e-02   5.656 1.09e-05 ***\ncpfor2      -7.864e-04  9.955e-04  -0.790   0.4380    \nthtden      -2.929e-02  1.048e-02  -2.795   0.0106 *  \nswamp        3.113e-02  5.898e-03   5.277 2.70e-05 ***\nI(swamp^2)  -2.618e-04  4.727e-05  -5.538 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1072 on 22 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8181,    Adjusted R-squared:  0.7767 \nF-statistic: 19.78 on 5 and 22 DF,  p-value: 1.774e-07\n\n\n\n\n\nLes résultats de cette analyse suggèrent qu’on devrait probablement exclure cpfor2 du modèle:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.poly2 &lt;- lm(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model.poly2)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19621 -0.05444 -0.01202  0.07116  0.21295 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.461e-01  1.769e-01  -1.957   0.0626 .  \nlogarea      2.232e-01  3.842e-02   5.810 6.40e-06 ***\nthtden      -2.570e-02  9.364e-03  -2.744   0.0116 *  \nswamp        2.956e-02  5.510e-03   5.365 1.89e-05 ***\nI(swamp^2)  -2.491e-04  4.409e-05  -5.649 9.46e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1063 on 23 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8129,    Adjusted R-squared:  0.7804 \nF-statistic: 24.98 on 4 and 23 DF,  p-value: 4.405e-08\n\n\n\n\n\nEst-ce qu’il y a possiblement un problème de multicolinéarité?\n\nvif(model.poly2)\n\n   logarea     thtden      swamp I(swamp^2) \n  1.053193   1.123491  45.845845  45.656453 \n\n\nLes valeurs d’inflation de la variance (VIF) pour les deux termes de swamp sont beaucoup plus élevés que le seuil de 5. Cependant, c’est la norme pour les termes polynomiaux et on ne doit pas s’en préoccuper outre mesure, surtout quand les deux termes sont hautement significatifs dans le modèle. Les fortes valeurs de VIF indiquent que les coefficients pour ces deux termes ne sont pas estimés précisément, mais leur utilisation dans le modèle permet tout de même de faire de bonnes prédictions (i.e. ils décrivent la réponse à swamp).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#vérifier-les-conditions-dapplication-de-modèles-de-régression-multiple",
    "href": "35-reg_mult.html#vérifier-les-conditions-dapplication-de-modèles-de-régression-multiple",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.7 Vérifier les conditions d’application de modèles de régression multiple",
    "text": "13.7 Vérifier les conditions d’application de modèles de régression multiple\nToutes les techniques de sélection des modèles présument que les conditions d’applications (indépendance, normalité, homoscédasticité, linéarité) sont remplies. Comme il y a un grand nombre de modèles qui peuvent être ajustés, il peut paraître quasi impossible de vérifier si les conditions sont remplies à chaque étape de construction. Cependant, il est souvent suffisant d’examiner les résidus du modèle complet (saturé) puis du modèle final. Les termes qui ne contribuent pas significativement à l’ajustement n’affectent pas beaucoup les résidus et donc les résidus du modèle final sont généralement similaires à ceux du modèle complet.\nExaminons donc les graphiques diagnostiques du modèle final:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.poly2)\n\n\n\n\n\n\nFigure 13.5: Conditions d’application du modèle model.poly2\n\n\n\n\n\n\n\nTout semble acceptable dans ce cas. Pour convaincre les sceptiques, on peut faire les tests formels des conditions d’application:\n\nshapiro.test(residuals(model.poly2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model.poly2)\nW = 0.9837, p-value = 0.9278\n\n\nLes résidus ne dévient pas significativement de la normalité. Bien.\n\nlibrary(lmtest)\nbptest(model.poly2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model.poly2\nBP = 3.8415, df = 4, p-value = 0.4279\n\n\nPas de déviation d’homoscédasticité non plus. Bien.\n\ndwtest(model.poly2)\n\n\n    Durbin-Watson test\n\ndata:  model.poly2\nDW = 1.725, p-value = 0.2095\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nPas de corrélation sérielle des résidus, donc pas d’évidence de nonindépendance.\n\nresettest(model.poly2, type = \"regressor\", data = mydata)\n\n\n    RESET test\n\ndata:  model.poly2\nRESET = 0.9823, df1 = 8, df2 = 15, p-value = 0.4859\n\n\nEt finalement, pas de déviation significative de la linéarité. Donc tout semble acceptable.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#visualiser-la-taille-deffet",
    "href": "35-reg_mult.html#visualiser-la-taille-deffet",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.8 Visualiser la taille d’effet",
    "text": "13.8 Visualiser la taille d’effet\nLes coefficients de la régression multiple peuvent mesurer la taille d’effet, quoiqu’il puisse être nécessaire de les standardiser pour qu’ils ne soient pas influencés par les unités de mesure. Mais un graphique est souvent plus informatif. Dans ce contexte, les graphiques des résidus partiels (appelés components+residual plots dans R) sont particulièrement utiles. Ces graphique illustrent comment la variable dépendante, corrigée pour l’effet des autres variables dans le modèle, varie avec chacune des variables indépendantes du modèle. Voyons voir:\n\n# Evaluate visually linearity and effect size\n# component + residual plot\ncrPlots(model.poly2)\n\n\n\n\n\n\nFigure 13.6: Graphiques de résidus partiels du modèle model.poly2\n\n\n\n\nNotez que l’échelle de l’axe des y varie sur chaque graphique. Pour thtden, la variable dépendante (log10(richesse des herptiles)) varie d’environ 0.4 unités entre la valeur minimum et maximum de thtden. Pour logarea, la variation est d’environ 0.6 unité log. Pour swamp, l’interprétation est plus compliquée parce qu’il y a deux termes qui quantifient son effet, et que ces termes ont des signes opposés (positif pour swamp et négatif pour swamp^2) ce qui donne une relation curvilinéaire de type parabole. Le graphique ne permet pas de bien visualiser cela. Ceci dit, ces graphique n’indiquent pas vraiment de violation de linéarité.\nPour illustrer ce qui serait visible sur ces graphiques si il y avait une déviation de linéarité, enlevons le terme du second degré pour swamp, puis on va refaire ces graphiques et effectuer le test RESET.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.nopoly &lt;- lm(\n  logherp ~ logarea + thtden + swamp,\n  data = mydata\n)\ncrPlots(model.nopoly)\n\n\n\n\n\n\nFigure 13.7: Graphiques de résidus partiels du modèle model.nopoly\n\n\n\n\n\n\n\nLa relation non-linéaire avec swamp devient évidente. Et le test RESET détecte bien cette non-linéarité:\n\nresettest(model.nopoly, type = \"regressor\")\n\n\n    RESET test\n\ndata:  model.nopoly\nRESET = 6.7588, df1 = 6, df2 = 18, p-value = 0.0007066",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#tester-la-présence-dinteractions",
    "href": "35-reg_mult.html#tester-la-présence-dinteractions",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.9 Tester la présence d’interactions",
    "text": "13.9 Tester la présence d’interactions\nLorsqu’il y a plusieurs variables indépendantes, vous devriez toujours garder à l’esprit la possibilité d’interactions. Dans la majorité des situations de régression multiple cela n’est pas évident parce que l’addition de termes d’interaction augmente la multicolinéarité des termes du modèle, et parce qu’il n’y a souvent pas assez d’observations pour éprouver toutes les interactions ou que les observations ne sont pas suffisamment balancées pour faire des tests puissants pour les interactions. Retournons à notre modèle “final” et voyons ce qui se passe si on essaie d’ajuster un modèle saturé avec toutes les interactions:\n\nfullmodel.withinteractions &lt;- lm(\n  logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2),\n  data = mydata\n)\nsummary(fullmodel.withinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), \n    data = mydata)\n\nResiduals:\nALL 28 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (4 not defined because of singularities)\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                            -5.948e+03        NaN     NaN      NaN\nlogarea                                 3.293e+03        NaN     NaN      NaN\ncpfor2                                  7.080e+01        NaN     NaN      NaN\nthtden                                  9.223e+02        NaN     NaN      NaN\nswamp                                   1.176e+02        NaN     NaN      NaN\nI(swamp^2)                             -3.517e-01        NaN     NaN      NaN\nlogarea:cpfor2                         -3.771e+01        NaN     NaN      NaN\nlogarea:thtden                         -4.781e+02        NaN     NaN      NaN\ncpfor2:thtden                          -1.115e+01        NaN     NaN      NaN\nlogarea:swamp                          -7.876e+01        NaN     NaN      NaN\ncpfor2:swamp                           -1.401e+00        NaN     NaN      NaN\nthtden:swamp                           -1.920e+01        NaN     NaN      NaN\nlogarea:I(swamp^2)                      5.105e-01        NaN     NaN      NaN\ncpfor2:I(swamp^2)                       3.825e-03        NaN     NaN      NaN\nthtden:I(swamp^2)                       7.826e-02        NaN     NaN      NaN\nswamp:I(swamp^2)                       -2.455e-03        NaN     NaN      NaN\nlogarea:cpfor2:thtden                   5.359e+00        NaN     NaN      NaN\nlogarea:cpfor2:swamp                    8.743e-01        NaN     NaN      NaN\nlogarea:thtden:swamp                    1.080e+01        NaN     NaN      NaN\ncpfor2:thtden:swamp                     2.620e-01        NaN     NaN      NaN\nlogarea:cpfor2:I(swamp^2)              -5.065e-03        NaN     NaN      NaN\nlogarea:thtden:I(swamp^2)              -6.125e-02        NaN     NaN      NaN\ncpfor2:thtden:I(swamp^2)               -1.551e-03        NaN     NaN      NaN\nlogarea:swamp:I(swamp^2)               -4.640e-04        NaN     NaN      NaN\ncpfor2:swamp:I(swamp^2)                 3.352e-05        NaN     NaN      NaN\nthtden:swamp:I(swamp^2)                 2.439e-04        NaN     NaN      NaN\nlogarea:cpfor2:thtden:swamp            -1.235e-01        NaN     NaN      NaN\nlogarea:cpfor2:thtden:I(swamp^2)        7.166e-04        NaN     NaN      NaN\nlogarea:cpfor2:swamp:I(swamp^2)                NA         NA      NA       NA\nlogarea:thtden:swamp:I(swamp^2)                NA         NA      NA       NA\ncpfor2:thtden:swamp:I(swamp^2)                 NA         NA      NA       NA\nlogarea:cpfor2:thtden:swamp:I(swamp^2)         NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 27 and 0 DF,  p-value: NA\n\n\nNotez les coefficients manquants aux dernières lignes: on ne peut inclure les 32 termes si on a seulement 28 observations. Il manque des observations, le R carré est 1, et le modèle “prédit” parfaitement les données.\nSi on essaie une méthode automatique pour identifier le “meilleur” modèle dans ce gâchis, R refuse:\n\nstep(fullmodel.withinteractions)\n\nError in step(fullmodel.withinteractions): AIC is -infinity for this model, so 'step' cannot proceed\n\n\nBon, est-ce qu’on oublie tout ça et qu’on accepte le modèle final sans ce soucier des interactions? Non, pas encore. Il y a un compromis possible: comparer notre modèle “final” à un modèle qui contient au moins un sous-ensemble des interactions, par exemple toutes les interactions du second degré, pour éprouver si l’addition de ces interactions améliore beaucoup l’ajustement du modèle.\n\nfull.model.2ndinteractions &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\n    + logarea:cpfor2\n    + logarea:thtden\n    + logarea:swamp\n    + cpfor2:thtden\n    + cpfor2:swamp\n    + thtden:swamp,\n  data = mydata\n)\nsummary(full.model.2ndinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + \n    logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + \n    cpfor2:swamp + thtden:swamp, data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.216880 -0.036534  0.003506  0.042990  0.175490 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4.339e-01  6.325e-01   0.686 0.502581    \nlogarea        -1.254e-01  2.684e-01  -0.467 0.646654    \ncpfor2         -9.344e-03  7.205e-03  -1.297 0.213032    \nthtden         -1.833e-01  9.035e-02  -2.028 0.059504 .  \nswamp           3.569e-02  7.861e-03   4.540 0.000334 ***\nI(swamp^2)     -3.090e-04  7.109e-05  -4.347 0.000500 ***\nlogarea:cpfor2  2.582e-03  2.577e-03   1.002 0.331132    \nlogarea:thtden  7.017e-02  3.359e-02   2.089 0.053036 .  \nlogarea:swamp  -5.290e-04  2.249e-03  -0.235 0.816981    \ncpfor2:thtden  -2.095e-04  6.120e-04  -0.342 0.736544    \ncpfor2:swamp    4.651e-05  5.431e-05   0.856 0.404390    \nthtden:swamp    2.248e-04  4.764e-04   0.472 0.643336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.108 on 16 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8658,    Adjusted R-squared:  0.7735 \nF-statistic: 9.382 on 11 and 16 DF,  p-value: 4.829e-05\n\n\nCe modèle s’ajuste un peu mieux aux données que les modèle “final” (il explique 86.6% de la variance de logherp, comparé à 81.2% pour le modèle “final” sans interactions), mais il compte deux fois plus de paramètres. De plus, si vous examinez les coefficients, il se passe d’étranges choses: le signe pour logare a changé par exemple. C’est un des symptômes de la multicolinéarité. Allons voir les facteurs d’inflation de la variance:\n\nvif(full.model.2ndinteractions)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n       logarea         cpfor2         thtden          swamp     I(swamp^2) \n      49.86060       78.49622      101.42437       90.47389      115.08457 \nlogarea:cpfor2 logarea:thtden  logarea:swamp  cpfor2:thtden   cpfor2:swamp \n      66.97792       71.69894       67.27034       14.66814       29.41422 \n  thtden:swamp \n      20.04410 \n\n\nAie! tous les VIF sont plus grands que 5, pas seulement les termes incluant swamp. Cette forte multicolinéarité empêche de quantifier avec précision l’effet de ces interactions. De plus, ce modèle avec interactions n’est pas plus informatif que le modèle “final” puisque son AIC est plus élevé (souvenez-vous qu’on privilégie le modèle avec la valeur d’AIC la plus basse):\n\nAIC(model.poly1)\n\n[1] -38.3433\n\nAIC(full.model.2ndinteractions)\n\n[1] -34.86123\n\n\nOn peut également utiliser la fonction anova() pour comparer l’ajustement des deux modèles et vérifier si l’addition des termes d’intération améliore significativement l’ajustement:\n\nanova(model.poly1, full.model.2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 0.25282                           \n2     16 0.18651  6  0.066314 0.9481  0.489\n\n\nIci, l’addition des termes d’interaction ne réduit pas significativement la variabilité résiduelle du modèle “complet”. Qu’en est-il de la si on compare le modèle avec interaction et notre modèle “final”?\n\nanova(model.poly2, full.model.2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     23 0.25999                           \n2     16 0.18651  7  0.073486 0.9006 0.5294\n\n\nLe test indique que ces deux modèles ont des variances résiduelles comparables, et donc que l’addition des termes d’interaction et de cpfor2 au modèle final n’apporte pas grand chose.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#recherche-du-meilleur-modèle-fondée-sur-la-théorie-de-linformation",
    "href": "35-reg_mult.html#recherche-du-meilleur-modèle-fondée-sur-la-théorie-de-linformation",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.10 Recherche du meilleur modèle fondée sur la théorie de l’information",
    "text": "13.10 Recherche du meilleur modèle fondée sur la théorie de l’information\nUne des principales critiques des méthodes pas-à-pas (stepwise) est que les valeurs de p ne sont pas strictement interprétables à cause du grand nombre de tests qui sont implicites dans le processus. C’est le problème des comparaisons ou tests multiples: en construisant un modèle linéaire (comme une régression multiple) à partir d’un grand nombre de variables et de leurs interactions, il y a tellement de combinaisons possibles qu’un ajustement de Bonferroni rendrait les tests trop conservateurs.\nUne alternative, défendue par Burnham et Anderson (2002, Model selection and multimodel inference: a practical information-theoretic approach. 2nd ed), est d’utiliser l’AIC (ou mieux encore AICc qui est plus approprié quand le nombre d’observations est inférieur à 40 fois le nombre de variables indépendantes) pour ordonner les modèles et identifier un sousensemble de modèles qui sont les meilleurs. On peut ensuite calculer les moyennes des coefficients pondérées par la probabilité que chacun des modèles soit le meilleur pour obtenir des coefficients qui sont plus robustes et moins sensibles à la multicolinéarité.\nL’approche de comparaison par AIC a d’abord été développé pour comparer un ensemble de modèle préalablement défini basé sur les connaissance du sytème et les hypothèses biologiques. Cependant, certains ont développé une approche plutôt brutale et sans scrupule de faire tous les modèles possibles et de les comparer par AIC. Cette approche a été suivie dans le package MuMIn. Les comparaisons de modèle par AICdoivent être faites en utilisant exactement le même jeu de données pour chaque modèle. Il faut donc s’arrurer d’enlever les données manquantes et de spécifier dans la fonction lm de ne pas marcher s’il y a des données manquantes.\n\n\n\n\n\n\nNote\n\n\n\nJe ne supporte pas l’approche stepwise ni l’approche par AIC. Je déteste l’approche par la fonction dredge() qui selon moi va à l’encontre de la philosophie des AIC et de la parsimonie. Je soutiens de dévelooper un modèle basé sur des hypothèses biologiques et de reporter ce modèle avec tous les effets significatifs ou non.\n\n\n\n# refaire le modèle en s'assurant qu'il n'y a pas de \"NA\" et en spécificant na.action\nfull.model.2ndinteractions &lt;- update(\n  full.model.2ndinteractions,\n  . ~ .,\n  data = mysub,\n  na.action = \"na.fail\"\n)\n\nlibrary(MuMIn)\ndd &lt;- dredge(full.model.2ndinteractions)\n\nFixed term is \"(Intercept)\"\n\n\nL’objet dd contient tous les modèles possibles (i.e. ceux qui ont toutes les combinaisons possibles) en utilisant les termes du modèle full.model.2ndinteractions ajusté précédemment. On peut ensuite extraire de l’objet dd le sous-ensemble de modèles qui ont un AICc semblable au meilleur modèle (Burnham et Anderson suggèrent que les modèles qui dévient par plus de 7 unités d’AICc du meilleur modèle ont peu de support empirique).\n\n# get models within 2 units of AICc from the best model\ntop.models.1 &lt;- get.models(dd, subset = delta &lt; 2)\navgmodel1 &lt;- model.avg(top.models.1) # compute average parameters\nsummary(avgmodel1) # display averaged model\n\n\nCall:\nmodel.avg(object = top.models.1)\n\nComponent model call: \nlm(formula = logherp ~ &lt;2 unique rhs&gt;, data = mysub, na.action = \n     na.fail)\n\nComponent models: \n      df logLik   AICc delta weight\n12345  7  27.78 -35.95  0.00   0.55\n1234   6  25.78 -35.56  0.39   0.45\n\nTerm codes: \n    I(swamp^2)        logarea          swamp         thtden logarea:thtden \n             1              2              3              4              5 \n\nModel-averaged coefficients:  \n(full average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.145e-01  2.308e-01   2.406e-01   0.891    0.373    \nlogarea         1.356e-01  1.089e-01   1.119e-01   1.213    0.225    \nswamp           3.180e-02  5.971e-03   6.273e-03   5.070    4e-07 ***\nI(swamp^2)     -2.669e-04  4.770e-05   5.011e-05   5.326    1e-07 ***\nthtden         -6.985e-02  5.233e-02   5.361e-02   1.303    0.193    \nlogarea:thtden  2.131e-02  2.487e-02   2.545e-02   0.837    0.403    \n \n(conditional average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.145e-01  2.308e-01   2.406e-01   0.891   0.3727    \nlogarea         1.356e-01  1.089e-01   1.119e-01   1.213   0.2253    \nswamp           3.180e-02  5.971e-03   6.273e-03   5.070    4e-07 ***\nI(swamp^2)     -2.669e-04  4.770e-05   5.011e-05   5.326    1e-07 ***\nthtden         -6.985e-02  5.233e-02   5.361e-02   1.303   0.1927    \nlogarea:thtden  3.882e-02  2.114e-02   2.237e-02   1.735   0.0827 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(avgmodel1) # display CI for averaged coefficients\n\n                       2.5 %       97.5 %\n(Intercept)    -0.6860022996  0.257064603\nlogarea        -0.0836067896  0.354883299\nswamp           0.0195105703  0.044099316\nI(swamp^2)     -0.0003650809 -0.000168656\nthtden         -0.1749296690  0.035236794\nlogarea:thtden -0.0050266778  0.082666701\n\n\n\nLa liste des modèles qui sont à 4 unités ou moins de l’AICc du meilleur modèle. Les variables dans chaque modèle sont codées et on retrouve la clé en dessous du tableau.\nPour chaque modèle, en plus de l’AICc, le poids Akaike est calculé. C’est un estimé de la probabilité que ce modèle est le meilleur. Ici on voit que le premier modèle (le meilleur) a seulement 34% des chance d’être vraiment le meilleur.\nÀ partir de ce sous-ensemble de modèles, la moyenne pondérée des coefficients (en utilisant les poids Akaike) est calculée, avec in IC à 95%. Notez que les termes absents d’un modèle sont considérés avoir un coefficient de 0 pour ce terme.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#bootstrap-et-régression-multiple",
    "href": "35-reg_mult.html#bootstrap-et-régression-multiple",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.11 Bootstrap et régression multiple",
    "text": "13.11 Bootstrap et régression multiple\nQuand les données ne rencontrent pas les conditions d’application de normalité et d’homoscédasticité et que les transformations n’arrivent pas à corriger ces violations, le bootstrap peut être utilisé pour calculer des intervalles de confiance pour les coefficients. Si la distribution des coefficients bootstrappés est symétrique et approximativement normale, on peut utiliser les percentiles empiriques pour calculer les limites de confiance.\nLe code qui suit, utilisant le package simpleboot, a été conçu pour être facilement modifiable et calcule les limites des IC à partir des percentiles empiriques.\n\n############################################################\n#######\n# Bootstrap analysis the simple way with library simpleboot\n# Define model to be bootstrapped and the data source used\nmymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata)\n# Set the number of bootstrap iterations\nnboot &lt;- 1000\nlibrary(simpleboot)\n# R is the number of bootstrap iterations\n# Setting rows to FALSE indicates resampling of residuals\nmysimpleboot &lt;- lm.boot(mymodel, R = nboot, rows = FALSE)\n# Extract bootstrap coefficients\nmyresults &lt;- sapply(mysimpleboot$boot.list, function(x) x$coef)\n# Transpose matrix so that lines are bootstrap iterations and columns are coefficients\ntmyresults &lt;- t(myresults)\n\nVous pouvez ensuite faire des graphiques pour voir les résultats. Lorsque vous tournerez ce code, il y aura une pause pour vous permettre d’examiner la distribution pour chaque coefficient du modèle sur des graphiques:\n\n# Plot histograms of bootstrapped coefficients\nncoefs &lt;- length(data.frame(tmyresults))\npar(mfrow = c(1, 2), mai = c(0.5, 0.5, 0.5, 0.5), ask = TRUE)\nfor (i in 1:ncoefs) {\n  lab &lt;- colnames(tmyresults)[i]\n  x &lt;- tmyresults[, i]\n  plot(density(x), main = lab, xlab = \"\")\n  abline(v = mymodel$coef[i], col = \"red\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  hist(x, main = lab, xlab = \"\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  abline(v = mymodel$coef[i], col = \"red\")\n}\n\n\n\n\n\n\n\n\nFigure 13.8: Distribution des estimé par bootstrap pour logarea\n\n\n\n\nLe graphique de droite illustre la densité lissée (kernel density) et celui de gauche est l’histogramme des estimés bootstrap du coefficient. La ligne rouge sur le graphique indique la valeur du coefficient ordinaire (pas bootstrap) et les deux lignes verticales noires marquent les limites de l’intervalle de confiance à 95%. Ici l’IC ne contient pas 0, et donc on peut conclure que l’effet de logarea sur logherp est significativement positif.\nLes limites précises peuvent être obtenues par:\n\n# Display empirical bootstrap quantiles (not corrected for bias)\np &lt;- c(0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995)\napply(tmyresults, 2, quantile, p)\n\n      (Intercept)   logarea       thtden      swamp    I(swamp^2)\n0.5%  -0.72015691 0.1408549 -0.049532934 0.01782288 -0.0003426127\n1%    -0.68763485 0.1458417 -0.045999063 0.01857755 -0.0003383352\n2.5%  -0.65598177 0.1547219 -0.042789810 0.02047106 -0.0003257695\n5%    -0.60369685 0.1662327 -0.040001926 0.02173747 -0.0003131280\n95%   -0.09267750 0.2790818 -0.012366646 0.03772895 -0.0001875184\n97.5% -0.01799830 0.2902973 -0.009936534 0.03888219 -0.0001738278\n99%    0.02846165 0.2979800 -0.006783287 0.04065709 -0.0001599040\n99.5%  0.05167335 0.3027551 -0.005732742 0.04121435 -0.0001542944\n\n\nCes intervalles de confiances ne sont pas fiables si la distribution des estimés bootstrap n’est pas Gaussienne. Dans ce cas, il vaut mieux calculer des coefficients non-biaisés (bias-corrected accelerated confidence limits, BCa):\n\n################################################\n# Bootstrap analysis in multiple regression with BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\n\nlibrary(boot)\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # allows boot to select sample\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = mydata, statistic = bs, R = 1000,\n  formula = logherp ~ logarea + thtden + swamp + I(swamp^2)\n)\n# view results\n\nPour obtenir les résultats, le code suivant va produire le graphique standard pour chaque coefficient, et les estimés BCa pour l’intervalle de confiance\n\nplot(results, index = 1) # intercept\nplot(results, index = 2) # logarea\nplot(results, index = 3) # thtden\nplot(results, index = 4) # swamp\nplot(results, index = 5) # swamp^2\n\n# get 95% confidence intervals\nboot.ci(results, type = \"bca\", index = 1)\nboot.ci(results, type = \"bca\", index = 2)\nboot.ci(results, type = \"bca\", index = 3)\nboot.ci(results, type = \"bca\", index = 4)\nboot.ci(results, type = \"bca\", index = 5)\n\nPour logarea, cela donne:\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   ( 0.1159,  0.3260 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\nFigure 13.9\n\n\n\n\nNotez que l’intervalle BCa va de 0.12 à 0.32, alors que l’intervalle standard était de 0.16 à 0.29. L’intervalle BCa est ici plus grand du côté inférieur et plus petit du côté supérieur comme il se doit compte tenu de la distribution non-Gaussienne et asymétrique des estimés bootstrap.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#perm_reg_mult",
    "href": "35-reg_mult.html#perm_reg_mult",
    "title": "\n13  Régression multiple\n",
    "section": "\n13.12 Test de permutation",
    "text": "13.12 Test de permutation\nLes tests de permutations sont plus rarement effectués en régression multiple que le bootstrap. Voici un fragment de code pour le faire tout de même.\n\n############################################################\n##########\n# Permutation in multiple regression\n#\n# using lmperm library\nlibrary(lmPerm)\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nmymodelProb &lt;- lmp(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata, perm = \"Prob\"\n)\nsummary(mymodel)\nsummary(mymodelProb)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html",
    "href": "36-ancova_glm.html",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "",
    "text": "14.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#set-anco",
    "href": "36-ancova_glm.html#set-anco",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\ncar\nlmtest\n\n\nles fichiers de données\n\nanc1dat.csv\nanc3dat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#modèle-linéaire-général",
    "href": "36-ancova_glm.html#modèle-linéaire-général",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.2 Modèle linéaire général",
    "text": "14.2 Modèle linéaire général\nLes modèles linéaires généraux ou General Linear Model en anglais sont différent des modèles linéaires généralisés (ou generalized linear model, GLM). Les modèles linéaires généraux sont des modèles statistiques de la forme \\(Y = B \\mathbf{X} + E\\), ou Y est un vecteur contenant la variable dépendante continue, B est un vecteur des paramètres estimés, \\(\\mathbf{X}\\) et la matrice des différents variables indépendantes et E est un vecteur de résidus homoscédastiques et normalement distribués. Tous les tests que nous avons étudiés à date (test de t, régression linéaire simple, ANOVA à un facteur de classification, ANOVA à plusieurs facteurs de classification et régression multiple) sont formulés ainsi. Notez que tous les modèles que nous avons rencontrés à ce jour ne contiennent qu’un type de variable indépendante (soit continue ou discontinue). Dans cet exercice de laboratoire, vous allez ajuster des modèles qui ont les deux types de variables indépendantes.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#ancova",
    "href": "36-ancova_glm.html#ancova",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.3 ANCOVA",
    "text": "14.3 ANCOVA\nANCOVA est l’abréviation pour l’analyse de covariance. C’est un type de modèle linéaire général dans lequel il y a une (ou plusieurs) variable indépendante continue (parfois appelé la covariable) et une (ou plusieurs) variable indépendante discontinue. Dans la présentation traditionnelle de l’ANCOVA dans les manuels de biostatistique, le modèle ANCOVA ne contient pas de termes d’interaction entre les variables continues et discontinues. Par conséquent, on doit précéder l’ajustement de ce modèle (réduit parce que sans terme d’interaction), par un test de signification de l’interaction qui correspond à éprouver l’égalité des pentes (coefficients pour la ou les variables continues) entre les différents niveaux de la ou les variables discontinues (i.e un test d’homogénéité des pentes).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#homogénéité-des-pentes",
    "href": "36-ancova_glm.html#homogénéité-des-pentes",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.4 Homogénéité des pentes",
    "text": "14.4 Homogénéité des pentes\nPour répondre à de nombreuses questions biologiques, il est nécessaire de déterminer si deux (ou plus de deux) régressions diffèrent significativement. Par exemple, pour comparer l’efficacité de deux insecticides on doit comparer la relation entre leur dose et la mortalité. Ou encore, pour comparer le taux de croissance des mâles et des femelles on doit comparer la relation entre la taille et l’âge des mâles et des femelles.\nComme chaque régression linéaire est décrite par deux paramètres, la pente et l’ordonnée à l’origine, on doit considérer les deux dans la comparaison. Le modèle d’ANCOVA, à strictement parler, n’éprouve que l’hypothèse d’égalité des ordonnées à l’origine. Cependant, avant d’ajuster ce modèle, il faut éprouver l’hypothèse d’égalité des pentes (homogénéité des pentes).\n\n14.4.1 Cas 1 - La taille en fonction de l’âge (exemple avec pente commune)\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les données du fichier anc1dat.csv, éprouvez l’hypothèse que le taux de croissance des esturgeons mâles et femelles de The Pas est le même (données de 1978-1980). Comme mesure du taux de croissance, nous allons utiliser la pente de la régression du log 10 de la longueur à la fourche, lfkl, sur le log 10 de l’âge, l’age.\n\n\nCommençons par examiner les données. Pour faciliter la comparaison, il serait utile de tracer la droite de régression et la trace lowess pour ainsi plus facilement évaluer la linéarité. On peut aussi ajouter un peu de trucs R pour obtenir des légendes plus complètes (remarquez l’utilisation de la commande expression() pour obtenir des indices):\n\nanc1dat &lt;- read.csv(\"data/anc1dat.csv\")\nanc1dat$sex &lt;- as.factor(anc1dat$sex)\nmyplot &lt;- ggplot(data = anc1dat, aes(x=lage,    y=log10(fklngth)))+facet_grid(.~sex)+geom_point()\nmyplot &lt;- myplot+\n  stat_smooth(method = lm, se=FALSE)+\n  stat_smooth(se=FALSE, color=\"red\") +\n  labs(\n    y = expression(log[10]~(Fork~length)),\n    x = expression(log[10]~(Age))\n)\nmyplot\n\n\n\n\n\n\nFigure 14.1: Longueur des esturgeons en fonction de l’age\n\n\n\n\nLa transformation log-log rend la relation linéaire et, à première vue, il ne semble pas y avoir de problème évident avec les conditions d’application. Ajustons donc le modèle complet avec l’interaction:\n\nmodel.full1&lt;-lm(lfkl ~ sex + lage + sex:lage, data = anc1dat)\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nProbabilité que le terme lage*sex n’affecte pas la longueur à la fourche (i.e. que la pente ne diffère pas entre les sexes, et que la différence de taille entre les mâles et femelles ne varie pas avec l’âge)\nAttrape. Notez que j’ai utilisé la fonction Anova() du package car avec un “a” majuscule au lieu de la fonction native anova() (avec un “a” minuscule”) associée aux objets produits par lm() pour obtenir les sommes de carrés de type III. Ces sommes des carrés des écarts de type III (partiels) sont calculées comme si la variable était entrée la dernière dans le modèle et correspondent à la différence entre la variance expliquée par le modèle complet et par le modèle dans lequel seule cette variable est omise. La fonction native anova() donne les sommes des carrés séquentielles, calculées au fur et à mesure que chaque variable est ajoutée au modèle nul avec seulement une ordonnée à l’origine. Dans de rares cas, les sommes des carrés de type I et III sont égales (quand le design est parfaitement orthogonal ou balancé). Dans la vaste majorité des cas, les sommes des carrés de type I et III sont différentes et je vous conseille de toujours utiliser les sommes des carrés de type III dans vos analyses.\nÀ partir de cette analyse, on devrait accepter les hypothèses nulles (1) d’égalité des pentes pour les deux sexes, et (2) que les ordonnées à l’origine sont les mêmes pour les deux sexes. Mais, avant d’accepter ces conclusions, il faut vérifier si les données rencontrent les conditions d’application, comme d’habitude…\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.full1)\n\n\n\n\n\n\nFigure 14.2: Conditions d’applications du modèle model.full1\n\n\n\n\n\n\n\nEn ce qui concerne la normalité, ça a l’air d’aller quoiqu’il y a quelques points, en haut à droite, qui dévient de la droite. Si on effectue le test de Wilk-Shapiro (W = .9764, p = 0.09329), on confirme que les résidus ne dévient pas significativement de la normalité. Les résidus semblent homoscédastiques, mais si vous voulez vous en assurer, vous pouvez l’éprouver par un des tests formels. Ici j’utilise le test Breusch-Pagan, qui est approprié quand certaines des variables indépendantes sont continues (Le test de Levene n’est approprié que lorsqu’il n’y a que des variables discontinues).\n\nbptest(model.full1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model.full1\nBP = 0.99979, df = 3, p-value = 0.8013\n\n\nComme l’hypothèse nulle de ce test est que les résidus sont homoscédastiques, et que p est relativement élevé, le test confirme l’évaluation visuelle. De plus, il n’y a pas de tendance évidente dans les résidus, suggérant qu’il n’y a pas de problème de linéarité. Ce qui peut également être éprouvé formellement:\n\nresettest(model.full1, power = 2:3, type = \"regressor\", data = anc1dat)\n\n\n    RESET test\n\ndata:  model.full1\nRESET = 0.59861, df1 = 2, df2 = 86, p-value = 0.5519\n\n\nLa dernière condition d’application est qu’il n’y a pas d’erreur de mesure sur la variable indépendante continue. On ne peut vraiment éprouver cette condition,, mais on sait que des estimés indépendants de l’âge des poissons obtenus par différents chercheurs donnent des âges qui concordent avec moins de 1-2 ans d’écart., ce qui est inférieur au 10% de la fourchette observée des âges et donc acceptable pour des analyses de modèles de type I (attention ici on ne parle pas des SC de type I, je sais, c’est facile de confondre…)\nVous noterez qu’il y a une observation qui a un résidu normalisé (studentized residual) qui est élevé, i.e. une valeur extrême (cas numéro 49). Éliminez-la de l’ensemble de données et refaites l’analyse. Vos conclusions changent-elles?\n\nmodel.full.no49&lt;-lm(lfkl ~ sex + lage + sex:lage, data = anc1dat[c(-49),])\nAnova(model.full.no49, type=3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value Pr(&gt;F)    \n(Intercept) 0.64255  1 895.9394 &lt;2e-16 ***\nsex         0.00038  1   0.5273 0.4697    \nlage        0.07378  1 102.8746 &lt;2e-16 ***\nsex:lage    0.00022  1   0.3135 0.5770    \nResiduals   0.06239 87                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa conclusion ne change pas après avoir enlevé la valeur extrême. Comme on n’a pas de bonne raison d’éliminer cette valeur, il est probablement mieux de la conserver. Un examen des conditions d’application après avoir enlevé cette valeur révèle qu’elles sont toutes rencontrées.\n\n14.4.2 Cas 2 - Taille en fonction de l’âge (exemple avec des pentes différentes)\n\n\n\n\n\n\nExercice\n\n\n\nLe fichier anc3dat.csv contient des données sur des esturgeons mâles de deux sites (locate) : Lake of the Woods dans le Nord-Ouest de l’Ontario et Chruchill River dans le Nord du Manitoba. En utilisant la même procédure, éprouvez l’hypothèse que la pente de la régression de lfkl sur lage est la même aux deux sites (alors Locate est la variable en catégories et non pas sex). Que concluez-vous?\n\n\n\nanc3dat &lt;- read.csv(\"data/anc3dat.csv\")\nmyplot &lt;- ggplot(data = anc3dat, aes(x = lage, y = log10(fklngth))) +\n  facet_grid(. ~ locate) +\n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  stat_smooth(se = FALSE, color = \"red\") +\n  labs(\n    y = expression(log[10] ~ (Fork ~ length)),\n    x = expression(log[10] ~ (Age))\n  )\nmyplot\nmodel.full2 &lt;- lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nAnova(model.full2, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.62951  1 1078.632 &lt; 2.2e-16 ***\nlage        0.07773  1  133.185 &lt; 2.2e-16 ***\nlocate      0.00968  1   16.591 0.0001012 ***\nlage:locate 0.00909  1   15.575 0.0001592 ***\nResiduals   0.05136 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nFigure 14.3: Longueur des esturgeons en fonction de l’age d’après anc3dat\n\n\n\n\nIci, on rejette les hypothèses nulles (1) que les pentes sont les mêmes dans les deux sites et (2) que les ordonnées à l’origine sont égales. En d’autres mots, si on veut prédire la longueur à la fourche d’un esturgeon à un âge donné précisément, il faut savoir de quel site il provient. Puisque les pentes diffèrent, il faut estimer des régressions séparées.\nMais avant d’accepter ces conclusions, on doit se convaincre que les conditions d’application sont rencontrées:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2,2))\nplot(model.full2)\n\n\n\n\n\n\nFigure 14.4: Conditions d’applications du modèle model.full2\n\n\n\n\n\n\n\nSi on examine les résidus selon les méthodes habituelles, on voit qu’il n’y a pas de problème de linéarité, ni d’homoscédasticité (BP = 2.8721, p = 0.4118). Cependant, le test de Wilk-Shapiro est significatif (W=0.97, p = 0.03). Étant donné la taille assez grande de l’échantillon (N=92), ce test a beaucoup de puissance, même si la déviation de normalité ne semble pas très grande. Compte-tenu de la robustesse relative des LM, de la taille de l’échantillon, on ne devrait pas ^tre trop inquiet de cette déviation de normalité.\nDonc, comme les conditions des LM sont suffisamment remplies, on peut accepter les résultats donnés par R. Tous les termes sont significatifs (location, lage, interaction). Ce modèle complet est équivalent à ajuster des régressions séparées pour chaque site. Pour obtenir les coefficients, on peut ajuster des régressions simples sur chaque sous-ensemble, ou extraire les coefficients ajustés du modèle complet:\n\nmodel.full2\n\n\nCall:\nlm(formula = lfkl ~ lage + locate + lage:locate, data = anc3dat)\n\nCoefficients:\n            (Intercept)                     lage       locateNELSON        \n                 1.2284                   0.3253                   0.2207  \nlage:locateNELSON        \n                -0.1656  \n\n\nPar défaut, la variable locate est encodée comme 0 pour le site qui vient le premier en ordre alphabétique (LofW) et 1 pour l’autre (Nelson). Les régressions pour chaque site deviennent donc:\nPour LofW: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 0 - 0.1656 \\times 0 \\times lage \\\\\n&= 1.2284 + 0.3253 \\times lage\n\\end{aligned}\\]\nPour Nelson: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 1 - 0.1656 \\times 1 \\times lage \\\\\n&= 1.4491 + 0.1597 \\times lage\n\\end{aligned}\\]\nVous pouvez vérifier en ajustant séparément les régressions pour chaque site:\n\nby(anc3dat, anc3dat$locate,function(x) lm(lfkl~lage, data=x))\n\nanc3dat$locate: LOFW        \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.2284       0.3253  \n\n------------------------------------------------------------ \nanc3dat$locate: NELSON      \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.4491       0.1597",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#le-modèle-dancova",
    "href": "36-ancova_glm.html#le-modèle-dancova",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.5 Le modèle d’ANCOVA",
    "text": "14.5 Le modèle d’ANCOVA\nSi le test d’homogénéité des pentes indique qu’elles diffèrent, alors on devrait estimer des régressions individuelles pour chaque niveau des variables discontinues. Cependant, si on accepte l’hypothèse d’égalité des pentes, l’étape suivante est de comparer les ordonnées à l’origine. Selon la “vieille école” i.e. l’approche traditionnelle, on ajuste un modèle avec la variable catégorique et la variable continue, mais sans interaction (le modèle ANCOVA sensus stricto) et on utilise la somme des carrés des écarts de type III, disons avec la fonction Anova(). C’est ce que la majorité des manuels de biostatistiques présentent.\nL’autre approche consiste à utiliser les résultats de l’analyse du modèle complet, et tester la signification de chaque terme à partir des sommes des carrés partiels. C’est plus rapide, mais moins puissant. Dans la plupart des cas, cette perte de puissance n’est pas trop préoccupante, sauf lorsque le modèle est très complexe et contient de nombreuses interactions non-significatives. Je vous suggère d’utiliser l’approche simplifiée, et de n’utiliser l’approche traditionnelle que lorsque vous acceptez l’hypothèse d’égalité des ordonnées à l’origine. Pourquoi?\nPuisque l’approche simplifiée est moins puissante, si vous rejetez quand même H0, alors votre conclusion ne changera pas, mais sera seulement renforcée, en utilisant l’approche traditionnelle.\nIci, je vais comparer l’approche traditionnelle et l’approche simplifiée. Rappelez-vous que vous voulez évaluer l’égalité des ordonnées à l’origine après avoir déterminé que les pentes étaient égales. Éprouver l’égalité des ordonnées à l’origine quand les pentes diffèrent (ou, si vous préférez, quand il y a une interaction) est rarement sensé, peut facilement être mal interprété, et ne devrait être effectué que rarement.\nDe retour aux données de anc1dat.csv, en comparant la relation entre lfkl et lage entre les sexes, nous avions obtenu les résultats suivants pour le modèle complet avec interactions:\n\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn avait déjà conclu que la pente ne varie pas entre les sexes (i.e. l’interaction n’est pas significative). Notez que la p-valeur associée au sexe (0.4795) n’est pas significative non plus.\nDe l’autre côté, selon l’approche traditionnelle, l’inférence quand à l’effet du sexe se fait à partir du modèle réduit (le modèle ANCOVA sensus stricto):\n\nmodel.ancova &lt;- lm(lfkl ~ sex + lage, data = anc1dat)\nAnova(model.ancova, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value Pr(&gt;F)    \n(Intercept) 1.13480  1 1410.1232 &lt;2e-16 ***\nsex         0.00149  1    1.8513 0.1771    \nlage        0.14338  1  178.1627 &lt;2e-16 ***\nResiduals   0.07162 89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.093992 -0.018457 -0.000876  0.022491  0.081161 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.225533   0.032636  37.552   &lt;2e-16 ***\nsexMALE         -0.008473   0.006228  -1.361    0.177    \nlage             0.327253   0.024517  13.348   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02837 on 89 degrees of freedom\nMultiple R-squared:  0.696, Adjusted R-squared:  0.6892 \nF-statistic: 101.9 on 2 and 89 DF,  p-value: &lt; 2.2e-16\n\n\nDans ce modèle, sex n’est pas significatif et on conclue donc que l’ordonnée à l’origine ne diffère pas entre les sexes. Notez que la pvaleur est plus petite (0.1771 vs 0.4795), ce qui reflète la puissance accrue de l’approche traditionnelle. Toutefois, les conclusions sont les mêmes: les ordonnées à l’origine ne diffèrent pas.\n\n\n\n\n\n\nExercice\n\n\n\nEn examinant les graphiques diagnostiques, vous noterez qu’il y a trois observations dont la valeur absolue du résidu est grande (cas 19, 49, et 50). Ces observations pourraient avoir un effet disproportionné sur les résultats de l’analyse. Éliminez-les et refaites l’analyse. Les conclusions changent-elles ?\n\n\n\nmodel.ancova.nooutliers &lt;- lm(lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19),])\nAnova(model.ancova.nooutliers, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value  Pr(&gt;F)    \n(Intercept) 1.09160  1 1896.5204 &lt; 2e-16 ***\nsex         0.00232  1    4.0374 0.04764 *  \nlage        0.13992  1  243.0946 &lt; 2e-16 ***\nResiduals   0.04950 86                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova.nooutliers)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19), \n    ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.058397 -0.018469 -0.000976  0.020696  0.040288 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.224000   0.028106  43.549   &lt;2e-16 ***\nsexMALE         -0.010823   0.005386  -2.009   0.0476 *  \nlage             0.328604   0.021076  15.591   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02399 on 86 degrees of freedom\nMultiple R-squared:  0.7706,    Adjusted R-squared:  0.7653 \nF-statistic: 144.4 on 2 and 86 DF,  p-value: &lt; 2.2e-16\n\n\nOuch! Les résultats changent. Il faudrait donc rejeter l’hypothèse nulle et conclure que les ordonnées à l’origine diffèrent! Une conclusion qualitativement différente de celle obtenue en considérant toutes les données. Pourquoi? Il y a deux raisons possibles : (1) les valeurs extrêmes influencent beaucoup les régressions ou (2) l’exclusion des valeurs extrêmes permet d’augmenter la puissance de détection d’une différence. La première explication est moins plausible parce que les valeurs extrêmes n’avaient pas une grande influence (leverage faible). Alors, la deuxième explication est plus plausible et vous pouvez le vérifier en faisant des régressions pour chaque sexe sans et avec les valeurs extrêmes. Si vous le faites, vous noterez que les ordonnées à l’origine pour chaque sexe ne changent presque pas alors que leurs erreurs-types changent beaucoup.\n\n\n\n\n\n\nExercice\n\n\n\nAjustez une régression simple entre lfkl et lage pour l’ensemble complet de données et aussi pour le sous-ensemble sans les 3 valeurs déviantes. Comparez ces modèles avec les modèles d’ANCOVA ajustés précédemment. Que concluez-vous ? Quel modèle, d’après vous, a le meilleur ajustement aux données ? Pourquoi ?\n\n\nLe modèle en excluant les valeurs extrêmes:\n\nmodel.linear.nooutliers&lt;-lm(lfkl ~ lage,data = anc1dat[c(-49, -50, -19),])\nsummary(model.linear.nooutliers)\n\n\nCall:\nlm(formula = lfkl ~ lage, data = anc1dat[c(-49, -50, -19), ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.055567 -0.017809 -0.002944  0.021272  0.044972 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.20378    0.02670   45.09   &lt;2e-16 ***\nlage         0.34075    0.02054   16.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02441 on 87 degrees of freedom\nMultiple R-squared:  0.7598,    Adjusted R-squared:  0.7571 \nF-statistic: 275.2 on 1 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nPour la régression simple (sans les valeurs extrêmes) on obtient un R 2 de 0.76 et une erreur-type des résidus de 0.02441, En comparant à l’erreur-type des résidus du modèle d’ANCOVA (0.02399) on réalise que la qualité des prédictions est essentiellement la même, même en ajustant des ordonnées à l’origine différentes pour chaque groupe. Par conséquent, les bénéfices de l’inclusion d’un terme pour les différentes ordonnées à l’origine sont faibles alors que le coût, en terme de complexité du modèle, est élevé (33% d’augmentation du nombre de termes pour un très faible amélioration de la qualité d’ajustement). Si vous examinez les résidus de ce modèle, vous trouverez qu’ils sont à peu près O.K.)\nSi on ajuste une régression simple sur toutes les données, on obtient:\n\nmodel.linear&lt;-lm(lfkl ~ lage, data = anc1dat)\nsummary(model.linear)\n\n\nCall:\nlm(formula = lfkl ~ lage, data = anc1dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090915 -0.018975 -0.002587  0.021270  0.085273 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.21064    0.03089   39.19   &lt;2e-16 ***\nlage         0.33606    0.02376   14.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0285 on 90 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6863 \nF-statistic: 200.1 on 1 and 90 DF,  p-value: &lt; 2.2e-16\n\n\nEncore une fois, l’erreur-type des résidus (0.0285) pour cette régression unique est semblable à la variance du modèle d’ANCOVA (0.02837) et le modèle simplifié prédit presque aussi bien que le modèle plus complexe. (Ici encore, toutes les conditions d’application semblent remplies, si ce n’est de la valeur extrême).\nDonc, dans les deux cas (avec ou sans les valeurs extrêmes), l’addition d’un terme supplémentaire pour le sexe n’ajoute pas grand-chose. Il semble donc que le meilleur modèle soit celui de la régression simple. Un estimé raisonnablement précis de la taille des esturgeons peut être obtenu de la régression commune sur l’ensemble des résultats.\nNote: Il est fréquent que l’élimination de valeurs extrêmes en fasse apparaître d’autres. C’est parce que ces valeurs extrêmes dépendent de la variabilité résiduelle. Si on élimine les valeurs les plus déviantes, la variabilité résiduelle diminue, et certaines observations qui n’étaient pas si déviantes que cela deviennent proportionnellement plus déviantes. Notez aussi qu’en éliminant des valeurs extrêmes, l’effectif diminue et que la puissance décroît. Il faut donc être prudent.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#comparer-lajustement-de-modèles",
    "href": "36-ancova_glm.html#comparer-lajustement-de-modèles",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.6 Comparer l’ajustement de modèles",
    "text": "14.6 Comparer l’ajustement de modèles\nComme vous venez de le voir, le processus d’ajustement de modèles est itératif. La plupart du temps il y a plus d’un modèle qui peut être ajusté aux données et c’est à vous de choisir celui qui est le meilleur compromis entre la qualité d’ajustement (qu’on essaie de maximiser) et la complexité (qu’on essaie de minimiser). La stratégie de base en ajustant des modèles linéaires (ANOVA, régression, ANCOVA) est de privilégier le modèle le plus simple si la qualité d’ajustement n’est pas significativement plus mauvaise. R peut calculer une statistique F vous permettant de comparer l’ajustement de deux modèles. Dans ce cas, l’hypothèse nulle est que la qualité d’ajustement ne diffère pas entre les deux modèles.\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les données de anc1dat comparez l’ajustement du modèle ANCOVA et de la régression commune:\n\n\n\nanova(model.ancova,model.linear)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ sex + lage\nModel 2: lfkl ~ lage\n  Res.Df      RSS Df  Sum of Sq      F Pr(&gt;F)\n1     89 0.071623                            \n2     90 0.073113 -1 -0.0014899 1.8513 0.1771\n\n\nLa fonction anova() utilise la différence entre la somme des carrés des deux modèles et la divise par la différence entre le nombre de degrés de liberté pour obtenir un carré moyen. Ce carré moyen est utilisé au numérateur et est divisé par la variance résiduelle du modèle le plus complexe pour obtenir la statistique F. Dans ce cas-ci, le test de F n’est pas significatif, et on conclut que les deux modèles ont une qualité d’ajustement équivalente, et qu’on devrait donc privilégier le modèle le plus simple, la régression linéaire simple.\n\n\n\n\n\n\nExercice\n\n\n\nRefaites le même processus avec le données de anc3dat, ajustez le modèle complet avec interaction (LFKL~LAGE+LOCATE+LAGE:LOCATE) et sans interaction (LFKL~LAGE+LOCATE), Comparez l’ajustement des deux modèles, que concluez vous?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.full.anc3dat&lt;-lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nmodel.ancova.anc3dat&lt;-lm(lfkl ~ lage + locate, data = anc3dat)\nanova(model.full.anc3dat,model.ancova.anc3dat)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ lage + locate + lage:locate\nModel 2: lfkl ~ lage + locate\n  Res.Df      RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1     88 0.051358                                   \n2     89 0.060448 -1 -0.0090901 15.575 0.0001592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCette fois-ci, le modèle plus complexe s’ajuste significativement mieux aux données. (Pas surprenant puisque nous avions précédemment conclu que l’interaction est significative avec ces données.)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#bootstrap",
    "href": "36-ancova_glm.html#bootstrap",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.7 Bootstrap",
    "text": "14.7 Bootstrap\n\n############################################################\n######\n# Bootstrap analysis\n# Bootstrap analysis BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\nlibrary(boot)\n\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata &lt;- anc3dat\n\n# create a myformula variable containing the formula for the model to be fitted\nmyformula &lt;- as.formula(lfkl ~ lage + locate + lage:locate)\n\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(data = mydata, statistic = bs, R = 1000, formula = myformula)\n\n# view results\nresults\nboot_res &lt;- summary(results)\nrownames(boot_res) &lt;- names(results$t0)\nboot_res\n\nop &lt;- par(ask = TRUE)\nfor (i in 1:length(results$t0)) {\n  plot(results, index = i)\n  title(names(results$t0)[i])\n}\npar(op)\n\n# get 95% confidence intervals\nfor (i in 1:length(results$t0)) {\n  cat(\"\\n\", names(results$t0)[i],\"\\n\")\n  print(boot.ci(results, type = \"bca\", index = i))\n}",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#permutation-test",
    "href": "36-ancova_glm.html#permutation-test",
    "title": "\n14  ANCOVA et modèle linéaire général\n",
    "section": "\n14.8 Permutation test",
    "text": "14.8 Permutation test\n\n############################################################\n##########\n# Permutation test\n#\n# using lmperm library\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata&lt;-anc3dat\n# create a myformula variable containing the formula for the\n# model to be fitted\nmyformula&lt;-as.formula(lfkl ~ lage + locate + lage:locate)\nrequire(lmPerm2)\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate p-values for each term by permutation\n# Note that lmp centers numeric variable by default, so to\n# get results that are\n# consistent with standard models, it is necessary to set\ncenter=FALSE\nmymodelProb &lt;- lmp(myformula, data = mydata, center=FALSE,\nperm = \"Prob\")\nsummary(mymodel)\nsummary(mymodelProb)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html",
    "href": "42-model_freq.html",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "",
    "text": "15.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#set-freq",
    "href": "42-model_freq.html#set-freq",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "",
    "text": "les paquets R:\n\nvcd\nvcdExtra\ncar\n\n\nles fichiers de données\n\nUSPopSurvey.csv\nloglin.csv\nsturgdat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#organisation-des-données-3-formats",
    "href": "42-model_freq.html#organisation-des-données-3-formats",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.2 Organisation des données: 3 formats",
    "text": "15.2 Organisation des données: 3 formats\nLes résultats de certaines expériences sont sous forme de fréquences, par exemple le nombre de plantes infectées par un pathogène sous différents régimes d’infection, ou le nombre de tortues mâles et femelles qui éclosent à différentes températures (oui, chez les tortues le sexe dépends de la température!), etc. La question statistique qui se pose généralement est de savoir si la proportion des observations dans chaque catégorie (infecté vs non infecté, mâle vs femelle, etc) diffère significativement entre les traitements (régime d’infection ou température dans les deux exemples). Pour répondre à cette question, on peut organiser les données de manière à refléter comment les observations se retrouvent dans chaque catégorie. Il existe 3 façons d’organiser ces données. Vous devriez être capable de choisir la manière appropriée pour votre analyse, et savoir convertir entre elles avec R.\nLe fichier USPopSurvey.csv contient les donnée de recensement d’une ville du midwest américain en 1980:\n\nUSPopSurvey &lt;- read.csv(\"data/USPopSurvey.csv\")\nUSPopSurvey\n\n   ageclass    sex frequency\n1       0-9 female     17619\n2     10-19 female     17947\n3     20-29 female     21344\n4     30-39 female     19138\n5     40-49 female     13135\n6     50-59 female     11617\n7     60-69 female     11053\n8     70-79 female      7712\n9       80+ female      4114\n10      0-9   male     17538\n11    10-19   male     18207\n12    20-29   male     21401\n13    30-39   male     18837\n14    40-49   male     12568\n15    50-59   male     10661\n16    60-69   male      9374\n17    70-79   male      5348\n18      80+   male      1926\n\n\nNotez qu’il y a 18 lignes et 3 colonnes dans ce fichier. Chaque ligne donne le nombre de personnes (frequency) pour un sexe et une classe d’âge. Il y a 239539 individus qui ont été classifiés selon les 18 catégories (2 sexes x 9 classes d’âge). Cette manière de représenter les données est sous le format de fréquences (frequency form). C’est un format compact permettant d’enregistrer les données quand il y a seulement des variables catégoriques à représenter.\nLorsqu’il y a des variables continues, ce format ne peut être utilisé. Les données doivent être enregistrée sous le format de cas (case form) dans laquelle chaque observation (individu) est représenté par une ligne dans le fichier, et où chaque variable est représentée par une colonne. Le package vcdExtra contient la fonction expand.dft() qui permet de convertir de la forme de fréquence à la forme de cas. Par exemple, pour créer un data frame avec 239439 lignes et 2 colonnes (sex et ageclass) à partir du data frame USPopSurvey:\n\nUSPopSurvey.caseform &lt;- expand.dft(USPopSurvey, freq = \"frequency\")\nhead(USPopSurvey.caseform)\n\n  ageclass    sex\n1      0-9 female\n2      0-9 female\n3      0-9 female\n4      0-9 female\n5      0-9 female\n6      0-9 female\n\ntail(USPopSurvey.caseform)\n\n       ageclass  sex\n239534      80+ male\n239535      80+ male\n239536      80+ male\n239537      80+ male\n239538      80+ male\n239539      80+ male\n\n\nCes données peuvent finalement être organisées sous le format de tableau (table form) de contingence où chacune des n variables est représentée par une dimension d’un tableau n-dimensionnel (dans notre exemple on a 2 variables, sexe et classe d’âge, et les rangées pourraient représenter les classes d’âge et les colonnes chaque sexe). Les cellules de ce tableau contiennent les fréquences. Le format tableau peut être obtenu du format de fréquence ou de cas par la commande xtabs() :\n\n# convert case form to table form\nxtabs(~ ageclass + sex, USPopSurvey.caseform)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n# convert frequency form to table form\nxtabs(frequency ~ ageclass + sex, data = USPopSurvey)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n\n\n(#tab:unnamed-chunk-1)Fonctions permettant la conversion de données de fréquences entre les différents formats.\n\n\n\n\n\n\n\nDe (ligne) \\ Vers (colonne)\nCas\nFréquence\nTableau\n\n\n\nCas\n\nxtabs(~ A + B)\ntable(A, B)\n\n\nFréquence\nexpand.dft(X)\n\nxtabs(count ~ A + B)\n\n\nTableau\nexpand.dft(X)\nas.data.frame(X)",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#visualiser-graphiquement-les-tableaux-de-contingence-et-test-dindépendance",
    "href": "42-model_freq.html#visualiser-graphiquement-les-tableaux-de-contingence-et-test-dindépendance",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.3 Visualiser graphiquement les tableaux de contingence et test d’indépendance",
    "text": "15.3 Visualiser graphiquement les tableaux de contingence et test d’indépendance\nLes tableaux de contingence peuvent servir à éprouver l’hypothèse d’indépendance des observations. Ceci équivaut à répondre à la question: est-ce que la classification des observations selon une variable (par exemple sex) indépendante de la classification par une autre variable (par exemple ageclass). En autres mots, est-ce que la proportion des mâles et femelles indépendante de l’âge ou varie avec l’âge?\nLe package vcd inclut la fonction mosaic() qui permet de représenter graphiquement le contenu d’un tableau de contingence:\n\nlibrary(vcd)\nUSTable &lt;- xtabs(frequency ~ ageclass + sex, data = USPopSurvey) # save the table form as USTable dataframe\n# Mosaic plot of the contingency table\nmosaic(USTable)\n\n\n\n\n\n\nFigure 15.1: Représentation mosaique de la proportion des sexes par classe d’age\n\n\n\n\nCette mosaïque représente la proportion des observations dans chaque combinaison de catégories (ici il y a 18 catégories, 2 sexes x 9 classes d’âge). Les catégories contenant une plus grande proportion d’observations sont représentées par de plus grands rectangles. Visuellement, on peut voir que la proportion des mâles et femelles est approximativement égale chez les jeunes, mais que la proportion des femelles augmente chez les personnes âgées.\nLe test de Chi carré permet d’éprouver l’hypothèse nulle que la proportion des mâles et femelles ne change pas avec l’âge (est indépendante de l’âge):\n\n# Test of independence\nchisq.test(USTable) # runs chi square test of independence of sex and age class\n\n\n    Pearson's Chi-squared test\n\ndata:  USTable\nX-squared = 1162.6, df = 8, p-value &lt; 2.2e-16\n\n\nLa valeur p étant très faible, on rejette donc l’hypothèse nulle que âge et sexe sont indépendants. Ces graphiques mosaïques peuvent êtres colorés pour souligner les catégories qui contribuent le plus à cette dépendance:\n\n# Mosaic plot of the contingency table with shading\nmosaic(USTable, shade = TRUE)\n\n\n\n\n\n\nFigure 15.2: Représentation mosaique de la proportion des sexes par classe d’age avec échelle de couleur\n\n\n\n\nLa couleur de chaque rectangle est proportionnelle à la déviation des fréquences observées de ce qui serait attendu si l’âge et le sexe étaient indépendants. Les classes d’âge 40-49 et 50-59 ont un rapport des sexe approximativement égal à celui de toutes les classes d’âge réunies. Il y a plus de jeunes mâles et de femelles âgées que si le rapport des sexe ne variait pas avec l’âge.et ces rectangles sont colorés en bleu. De l’autre côté, il y a moins de jeunes femelles et de mâles âgés que si le rapport des sexe était indépendant de l’âge, et ces rectangles sont en rouge. La valeur p à la droite de la figure est pour le test de Chi carré qui éprouve l’hypothèse nulle d’indépendance pour l’ensemble des observations, toutes classes d’âge confondues.\nL’estimation de la valeur p associée à la statistique du Chi carré est approximative lorsque les fréquences attendues sont faibles dans certaines cellules, et ce particulièrement pour les tableaux de contingence 2x2. Deux options permettant des valeurs p plus exactes sont préférées dans ce cas, et le choix dépends du nombre total d’observations. Pour de grands échantillons (comme ici avec plus de 200,000 observations!), une approche par simulation de type Monte Carlo est suggérée et peut être obtenue en ajoutant simulate.p.value=TRUE comme argument à la fonction chisq.test() :\n\n# Monte-carlo estimation of p value (better for small n)\nchisq.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 10000\n    replicates)\n\ndata:  USTable\nX-squared = 1162.6, df = NA, p-value = 9.999e-05\n\n\nIci, la simulation a été faite B=10000 fois, et la valeur de Chi carré observée avec les données réelles n’a jamais été observée. Par conséquent, p a été estimé à 1/10001=9.999e-05, qui est beaucoup plus élevé que la valeur p estimée à partir de la distribution théorique de Chi carré (p&lt; 2.2e-16). Cette différence est due au moins en partie à un artéfacts de la simulation. Pour obtenir des valeurs p de l’ordre de 1e-16, il faut effectuer au moins 10 16 simulations. Et je ne suis pas aussi patient que ça!\nPour de petits tableaux de contingence avec des fréquences attendues petites, le test exact de Fisher peut servir à estimer la valeur p associée à l’hypothèse d’indépendance. Mais ce test ne peut être effectué avec de grands échantillons, comme ici:\n\n# Fisher exact test for contingency tables (small samples and small tables)\nfisher.test(USTable) # fails here because too many observations\n\nError in fisher.test(USTable): FEXACT error 40.\nOut of workspace.\n\nfisher.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    10000 replicates)\n\ndata:  USTable\np-value = 9.999e-05\nalternative hypothesis: two.sided",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#régression-de-poisson-une-alternative-au-test-de-chi-carré-pour-les-tableaux-de-contingence",
    "href": "42-model_freq.html#régression-de-poisson-une-alternative-au-test-de-chi-carré-pour-les-tableaux-de-contingence",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.4 Régression de Poisson: une alternative au test de Chi carré pour les tableaux de contingence",
    "text": "15.4 Régression de Poisson: une alternative au test de Chi carré pour les tableaux de contingence\nRendu à ce stade, vous devriez avoir appris à apprécier la flexibilité et la généralité des modèles linéaires, et réaliser que le test de t est un cas spécial d’un modèle linéaire avec une variable indépendante catégorique. L’analyse des tableaux de contingence par le test du Chi carré peut également être généralisé. Un modèle linéaire généralisé pour une distribution de Poisson peut être utilisé quand la variable dépendante est une fréquence d’observations et les variables indépendantes sont catégorique (comme pour les tableaux de contingence, on parle alors de modèles log-linéaires), continue (régression Poisson), ou une combinaison de variables indépendante continues et catégoriques (aussi appelé régression de Poisson, mais avec des variables catégoriques en plus, analogue à l’ANCOVA sensu largo).\nCes modèles prédisent le logarithme naturel de la fréquence des observations en fonction des variables indépendantes. Comme pour les modèles linéaires qui présument de la normalité des résidus, on peut évaluer la qualité d’ajustement du modèle (par AICc par exemple) et la signification statistique des termes du modèle (par exemple en comparant l’ajustement d’un modèle “complet” et celui d’un modèle qui exclue un terme à tester). On peut également obtenir des estimés des paramètre pour chaque terme dans le modèle, avec des intervalles de confiance et des valeur p pour l’hypothèse nulle que ce terme n’a pas d’influence sur la fréquence.\nLa fonction glm() avec l’option family=poisson() permet l’estimation, par la méthode du maximum de vraisemblance, de modèles linéaires pour des fréquences. Comparativement aux modèles linéaires vus précédemment, une des particularité de ces modèles est que seuls les termes d’interaction sont d’intérêt. En partant des données de recensement en forme tableau, on peut ajuster un glm aux fréquences observées par sexe et classe d’âge par:\n\nmymodel &lt;- glm(frequency ~ sex * ageclass, family = poisson(), data = USPopSurvey)\nsummary(mymodel)\n\n\nCall:\nglm(formula = frequency ~ sex * ageclass, family = poisson(), \n    data = USPopSurvey)\n\nCoefficients:\n                       Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)            9.776733   0.007534 1297.730  &lt; 2e-16 ***\nsexmale               -0.004608   0.010667   -0.432   0.6657    \nageclass10-19          0.018445   0.010605    1.739   0.0820 .  \nageclass20-29          0.191793   0.010179   18.842  &lt; 2e-16 ***\nageclass30-39          0.082698   0.010441    7.921 2.36e-15 ***\nageclass40-49         -0.293697   0.011528  -25.477  &lt; 2e-16 ***\nageclass50-59         -0.416508   0.011951  -34.850  &lt; 2e-16 ***\nageclass60-69         -0.466276   0.012134  -38.428  &lt; 2e-16 ***\nageclass70-79         -0.826200   0.013654  -60.511  &lt; 2e-16 ***\nageclass80+           -1.454582   0.017316  -84.004  &lt; 2e-16 ***\nsexmale:ageclass10-19  0.018991   0.014981    1.268   0.2049    \nsexmale:ageclass20-29  0.007275   0.014400    0.505   0.6134    \nsexmale:ageclass30-39 -0.011245   0.014803   -0.760   0.4475    \nsexmale:ageclass40-49 -0.039519   0.016416   -2.407   0.0161 *  \nsexmale:ageclass50-59 -0.081269   0.017136   -4.742 2.11e-06 ***\nsexmale:ageclass60-69 -0.160154   0.017633   -9.083  &lt; 2e-16 ***\nsexmale:ageclass70-79 -0.361447   0.020747  -17.422  &lt; 2e-16 ***\nsexmale:ageclass80+   -0.754343   0.029598  -25.486  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.3611e+04  on 17  degrees of freedom\nResidual deviance: 6.5463e-12  on  0  degrees of freedom\nAIC: 237.31\n\nNumber of Fisher Scoring iterations: 2\n\n\nL’ajustement du modèle complet, avec L’interaction triple sex:ageclass interaction, permet à la proportion des mâles et femelles de changer entre les classes d’âge, et donc d’estimer exactement les fréquences observées pour chaque combinaison de sexe et classe d’âge (notez que les résidus (deviance residuals) sont tous 0, et que l’estimé de déviance résiduelle est également approximativement zéro).\nUn masochiste peut utiliser le tableau des coefficients pour obtenir la fréquence prédite pour les différentes catégories. Les fréquences prédites, comme pour l’ANOVA à critères multiple, sont obtenus en additionnant les coefficients appropriés. Puisque, en R, le premier niveau d’une variable catégorique (facteur) en ordre alphabétique) est utilisé comme référence, l’ordonnée à l’origine (9.776733) est la valeur prédite pour le logarithme naturel de la fréquence des femelles dans la première classe d’âge (0 to 9). En effet, 9.776733 est approximativement égal à 17619, le nombre observé de femelles dans cette classe d’âge.\nPour les mâles dans la classe d’âge 80+, il faut calcule l’antilog du coefficient pour l’ordonnée à l’origine (pour les femelles dans la première classe d’âge), plus le coefficient pour sexmale (égal à la différence du log de la fréquence entre les femelles et les mâles), plus le coefficient pour la classe d’âge 80+ qui corresponds à la différence de fréquence entre cette classe d’âge et la classe d’âge de référence, plus le coefficient pour l’interaction sexmale:ageclass80+ (qui corresponds à la différence de proportion de mâles dans cette classe d’âge par rapport à la classe d’âge de référence). Ceci donne: ln(frequency)=9.776733-0.004608-1.454582-0.754343=7.5632, et la fréquence est égale à e 7.5632 =1926\nIl y a de nombreuses valeur p dans ce tableau, mais elle ne sont en général pas très utiles. Pour éprouver l’hypothèse que l’effet du sexe sur la fréquence est identique dans chaque classe d’âge (i.e. que sexe et âge sont indépendants), vous devez ajuster un modèle qui exclut cette interaction (sex:ageclass) et déterminer comment l’ajustement du modèle est affecté.\nLa fonction Anova() du package car permet de prendre un raccourci:\n\nAnova(mymodel, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n             LR Chisq Df Pr(&gt;Chisq)    \nsex               0.2  1     0.6657    \nageclass      21074.6  8     &lt;2e-16 ***\nsex:ageclass   1182.2  8     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLes arguments type=3 and test=\"LR\" font en sorte que le test effectué pour comparer le modèle complet aux modèles réduits est les test de Chi carré sur le rapport de vraisemblance (Likelihood Ratio Chi-Square) à partir de la variance résiduelle, et que c’est un test partiel, et non séquentiel.\nSelon ces tests, il n’y a pas d’effet principal de sex (p=0.667) mais il y a un effet principal de ageclass et une interaction significative sex:ageclass. L’interaction significative signifie que l’effet du sexe sur la fréquence varie selon les classes d’âge, bref que le rapport des sexe varie avec l’âge. L’effet principal de ageclass signifie que la fréquence des individus varie avec l’âge dans la population recensée (i.e. que certaines classes d’âge sont plus populeuses que d’autres). L’absence d’un effet principal du sexe suggère qu’il y a approximativement le même nombre de mâles et femelles dans l’échantillon (quoique, puisqu’il y a une interaction, vous devez être prudents en faisant cette déclaration. C’est “vrai” au total, mais semble incorrect pour certaines classes d’âge).",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#tester-une-hypothèse-extrinsèque",
    "href": "42-model_freq.html#tester-une-hypothèse-extrinsèque",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.5 Tester une hypothèse extrinsèque",
    "text": "15.5 Tester une hypothèse extrinsèque\nLe test d’indépendance ci-dessus éprouve une hypothèse intrinsèque parce que les proportions utilisées pour calculer les valeurs attendues et tester l’indépendance sont celles observées (i.e. la proportion des mâles et femelles dans tout l’échantillon, et la proportion des individus dans chaque classe d’âge).\nPour éprouver l’hypothèse (extrinsèque) que le rapport des sexes est 1:1 pour les individus les plus jeunes (ageclass 0-9), on doit produire le tableau 2X2 des fréquences observées et attendues. Les fréquences attendues sont obtenues simplement en divisant le total des mâles et femelles par 2.\nCode R pour créer et analyser un tableau de contingence 2X2 et éprouver une hypothèse extrinsèque\n\n### Produce a table of obs vs exp for 0-9 age class\nPopn0.9 &lt;- rbind(c(17578, 17578), c(17619, 17538))\n### Run X2 test on above table\nchisq.test(Popn0.9, correct = F) ### X2 without Yates\nchisq.test(Popn0.9) ### X2 with Yates\n\n\n\n\n\n\n\nExercice\n\n\n\nÉprouvez l’hypothèse nulle que la proportion de mâles et femelles à la naissance est égale. Que concluez-vous? Croyez-vous que ces données sont appropriées pour tester cette hypothèse?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nchisq.test(Popn0.9, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  Popn0.9\nX-squared = 0.093309, df = 1, p-value = 0.76\n\nchisq.test(Popn0.9)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  Popn0.9\nX-squared = 0.088758, df = 1, p-value = 0.7658\n\n\n\n\n\nNotez que pour un tableau 2X2, on devrait utiliser une correction de Yates ou un test de Fisher. Le test de Fisher ne pouvant être utilisé lorsque l’échantillon dépasse 200, on utilise la correction de Yates. Selon cette analyse, on accepte l’hypothèse nulle que le rapport des sexes est 1:1à la naissance. Ceci dit, ces données ne sont pas très appropriées pour éprouver l’hypothèse car la première classe d’âge est trop grossière. Il est possible que le rapport des sexes à la naissance soit différent de 1:1 mais que la mortalité différentielle des deux sexes compense au cours des 9 premières années (par exemple si il y a plus de mâles à la naissance, mais que les jeunes garçons ont une survie plus faible au cours de leurs 9 premières années). Dans un tel cas, le rapport des sexes n’est PAS de 1:1 à la naissance, mais on accepte l’hypothèse nulle à partir des données dans la classe d’âge 0-9.",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#régression-de-poisson-pour-lanalyse-de-tableaux-de-contingence-à-plusieurs-critères",
    "href": "42-model_freq.html#régression-de-poisson-pour-lanalyse-de-tableaux-de-contingence-à-plusieurs-critères",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.6 Régression de Poisson pour l’analyse de tableaux de contingence à plusieurs critères",
    "text": "15.6 Régression de Poisson pour l’analyse de tableaux de contingence à plusieurs critères\nLe principe d’éprouver l’indépendance en examinant les interactions peut être utilisé avec les tableaux de contingence à plusieurs critères. Par exemple, examinons si la température (2 niveaux: base et haute) et l’éclairage (2 niveaux: bas et haut) affectent si des plantes sont infectées (2 niveaux: infecté et non-infecté) par un pathogène. On peut représenter ces données par un tableau de contingence à 3 critères (température, lumière, statut d’infection).\nL’ajustement de modèles log-linéaires à des données de fréquence implique que l’on éprouve plusieurs modèles en les comparant au modèle complet (saturé). Une série de modèles contenant tous les termes sauf une des interactions qui nous intéressent est produite, et l’ajustement de chaque modèle est comparé à celui du modèle complet. Si la réduction de la qualité d’ajustement n’est pas significative, cela implique que l’interaction manquante contribue peu à la qualité de l’ajustement. Par contre, si le modèle réduit s’ajuste nettement moins bien aux données, alors l’interaction manquante contribue beaucoup à l’ajustement du modèle complet. Comme pour les tableaux de contingence 2X2, les termes qui nous intéressent le plus sont les interactions, pas les effets principaux, si l’on teste pour l’indépendance des différents facteurs.\nLe fichier loglin.csv contient les fréquences (frequency) des plantes infectées ou non infectées (infected) à basse et haute température (temperature) à basse et haute lumière (light). Pour visualiser ces données et déterminer si le taux d’infection dépends de la lumière et de la température, on peut faire une figure mosaïque et ajuster un modèle log-linéaire:\n\nloglin &lt;- read.csv(\"data/loglin.csv\")\n# Convert from frequency form to table form for mosaic plot\nloglinTable &lt;- xtabs(frequency ~ temperature + light + infected, data = loglin)\n# Create mosaic plot to look at data\nmosaic(loglinTable, shade = TRUE)\n\n\n\n\n\n\nFigure 15.3: Proportion de plantes infectées en fonction de la température er la lumière\n\n\n\n\nCette expérience contrôlée avec le même nombre de plantes à chaque niveau de lumière et de température produit une mosaïque où la surface occupée par les observations dans les quatre quadrants est égale. Ce qui nous intéresse, le taux d’infection par le pathogène, semble varier entre les quadrants (i.e. les niveaux de température et de lumière). Le rectangle rouge dans le coin en bas à gauche indique que le nombre de plantes infectées à basse température et haute lumière est plus faible qu’attendu si ces deux facteurs n’influencent pas le taux d’infection. Même chose pour les conditions de basse lumière et de haute température (coin supérieur droit). La valeur p au bas de l’échelle représente un test d’indépendance équivalent à comparer le modèle complet au modèle excluant toutes les interactions et ne contenant que les effets principaux de la température, la lumière, et le statut d’infection sur le logarithme naturel du nombre d’observations.\n\n# Fit full model\nfull.model &lt;- glm(frequency ~ temperature * light * infected, family = poisson(), data = loglin)\n# Test partial effect of terms in full model\nAnova(full.model, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n                           LR Chisq Df Pr(&gt;Chisq)    \ntemperature                  9.1786  1  0.0024487 ** \nlight                       13.2829  1  0.0002678 ***\ninfected                     0.0000  1  0.9999999    \ntemperature:light            5.6758  1  0.0172008 *  \ntemperature:infected        29.0612  1  7.013e-08 ***\nlight:infected              20.2687  1  6.729e-06 ***\ntemperature:light:infected   1.0840  1  0.2978126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLes probabilités associées à chaque terme sont ici calculées en comparant l’ajustement du modèle complet à un modèle qui exclue seulement le terme d’intérêt. Plusieurs des termes sont ici sans véritable intérêt puisque les fréquences sont partiellement contrôlées dans notre expérience. Puisque la question biologique porte sur le taux d’infection, les seuls termes d’intérêt sont les termes d’interactions qui incluent le statut d’infection (temperature:infected, light:infected et temperature:light:infected.\n\nL’interation significative temperature:infected implique que le taux d’infection n’est pas indépendant de la température. D’ailleurs il est apparent dans la mosaïque que le taux d’infection (le nombre relatif de plantes infectées) est supérieur à haute température.\nL’interaction significative light:infected implique que le taux d’infection dépends de la lumière. La mosaïque illustre que la proportion des plantes infectées est plus élevée en basse lumière.\nL’interaction temperature:light:infected n’est pas significative. Cela implique que l’effet de la température et de la lumière sur le taux d’infection sont indépendants. Autrement dit, l’effet de la lumière sur le taux d’infection ne dépends pas de la température, et vice versa.",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#ex-glm",
    "href": "42-model_freq.html#ex-glm",
    "title": "\n15  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n15.7 Exercice",
    "text": "15.7 Exercice\nLe fichier Sturgdat contient les données qui vous permettront d’éprouver l’hypothèse que le nombre d’esturgeons capturé est indépendants du site, de l’année, et du sexe. Avant de commencer l’analyse, les données devront être réorganisées pour pouvoir ajuster un modèle log-linéaire:\n\n\n\n\n\n\nExercice\n\n\n\nOuvrez sturgdat.csv, puis utilisez la fonction table() pour obtenir les fréquence d’individus capturés par sex, location, et year . Sauvegardez ce tableau comme strugdat.table . Faites une figure mosaïque de ces données.\n\n\n\nsturgdat &lt;- read.csv(\"data/sturgdat.csv\")\n# Reorganize data from case form to table form\nsturgdat.table &lt;- with(sturgdat, table(sex, year, location))\n# display the table\nsturgdat.table\n\n, , location = CUMBERLAND  \n\n              year\nsex            1978 1979 1980\n  FEMALE         10   30   11\n  MALE           14   14    6\n\n, , location = THE_PAS     \n\n              year\nsex            1978 1979 1980\n  FEMALE          5   12   38\n  MALE           16   12   18\n\n# Create data frame while converting from table form to frequency form\nsturgdat.freq &lt;- as.data.frame(sturgdat.table)\n# display data frame\nsturgdat.freq\n\n            sex year     location Freq\n1  FEMALE       1978 CUMBERLAND     10\n2  MALE         1978 CUMBERLAND     14\n3  FEMALE       1979 CUMBERLAND     30\n4  MALE         1979 CUMBERLAND     14\n5  FEMALE       1980 CUMBERLAND     11\n6  MALE         1980 CUMBERLAND      6\n7  FEMALE       1978 THE_PAS         5\n8  MALE         1978 THE_PAS        16\n9  FEMALE       1979 THE_PAS        12\n10 MALE         1979 THE_PAS        12\n11 FEMALE       1980 THE_PAS        38\n12 MALE         1980 THE_PAS        18\n\n# Look at the data as mosaic plot\n# mosaic using the table created above\nmosaic(sturgdat.table, shade = TRUE)\n\n\n\n\n\n\nFigure 15.4: Fréquence de femelles et males en fonction de l’année et du lieu\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nÀ partir de ces données en format de fréquence, ajustez le modèle loglinéaire complet et le tableau d’anova avec les statistique de Chi carré pour les termes du modèles. Est-ce que l’interaction triple (location:year:sex) est significative? Est-ce que le rapport des sexes varien entre les sites ou d’une année à l’autre?.\n\n\n\n# Fit full model\nfull.model &lt;- glm(Freq ~ sex * year * location, data = sturgdat.freq, family = \"poisson\")\nsummary(full.model)\n\n\nCall:\nglm(formula = Freq ~ sex * year * location, family = \"poisson\", \n    data = sturgdat.freq)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                    2.30259    0.31623   7.281\nsexMALE                                        0.33647    0.41404   0.813\nyear1979                                       1.09861    0.36515   3.009\nyear1980                                       0.09531    0.43693   0.218\nlocationTHE_PAS                               -0.69315    0.54772  -1.266\nsexMALE        :year1979                      -1.09861    0.52554  -2.090\nsexMALE        :year1980                      -0.94261    0.65498  -1.439\nsexMALE        :locationTHE_PAS                0.82668    0.65873   1.255\nyear1979:locationTHE_PAS                      -0.22314    0.64550  -0.346\nyear1980:locationTHE_PAS                       1.93284    0.64593   2.992\nsexMALE        :year1979:locationTHE_PAS      -0.06454    0.83986  -0.077\nsexMALE        :year1980:locationTHE_PAS      -0.96776    0.87942  -1.100\n                                              Pr(&gt;|z|)    \n(Intercept)                                    3.3e-13 ***\nsexMALE                                        0.41641    \nyear1979                                       0.00262 ** \nyear1980                                       0.82732    \nlocationTHE_PAS                                0.20569    \nsexMALE        :year1979                       0.03658 *  \nsexMALE        :year1980                       0.15011    \nsexMALE        :locationTHE_PAS                0.20950    \nyear1979:locationTHE_PAS                       0.72957    \nyear1980:locationTHE_PAS                       0.00277 ** \nsexMALE        :year1979:locationTHE_PAS       0.93875    \nsexMALE        :year1980:locationTHE_PAS       0.27114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  5.7176e+01  on 11  degrees of freedom\nResidual deviance: -2.6645e-15  on  0  degrees of freedom\nAIC: 77.28\n\nNumber of Fisher Scoring iterations: 3\n\nAnova(full.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n                  LR Chisq Df Pr(&gt;Chisq)    \nsex                 0.6698  1  0.4131256    \nyear               13.8895  2  0.0009637 ***\nlocation            1.6990  1  0.1924201    \nsex:year            4.6930  2  0.0957024 .  \nsex:location        1.6323  1  0.2013888    \nyear:location      25.2580  2  3.276e-06 ***\nsex:year:location   1.6677  2  0.4343666    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCe tableau a trois critères: sex, location et year . Donc le modèles compelt (saturé) contient 7 termes: trois effets principaux (sex, location et year), trois interactions du second degré (double) (sex:year, sex:location et year: location) et une interaction du troisième degré (triple)(sex:year:location). La déviance nulle est 57.17574, la déviance résiduelle du modèle complet est, sans surprise, 0. La déviance pouvant être attribuée à l’interaction triple est 1.6677, non significative.\nQu’est ce que cela implique? S’il y a des interactions doubles, alors elles ne dépendent pas de la troisième variable. Par exemple, si le rapport des sexe des esturgeons varie d’une année à l’autre (une interaction sex:year), alors cette tendance est la même aux 2 stations.\nPuisqu’il n’y a pas d’interaction triple, il est (statistiquement) justifié de combiner les données pour éprouver les interactions du second degré. Par exemple, pour tester l’effet sex:location, on peut combiner les années. Pour tester l’effet sex:year, on peut combiner les sites. Cette aggrégation a pour effet d’augmenter la puissance, et est analogue à la stratégie en ANOVA à critères multiples. L’approche de la régression de Poisson permet de faire l’équivalent simplement en ajustant le modèle sans l’interaction du troisième degré.\n\nAjustez le modèle en excluant l’interaction du troisième degré:\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\no2int.model &lt;- glm(Freq ~ sex + year + location + sex:year + sex:location + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             1.8691  1  0.1715807    \nyear           15.1289  2  0.0005186 ***\nlocation        1.5444  1  0.2139568    \nsex:year       15.5847  2  0.0004129 ***\nsex:location    2.1762  1  0.1401583    \nyear:location  28.3499  2  6.981e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nL’interaction sex:location n’explique pas une portion significative de la déviance, alors que les deux autres sont significatives. Le rapport des sexes ne varie pas entre les sites, mais il varie selon les années. L’interaction year:location est aussi significative (voir plus pas pour son interprétation).\nDevriez vous tenter de simplifier le modèle encore plus? Les vrais statisticiens sont divisés sur cette question. Tous s’entendent cependant sur le fait que conserver des interactions non significatives dans un modèle peut réduire la puissance. De l’autre côté, le retrait des interactions non significatives peut rendre l’interprétation plus délicate lorsque les observations ne sont pas bien balancées (i.e. il y a de la colinéarité entre les termes du modèle).\n\nAjustez le modèle sans l’interaction sex:location :\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\no2int.model2 &lt;- glm(Freq ~ sex + year + location + sex:year + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model2, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             5.0970  1  0.0239677 *  \nyear           16.1226  2  0.0003155 ***\nlocation        0.2001  1  0.6546011    \nsex:year       13.9883  2  0.0009173 ***\nyear:location  26.7534  2  1.551e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nLes deux interactions sont significatives et ce modèle semble le meilleur. Ce modèle est:\n\\[ln[f_{(ijk)} ] = location + sex + year + sex:year + location:year\\]\nComment ces effets peuvent-ils être interprétés biologiquement? Souvenez vous que, comme dans les test d’indépendance, on n’est pas vraiment intéressé aux effets principaux, seulement par les interactions. Par exemple, l’effet principal de location tnous dit que le nombre total d’esturgeons capturé (le total des 2 sexes pendant les 3 années d’échantillonnage) diffère entre les 2 sites. Cela n’est pas vraiment surprenant et peu intéressant en l’absence d’information sur l’effort de pêche. Cependant, l’interaction sex:year nous dit que le rapport des sexes a changé d’une année à l’autre. Et puisque l’interaction du troisième degré n’est pas significative, on sait que ce changement dans le temps est approximativement le même dans les deux sites. Un résultat possiblement intéressant. Pourquoi? Comme l’expliquer?\nL’interaction location:year nous dit que le nombre d’esturgeons n’a pas seulementt varié d’une année à l’autre, mais que la tendance dans le temps diffère entre les deux sites. Ceci pourrait refléter une différence d’effort de pêche à un des sites durant l’une des campagnes d’échantillonnage, ou un impact à seulement un des deux sites la dernière année par exemple. Mais cette tendance est la même pour les mâles et les femelles (donc n’a pas affecté le rapport des sexes) puisque l’interaction triple n’est pas significative.",
    "crumbs": [
      "Données",
      "Modèles linéaires généralisés",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "901-bibliographie.html",
    "href": "901-bibliographie.html",
    "title": "Références",
    "section": "",
    "text": "Paquets R\nCe livre a utilisé les paquets R (excluant leurs dépendances) listé dans le tableau Table 1. Comme recommandé par l’équipe de de développement de ‘tidyverse’, seul le paquets ‘tidyverse’ est cité et non pas chacun de ses composants.\nA large number of files (6573 in total) have been discovered.\nIt may take renv a long time to crawl these files for dependencies.\nConsider using .renvignore to ignore irrelevant files.\nSee `?renv::dependencies` for more information.\nSet `options(renv.config.dependencies.limit = Inf)` to disable this warning.\n\n\n\nTable 1: Paquets utilisés dans le livre\n\n\n\n\n\n\n\n\n\nPaquets\nVersion\nCitation\n\n\n\nbase\n4.4.1\nR Core Team (2024)\n\n\nboot\n1.3.31\n\nA. C. Davison et D. V. Hinkley (1997); Angelo Canty et B. D. Ripley (2024)\n\n\n\ncar\n3.1.2\nFox et Weisberg (2019a)\n\n\neffects\n4.2.2\n\nFox (2003); Fox et Hong (2009); Fox et Weisberg (2018); Fox et Weisberg (2019b)\n\n\n\nemoji\n15.0\nHvitfeldt (2022)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggcleveland\n0.1.0\nPrunello et Mari (2021)\n\n\nggpubr\n0.6.0\nKassambara (2023)\n\n\ngrateful\n0.2.10\nRodriguez-Sanchez et Jackson (2023)\n\n\ngt\n0.11.0\nIannone et al. (2024)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nknitr\n1.48\n\nXie (2014); Xie (2015); Xie (2024)\n\n\n\nlme4\n1.1.35.5\nBates et al. (2015)\n\n\nlmPerm\n2.1.0\nWheeler et Torchiano (2016)\n\n\nlmtest\n0.9.40\nZeileis et Hothorn (2002)\n\n\nmultcomp\n1.4.26\nHothorn et al. (2008)\n\n\nMuMIn\n1.48.4\nBartoń (2024)\n\n\npalmerpenguins\n0.1.1\nHorst et al. (2020)\n\n\npatchwork\n1.2.0\nPedersen (2024)\n\n\nperformance\n0.12.3\nLüdecke et al. (2021)\n\n\npwr\n1.3.0\nChampely (2020)\n\n\nreshape2\n1.4.4\nWickham (2007)\n\n\nrmarkdown\n2.28\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2024)\n\n\n\nsimpleboot\n1.1.8\nPeng (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\nvcd\n1.4.12\n\nMeyer et al. (2006); Zeileis et al. (2007); (vcd2023?)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\nvioplot\n0.5.0\nAdler et al. (2024)",
    "crumbs": [
      "Données",
      "Références"
    ]
  },
  {
    "objectID": "901-bibliographie.html#bibliographie",
    "href": "901-bibliographie.html#bibliographie",
    "title": "Références",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nA. C. Davison, et D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge University Press, Cambridge.\n\n\nAdler, D., S. T. Kelly, T. Elliott, et J. Adamson. 2024. vioplot: violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey, A. Atkins, H. Wickham, J. Cheng, W. Chang, et R. Iannone. 2024. rmarkdown: Dynamic Documents for R.\n\n\nAngelo Canty, et B. D. Ripley. 2024. boot: Bootstrap R (S-Plus) Functions.\n\n\nBartoń, K. 2024. MuMIn: Multi-Model Inference.\n\n\nBates, D., M. Mächler, B. Bolker, et S. Walker. 2015. Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software 67:1‑48.\n\n\nChampely, S. 2020. pwr: Basic Functions for Power Analysis.\n\n\nFox, J. 2003. Effect Displays in R for Generalised Linear Models. Journal of Statistical Software 8:1‑27.\n\n\nFox, J., et J. Hong. 2009. Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package. Journal of Statistical Software 32:1‑24.\n\n\nFox, J., et S. Weisberg. 2018. Visualizing Fit and Lack of Fit in Complex Regression Models with Predictor Effect Plots and Partial Residuals. Journal of Statistical Software 87:1‑27.\n\n\nFox, J., et S. Weisberg. 2019a. An R Companion to Applied Regression. Third. Sage, Thousand Oaks CA.\n\n\nFox, J., et S. Weisberg. 2019b. An R Companion to Applied Regression. 3rd édition. Sage, Thousand Oaks CA.\n\n\nFriendly, M. 2023. vcdExtra: « vcd » Extensions and Additions.\n\n\nHorst, A. M., A. P. Hill, et K. B. Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data.\n\n\nHothorn, T., F. Bretz, et P. Westfall. 2008. Simultaneous Inference in General Parametric Models. Biometrical Journal 50:346‑363.\n\n\nHvitfeldt, E. 2022. emoji: Data and Function to Work with Emojis.\n\n\nIannone, R., J. Cheng, B. Schloerke, E. Hughes, A. Lauer, J. Seo, K. Brevoort, et O. Roy. 2024. gt: Easily Create Presentation-Ready Display Tables.\n\n\nKassambara, A. 2023. ggpubr: « ggplot2 » Based Publication Ready Plots.\n\n\nLüdecke, D., M. S. Ben-Shachar, I. Patil, P. Waggoner, et D. Makowski. 2021. performance: An R Package for Assessment, Comparison and Testing of Statistical Models. Journal of Open Source Software 6:3139.\n\n\nMeyer, D., A. Zeileis, et K. Hornik. 2006. The Strucplot Framework: Visualizing Multi-Way Contingency Tables with vcd. Journal of Statistical Software 17:1‑48.\n\n\nPedersen, T. L. 2024. patchwork: The Composer of Plots.\n\n\nPeng, R. D. 2024. simpleboot: Simple Bootstrap Routines.\n\n\nPrunello, M., et G. Mari. 2021. ggcleveland: Implementation of Plots from Cleveland’s Visualizing Data Book.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nRodriguez-Sanchez, F., et C. P. Jackson. 2023. grateful: Facilitate citation of R packages.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen, A. Elberg, et J. Crowley. 2024. GGally: Extension to « ggplot2 ».\n\n\nWheeler, B., et M. Torchiano. 2016. lmPerm: Permutation Tests for Linear Models.\n\n\nWickham, H. 2007. Reshaping Data with the reshape Package. Journal of Statistical Software 21:1‑20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E. Miller, S. M. Bache, K. Müller, J. Ooms, D. Robinson, D. P. Seidel, V. Spinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, et H. Yutani. 2019. Welcome to the tidyverse. Journal of Open Source Software 4:1686.\n\n\nXie, Y. 2014. knitr: A Comprehensive Tool for Reproducible Research in R. in V. Stodden, F. Leisch, et R. D. Peng, éditeurs. Implementing Reproducible Computational Research. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic Documents with R and knitr. 2nd édition. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y. 2024. knitr: A General-Purpose Package for Dynamic Report Generation in R.\n\n\nXie, Y., J. J. Allaire, et G. Grolemund. 2018. R Markdown: The Definitive Guide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, et E. Riederer. 2020. R Markdown Cookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZeileis, A., et T. Hothorn. 2002. Diagnostic Checking in Regression Relationships. R News 2:7‑10.\n\n\nZeileis, A., D. Meyer, et K. Hornik. 2007. Residual-based Shadings for Visualizing (Conditional) Independence. Journal of Computational and Graphical Statistics 16:507‑525.\n\n\nZhu, H. 2024. kableExtra: Construct Complex Table with « kable » and Pipe Syntax.",
    "crumbs": [
      "Données",
      "Références"
    ]
  },
  {
    "objectID": "902-donnees.html",
    "href": "902-donnees.html",
    "title": "Annexe A — Données utilisées dans le livre",
    "section": "",
    "text": "A.1 Fichier compressé tout-inclus\nTous les fichiers de données et de code dans un fichier zip",
    "crumbs": [
      "Données",
      "Annexes",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Données utilisées dans le livre</span>"
    ]
  },
  {
    "objectID": "902-donnees.html#tous-les-fichiers-séparés",
    "href": "902-donnees.html#tous-les-fichiers-séparés",
    "title": "Annexe A — Données utilisées dans le livre",
    "section": "\nA.2 Tous les fichiers séparés",
    "text": "A.2 Tous les fichiers séparés\n\nage.csv\nanc1dat.csv\nanc3dat.csv\natmosphere.txt\nBanta_TotalFruits.csv\nBiston_pd_1.csv\nBiston_pd_2.csv\nBiston_student.csv\nBiston.postdoc.csv\nBiston.prof.csv\nDam10dat.csv\ndragons.csv\nErablesGatineau.csv\ngala.txt\nhypoxia.uottawa.csv\nJacobsenDangles_1.csv\nJacobsenDangles_2.csv\nloglin.csv\nmouflon.csv\nMregdat.csv\nnematodes.csv\nnestdat.csv\nnr2wdat.csv\npollution.txt\nsalmonella.csv\nsimulies.csv\nsimuliidae.csv\nskulldat_2020.csv\nskulldat-rm.csv\nsmoking.txt\nStu2mdat.csv\nStu2wdat.csv\nsturgdat.csv\nsturgeon.csv\nunicorns_aggression.csv\nunicorns.csv\nunicorns.txt\nunicorns.xlsx\nUSPopSurvey.csv\nwmc2dat2.csv\nwmcdat2.csv",
    "crumbs": [
      "Données",
      "Annexes",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Données utilisées dans le livre</span>"
    ]
  },
  {
    "objectID": "902-donnees.html#code-r-et-fonctions-utilisées-dans-les-diapositives",
    "href": "902-donnees.html#code-r-et-fonctions-utilisées-dans-les-diapositives",
    "title": "Annexe A — Données utilisées dans le livre",
    "section": "\nA.3 Code R et fonctions utilisées dans les diapositives",
    "text": "A.3 Code R et fonctions utilisées dans les diapositives\n\nbook-fnc.R\nextra_funs.R\nglmm_simdev.rda",
    "crumbs": [
      "Données",
      "Annexes",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Données utilisées dans le livre</span>"
    ]
  }
]